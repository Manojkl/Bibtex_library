% This file was created with JabRef 2.10.
% Encoding: UTF-8


@Article{babaeizadeh2016ga3c,
  Title                    = {GA3C: GPU-based A3C for deep reinforcement learning},
  Author                   = {Babaeizadeh, Mohammad and Frosio, Iuri and Tyree, Stephen and Clemons, Jason and Kautz, Jan},
  Journal                  = {CoRR abs/1611.06256},
  Year                     = {2016},

  Abstract                 = {We introduce and analyze the computational aspects of a hybrid CPU/GPU implementation of the Asynchronous Advantage Actor-Critic (A3C) algorithm, currently the state-of-the-art method in reinforcement learning for various gaming tasks. Our analysis concentrates on the critical aspects to leverage the GPU’s computational power, including the introduction of a system of queues and a dynamic scheduling strategy, potentially helpful for other asynchronous algorithms as well. We also show the potential for the use of larger DNN models on a GPU. Our TensorFlow implementation achieves a significant speed up compared to our CPU-only implementation, and it will be made publicly available to other researchers.},
  Comment                  = {GPU based reinforcemetn learning. Citations - 35},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {https://www.researchgate.net/profile/Iuri_Frosio2/publication/310610848_GA3C_GPU-based_A3C_for_Deep_Reinforcement_Learning/links/583c6c0b08ae502a85e3dbb9/GA3C-GPU-based-A3C-for-Deep-Reinforcement-Learning.pdf}
}

@Article{bao2017deep,
  Title                    = {A deep learning framework for financial time series using stacked autoencoders and long-short term memory},
  Author                   = {Bao, Wei and Yue, Jun and Rao, Yulei},
  Journal                  = {PloS one},
  Year                     = {2017},
  Number                   = {7},
  Pages                    = {e0180944},
  Volume                   = {12},

  Abstract                 = {The application of deep learning approaches to finance has received a great deal of attention from both investors and researchers. This study presents a novel deep learning framework where wavelet transforms (WT), stacked autoencoders (SAEs) and long-short term memory (LSTM) are combined for stock price forecasting. The SAEs for hierarchically extracted deep features is introduced into stock price forecasting for the first time. The deep learning framework comprises three stages. First, the stock price time series is decomposed by WT to eliminate noise. Second, SAEs is applied to generate deep high-level features for predicting the stock price. Third, high-level denoising features are fed into LSTM to forecast the next day’s closing price. Six market indices and their corresponding index futures are chosen to examine the performance of the proposed model. Results show that the proposed model outperforms other similar models in both predictive accuracy and profitability performance.},
  Comment                  = {Deep learning framework for Time series analysis. Citations - 323},
  Owner                    = {manoj},
  Publisher                = {Public Library of Science San Francisco, CA USA},
  Timestamp                = {2020.08.27},
  Url                      = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0180944}
}

@Article{box1968some,
  Title                    = {Some recent advances in forecasting and control},
  Author                   = {Box, George EP and Jenkins, Gwilym M},
  Journal                  = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
  Year                     = {1968},
  Number                   = {2},
  Pages                    = {91--109},
  Volume                   = {17},

  Abstract                 = {A brief discussion of Statistical Quality Control Charting procedures is first presented with special reference to the relevance of the objectives and assumptions. An approach to the design of discrete feedforward and feedback control schemes, which are of great importance for example, in the chemical industry, is then given. This approach to control employs discrete stochastic and dynamic models discussed in Part I of this paper (Box and Jenkins, 1968) and has a close link with the forecasting problems discussed there. The control algorithms obtained are ideally suited to discrete digital computer control. However, for common simple situations the algorithms may be represented by suitable charts or nomograms which may be employed to obtain improved manual control. The paper ends with a discussion of a problem typical of that arising in the parts manufacturing industry. Here, attention must be given to the cost of making an adjustment to the machine as well as to the cost of being off target and to the stochastic nature of the disturbance. An example is given where the appropriate form of action is like that required by Roberts's modification of a Shewhart chart. However, the justification required to make such action appropriate is very different from that previously given.},
  Comment                  = {Forecasting and control. CItations - 173},
  Owner                    = {manoj},
  Publisher                = {JSTOR},
  Timestamp                = {2020.08.27},
  Url                      = {https://www.jstor.org/stable/2985674}
}

@Article{cao2003support,
  Title                    = {Support vector machine with adaptive parameters in financial time series forecasting},
  Author                   = {Cao, Li-Juan and Tay, Francis Eng Hock},
  Journal                  = {IEEE Transactions on neural networks},
  Year                     = {2003},
  Number                   = {6},
  Pages                    = {1506--1518},
  Volume                   = {14},

  Abstract                 = {A  novel  type  of  learning  machine  called  supportvector  machine  (SVM)  has  been  receiving  increasing  interest  inareas ranging from its original application in pattern recognitionto  other  applications  such  as  regression  estimation  due  to  itsremarkable  generalization  performance.  This  paper  deals  withthe   application   of   SVM   in   financial   time   series   forecasting.The  feasibility  of  applying  SVM  in  financial  forecasting  is  firstexamined by comparing it with the multilayer back-propagation(BP)  neural  network  and  the  regularized  radial  basis  function(RBF)  neural  network.  The  variability  in  performance  of  SVMwith respect to the free parameters is investigated experimentally.Adaptive  parameters  are  then  proposed  by  incorporating  thenonstationarity of financial time series into SVM. Five real futurescontracts collated from the Chicago Mercantile Market are used asthe data sets. The simulation shows that among the three methods,SVM outperforms the BP neural network in financial forecasting,and  there  are  comparable  generalization  performance  betweenSVM and the regularized RBF neural network. Furthermore, thefree parameters of SVM have a great effect on the generalizationperformance.  SVM  with  adaptive  parameters  can  both  achievehigher generalization performance and use fewer support vectorsthan the standard SVM in financial forecasting},
  Comment                  = {SVM for financial time series forecasting. Citations - 1000},
  Owner                    = {manoj},
  Publisher                = {IEEE},
  Timestamp                = {2020.08.27},
  Url                      = {https://c.mql5.com/forextsd/forum/35/caotay2003.pdf}
}

@Article{chung2014empirical,
  Title                    = {Empirical evaluation of gated recurrent neural networks on sequence modeling},
  Author                   = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  Journal                  = {arXiv preprint arXiv:1412.3555},
  Year                     = {2014},

  Abstract                 = {In this paper we compare different types of recurrent units in recurrent neural net-works (RNNs).  Especially, we focus on more sophisticated units that implementa gating mechanism, such as a long short-term memory (LSTM) unit and a re-cently proposed gated recurrent unit (GRU). We evaluate these recurrent units onthe tasks of polyphonic music modeling and speech signal modeling.  Our exper-iments revealed that these advanced recurrent units are indeed better than moretraditional recurrent units such astanhunits. Also, we found GRU to be compa-rable to LSTM.},
  Comment                  = {gated RNN. Citations - 5072},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {https://arxiv.org/pdf/1412.3555.pdf?ref=hackernoon.com}
}

@Article{das2013data,
  Title                    = {Data mining and neural network techniques in stock market prediction: A methodological review},
  Author                   = {Das, Debashish and Uddin, Mohammad Shorif},
  Journal                  = {International journal of artificial intelligence \& applications},
  Year                     = {2013},
  Number                   = {1},
  Pages                    = {117},
  Volume                   = {4},

  Abstract                 = {Prediction in any field is a complicated, challenging and daunting process. Employing traditional methods may not ensure the reliability of the prediction. In this paper, we are reviewing the possibility of applying two well-known techniques neural network and data mining in stock market prediction. As neural network is able to extract useful information from a huge data set and data mining is also able to predict future trends and behaviors. Therefore, a combination of both these techniques could make the prediction much reliable.},
  Comment                  = {Data mining and neural network in stock market. CItations - 21},
  Owner                    = {manoj},
  Publisher                = {Academy \& Industry Research Collaboration Center (AIRCC)},
  Timestamp                = {2020.08.27},
  Url                      = {http://search.proquest.com/openview/d9d7c6deefa76f29f94a4a43d7e8e222/1.pdf?pq-origsite=gscholar&cbl=646378}
}

@Article{deng2016deep,
  Title                    = {Deep direct reinforcement learning for financial signal representation and trading},
  Author                   = {Deng, Yue and Bao, Feng and Kong, Youyong and Ren, Zhiquan and Dai, Qionghai},
  Journal                  = {IEEE transactions on neural networks and learning systems},
  Year                     = {2016},
  Number                   = {3},
  Pages                    = {653--664},
  Volume                   = {28},

  Abstract                 = {Can  we  train  the  computer  to  beat  experiencedtraders  for  financial  assert  trading?  In  this  paper,  we  try  toaddress  this  challenge  by  introducing  a  recurrent  deep  neuralnetwork  (NN)  for  real-time  financial  signal  representation  andtrading. Our model is inspired by two biological-related learningconcepts of deep learning (DL) and reinforcement learning (RL).In the framework, the DL part automatically senses the dynamicmarket condition for informative feature learning. Then, the RLmodule  interacts  with  deep  representations  and  makes  tradingdecisions  to  accumulate  the  ultimate  rewards  in  an  unknownenvironment.  The  learning  system  is  implemented  in  a  complexNN that exhibits both the deep and recurrent structures. Hence,we propose a task-aware backpropagation through time methodto  cope  with  the  gradient  vanishing  issue  in  deep  training.  Therobustness of the neural system is verified on both the stock andthe commodity  future markets  under broad  testing conditions.},
  Comment                  = {Reinforcment learning in financial signal. Citations - 252},
  Owner                    = {manoj},
  Publisher                = {IEEE},
  Timestamp                = {2020.08.27},
  Url                      = {http://cslt.riit.tsinghua.edu.cn/mediawiki/images/a/aa/07407387.pdf}
}

@InProceedings{ding2015deep,
  Title                    = {Deep learning for event-driven stock prediction},
  Author                   = {Ding, Xiao and Zhang, Yue and Liu, Ting and Duan, Junwen},
  Booktitle                = {Twenty-fourth international joint conference on artificial intelligence},
  Year                     = {2015},

  Abstract                 = {We propose a deep learning method for eventdriven stock market prediction. First, events are extracted from news text, and represented as dense vectors, trained using a novel neural tensor network. Second, a deep convolutional neural network is used to model both short-term and long-term influences of events on stock price movements. Experimental results show that our model can achieve nearly 6% improvements on S&P 500 index prediction and individual stock prediction, respectively, compared to state-of-the-art baseline methods. In addition, market simulation results show that our system is more capable of making profits than previously reported systems trained on S&P 500 stock historical data.},
  Comment                  = {Deep learning for stock prediction. Citations - 378},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {http://www.wins.or.kr/DataPool/Board/4xxxx/455xx/45587/329.pdf}
}

@Article{duan1995garch,
  Title                    = {The GARCH option pricing model},
  Author                   = {Duan, Jin-Chuan},
  Journal                  = {Mathematical finance},
  Year                     = {1995},
  Number                   = {1},
  Pages                    = {13--32},
  Volume                   = {5},

  Abstract                 = {This article develops an option pricing model and its corresponding delta formula in the context of the generalized autoregressive conditional heteroskedastic (GARCH) asset return process. the development utilizes the locally risk‐neutral valuation relationship (LRNVR). the LRNVR is shown to hold under certain combinations of preference and distribution assumptions. the GARCH option pricing model is capable of reflecting the changes in the conditional volatility of the underlying asset in a parsimonious manner. Numerical analyses suggest that the GARCH model may be able to explain some well‐documented systematic biases associated with the Black‐Scholes model.},
  Comment                  = {GARCH option. Citations - 1486},
  Owner                    = {manoj},
  Publisher                = {Wiley Online Library},
  Timestamp                = {2020.08.27},
  Url                      = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9965.1995.tb00099.x}
}

@Article{fischer2018deep,
  Title                    = {Deep learning with long short-term memory networks for financial market predictions},
  Author                   = {Fischer, Thomas and Krauss, Christopher},
  Journal                  = {European Journal of Operational Research},
  Year                     = {2018},
  Number                   = {2},
  Pages                    = {654--669},
  Volume                   = {270},

  Abstract                 = {Long short-term memory (LSTM) networks are a state-of-the-art technique for sequence learning. They are less commonly applied to financial time series predictions, yet inherently suitable for this domain. We deploy LSTM networks for predicting out-of-sample directional movements for the constituent stocks of the S&P 500 from 1992 until 2015. With daily returns of 0.46 percent and a Sharpe Ratio of 5.8 prior to transaction costs, we find LSTM networks to outperform memoryfree classification methods, i.e., a random forest (RAF), a deep neural net (DNN), and a logistic regression classifier (LOG). We unveil sources of profitability, thereby shedding light into the black box of artificial neural networks. Specifically, we find one common pattern among the stocks selected
for trading - they exhibit high volatility and a short-term reversal return profile. Leveraging these findings, we are able to formalize a rules-based short-term reversal strategy that is able to explain a portion of the returns of the LSTM.},
  Comment                  = {Deep learning with LSTM. Citations - 413},
  Owner                    = {manoj},
  Publisher                = {Elsevier},
  Timestamp                = {2020.08.27},
  Url                      = {https://www.econstor.eu/bitstream/10419/157808/1/886576210.pdf}
}

@InProceedings{graves2013speech,
  Title                    = {Speech recognition with deep recurrent neural networks},
  Author                   = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  Booktitle                = {2013 IEEE international conference on acoustics, speech and signal processing},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {6645--6649},

  Abstract                 = {Recurrent neural networks (RNNs) are a powerful model forsequential data. End-to-end training methods such as Connec-tionist Temporal Classification make it possible to train RNNsfor sequence labelling problems where the input-output align-ment is unknown.   The combination of these methods withthe Long Short-term Memory RNN architecture has provedparticularly fruitful, delivering state-of-the-art results in cur-sive handwriting recognition. However RNN performance inspeech recognition has so far been disappointing, with betterresults returned by deep feedforward networks. This paper in-vestigatesdeep recurrent neural networks, which combine themultiple levels of representation that have proved so effectivein deep networks with the flexible use of long range contextthat  empowers  RNNs.   When  trained  end-to-end  with  suit-able regularisation, we find that deep Long Short-term Mem-ory  RNNs  achieve  a  test  set  error  of  17.7%  on  the  TIMITphoneme recognition benchmark, which to our knowledge isthe best recorded score.},
  Comment                  = {Speech recognition using DRNN. Citations - 6139},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {https://arxiv.org/pdf/1303.5778.pdf%C3%AF%C2%BC%E2%80%B0%C3%AF%C2%BC%C5%A1%E2%80%9C%C3%A5%C2%A6%E2%80%9A%C3%A6%C5%BE%C5%93LSTM%C3%A7%E2%80%9D%C2%A8%C3%A4%C2%BA%C5%BD%C3%A9%C5%A1%20%C3%A8%E2%80%94%20%C3%A5%C2%B1%E2%80%9A%C3%AF%C2%BC%C5%92%C3%A6%CB%86%E2%80%98%C3%A4%C2%BB%C2%AC%C3%A5%C2%B0%E2%80%A0%C3%A5%C2%BE%E2%80%94%C3%A5%CB%86%C2%B0}
}

@Article{hendershott2011does,
  Title                    = {Does algorithmic trading improve liquidity?},
  Author                   = {Hendershott, Terrence and Jones, Charles M and Menkveld, Albert J},
  Journal                  = {The Journal of finance},
  Year                     = {2011},
  Number                   = {1},
  Pages                    = {1--33},
  Volume                   = {66},

  Abstract                 = {Algorithmic trading (AT) has increased sharply over the past decade. Does it improve market quality, and should it be encouraged? We provide the first analysis of this question. The New York Stock Exchange automated quote dissemination in 2003, and we use this change in market structure that increases AT as an exogenous instrument to measure the causal effect of AT on liquidity. For large stocks in particular, AT narrows spreads, reduces adverse selection, and reduces trade‐related price discovery. The findings indicate that AT improves liquidity and enhances the informativeness of quotes.},
  Comment                  = {Algorithmic trading in liquidity. CItations - 1496},
  Owner                    = {manoj},
  Publisher                = {Wiley Online Library},
  Timestamp                = {2020.08.27},
  Url                      = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1540-6261.2010.01624.x}
}

@Article{hendershott2009algorithmic,
  Title                    = {Algorithmic trading and information},
  Author                   = {Hendershott, Terrence and Riordan, Ryan and others},
  Journal                  = {Manuscript, University of California, Berkeley},
  Year                     = {2009},

  Abstract                 = {We examine algorithmic trades (AT) and their role in the price discovery process in the 30DAX stocks on the Deutsche Boerse in January 2008.  AT liquidity demand represents 52% ofvolume and AT supplies liquidity on 50% of volume.  AT act strategically by monitoring themarket for liquidity and deviations of price from fundamental value.  AT consume liquidity whenit is cheap and supply liquidity when it is expensive.  AT contribute more to the efficient priceby placing more efficient quotes and AT demanding liquidity to move the prices towards theefficient price.},
  Comment                  = {Algorithmic trading. CItations - 268},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {http://www.academia.edu/download/41530807/Into_the_Breech_The_Increasing_Gap_betwe20160124-6877-1n8lopr.pdf}
}

@Article{hinton2012deep,
  Title                    = {Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  Author                   = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  Journal                  = {IEEE Signal processing magazine},
  Year                     = {2012},
  Number                   = {6},
  Pages                    = {82--97},
  Volume                   = {29},

  Abstract                 = {Most  current  speech  recognition  systems  use  hidden  Markov  models  (HMMs)  to  deal  with  the  temporal  variability  of  speech  and  Gaussian  mixture  models  (GMMs)  to  deter-mine how well each state of each HMM fits a frame  or  a  short  window  of  frames  of  coefficients  that  repre-sents the acoustic input. An alternative way to evaluate the fit is  to  use  a  feed-forward  neural  network  that  takes  several  frames  of  coefficients  as  input  and  produces  posterior  proba-bilities  over  HMM  states  as  output.  Deep  neural  networks  (DNNs)  that  have  many  hidden  layers  and  are  trained  using  new methods have been shown to outperform GMMs on a vari-ety  of  speech  recognition  benchmarks,  sometimes  by  a  large  margin. This article provides an overview of this progress and represents the shared views of four research groups that have had  recent  successes  in  using  DNNs  for  acoustic  modeling  in  speech recognition},
  Comment                  = {DNN for speech. Citations - 8210},
  Owner                    = {manoj},
  Publisher                = {IEEE},
  Timestamp                = {2020.08.27},
  Url                      = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/HintonDengYuEtAl-SPM2012.pdf}
}

@Article{hochreiter1997long,
  Title                    = {Long short-term memory},
  Author                   = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  Journal                  = {Neural computation},
  Year                     = {1997},
  Number                   = {8},
  Pages                    = {1735--1780},
  Volume                   = {9},

  Abstract                 = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
  Comment                  = {LSTM. CItations - 35943},
  Owner                    = {manoj},
  Publisher                = {MIT Press},
  Timestamp                = {2020.08.27},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.676.4320&rep=rep1&type=pdf}
}

@Article{jin2016portfolio,
  Title                    = {Portfolio management using reinforcement learning},
  Author                   = {Jin, Olivier and El-Saawy, Hamza},
  Journal                  = {Stanford University},
  Year                     = {2016},

  Abstract                 = {In this project, we use deep Q-learning to train a neural network to manage a stock portfolio of two stocks. In most cases the neural networks performed on par with benchmarks, although some models did significantly better according in terms of raw returns.},
  Comment                  = {Portfolio managemetn. Citations - 7},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {http://cs229.stanford.edu/proj2016/report/JinElSaawy-PortfolioManagementusingReinforcementLearning-report.pdf}
}

@InProceedings{kakade2002approximately,
  Title                    = {Approximately optimal approximate reinforcement learning},
  Author                   = {Kakade, Sham and Langford, John},
  Booktitle                = {ICML},
  Year                     = {2002},
  Pages                    = {267--274},
  Volume                   = {2},

  Abstract                 = {In order to solve realistic reinforcement learning problems, it is critical that approximate algorithms be used. In this paper, we present the conservative policy iteration algorithm which finds an" approximately" optimal policy, given access to a restart distribution (which draws the next state from a particular distribution) and an approximate greedy policy chooser. Crudely, the greedy policy chooser outputs a policy that usually chooses actions
with the largest state-action values of the current policy, ie it outputs an" approximate".},
  Comment                  = {Reinforcement learning. Citations - 422},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {https://www.cs.cmu.edu/~./jcl/papers/aoarl/Final.pdf}
}

@Article{kercheval2015modelling,
  Title                    = {Modelling high-frequency limit order book dynamics with support vector machines},
  Author                   = {Kercheval, Alec N and Zhang, Yuan},
  Journal                  = {Quantitative Finance},
  Year                     = {2015},
  Number                   = {8},
  Pages                    = {1315--1329},
  Volume                   = {15},

  Abstract                 = {We propose a machine learning framework to capture the dynamics of high-frequency limit order books in financial equity markets and automate real-timeprediction of metrics such as mid-price movement and price spread crossing.  Bycharacterizing each entry in a limit order book with a vector of attributes such asprice and volume at different levels, the proposed framework builds a learningmodel  for  each  metric  with  the  help  of  multi-class  support  vector  machines(SVMs).   Experiments  with  real  data  establish  that  features  selected  by  theproposed framework are effective for short term price movement forecasts.},
  Comment                  = {High frequency limit order using SVM. Citations - 87},
  Owner                    = {manoj},
  Publisher                = {Taylor \& Francis},
  Timestamp                = {2020.08.27},
  Url                      = {http://www.smallake.kr/wp-content/uploads/2015/09/paper462.pdf}
}

@Article{khaidem2016predicting,
  Title                    = {Predicting the direction of stock market prices using random forest},
  Author                   = {Khaidem, Luckyson and Saha, Snehanshu and Dey, Sudeepa Roy},
  Journal                  = {arXiv preprint arXiv:1605.00003},
  Year                     = {2016},

  Abstract                 = {Predicting trends in stock market prices has been an area of interest for researchers for many years due to its complex and dynamic nature. Intrinsic volatility in stock market across the globe makes the task of prediction challenging. Forecasting and diffusion modeling, although effective can't be the panacea to the diverse range of problems encountered in prediction, short-term or otherwise. Market risk, strongly correlated with forecasting errors, needs to be minimized to ensure minimal risk in investment. The authors propose to minimize forecasting error by treating the forecasting problem as a classification problem, a popular suite of algorithms in Machine learning. In this paper, we propose a novel way to minimize the risk of investment in stock market by predicting the returns of a stock using a class of powerful machine learning algorithms known as ensemble learning. Some of the technical indicators such as Relative Strength Index (RSI), stochastic oscillator etc are used as inputs to train our model. The learning model used is an ensemble of multiple decision trees. The algorithm is shown to outperform existing algo- rithms found in the literature. Out of Bag (OOB) error estimates have been found to be encouraging. Key Words: Random Forest Classifier, stock price forecasting, Exponential smoothing, feature extraction, OOB error and convergence.},
  Comment                  = {Stock market prediction using random forest. Citations - 112},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {https://arxiv.org/pdf/1605.00003}
}

@Article{lei2020time,
  Title                    = {Time-driven feature-aware jointly deep reinforcement learning for financial signal representation and algorithmic trading},
  Author                   = {Lei, Kai and Zhang, Bing and Li, Yu and Yang, Min and Shen, Ying},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2020},
  Pages                    = {112872},
  Volume                   = {140},

  Abstract                 = {In algorithmic trading, feature extraction and trading strategy design are two prominentchallenges to acquire long-term profits. However, the previously proposed methods rely heavily on domainknowledge to extract handcrafted features and lack an effective way to dynamically adjust the tradingstrategy. With the recent breakthroughs of deep reinforcement learning (DRL), sequential real-worldproblems can be modeled and solved with a more human-like approach. In this paper, we propose anovel trading agent, based on deep reinforcement learning, to autonomously make trading decisions andgain profits in the dynamic financial markets. We extend the value-based deep Q-network (DQN) and theasynchronous advantage actor-critic (A3C) for better adapting to the trading market. Specifically, in orderto automatically extract robust market representations and resolve the financial time series dependence,we utilize the stacked denoising autoencoders (SDAEs) and the long short-term memory (LSTM) as parts ofthe function approximator, respectively. Furthermore, we design several elaborate mechanisms to make thetrading agent more practical to the real trading environment, such as position-controlled action and n-stepreward. The experimental results show that our trading agent outperforms the baselines and achieves stablerisk-adjusted returns in both the stock and the futures markets.},
  Comment                  = {Reinforcement learning. Citations - 8},
  Owner                    = {manoj},
  Publisher                = {Elsevier},
  Timestamp                = {2020.08.27},
  Url                      = {https://ieeexplore.ieee.org/iel7/6287639/8600701/08786132.pdf}
}

@Article{matiisen2019teacher,
  Title                    = {Teacher-student curriculum learning},
  Author                   = {Matiisen, Tambet and Oliver, Avital and Cohen, Taco and Schulman, John},
  Journal                  = {IEEE transactions on neural networks and learning systems},
  Year                     = {2019},

  Abstract                 = {We propose Teacher-Student Curriculum Learning (TSCL), a framework for automatic curriculum learning, where the Student tries to learn a complex task and the Teacher automatically chooses subtasks from a given set for the Student to train on. We describe a family of Teacher algorithms that rely on the intuition that the Student should practice more those tasks on which it makes the fastest progress, i.e. where the slope of the learning curve is highest. In addition, the Teacher algorithms address the problem of forgetting by also choosing tasks where the Student’s performance is getting worse. We demonstrate that TSCL matches or surpasses the results of carefully hand-crafted curricula in two tasks: addition of decimal numbers with LSTM and navigation in Minecraft. Using our automatically generated curriculum enabled to solve a Minecraft maze that could not be solved at all when training directly on solving the maze, and the learning was an order of magnitude faster than uniform sampling of subtasks.},
  Comment                  = {Reinforcement learning. Citations - 86},
  Owner                    = {manoj},
  Publisher                = {IEEE},
  Timestamp                = {2020.08.27},
  Url                      = {https://arxiv.org/pdf/1707.00183}
}

@InProceedings{mnih2016asynchronous,
  Title                    = {Asynchronous methods for deep reinforcement learning},
  Author                   = {Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  Booktitle                = {International conference on machine learning},
  Year                     = {2016},
  Pages                    = {1928--1937},

  Abstract                 = {We propose a conceptually simpleandlightweight    framework    for    deep    reinforce-ment  learning  that  uses  asynchronous  gradientdescent for optimization of deep neural networkcontrollers. We present asynchronous variants offour standard reinforcement learning algorithmsand  show  that  parallel  actor-learners  have  astabilizing  effect  on  training  allowing  all  fourmethods  to  successfully  train  neural  networkcontrollers.The  best  performing  method,  anasynchronous  variant  of  actor-critic,  surpassesthe current state-of-the-art on the Atari domainwhile  training  for  half  the  time  on  a  singlemulti-core CPU instead of a GPU. Furthermore,we show that asynchronous actor-critic succeedson  a  wide  variety  of  continuous  motor  controlproblems as well as on a new task of navigatingrandom 3D mazes using a visual input.},
  Comment                  = {DRL. CItations - 3646},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {http://www.jmlr.org/proceedings/papers/v48/mniha16.pdf}
}

@Article{mnih2015human,
  Title                    = {Human-level control through deep reinforcement learning},
  Author                   = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  Journal                  = {nature},
  Year                     = {2015},
  Number                   = {7540},
  Pages                    = {529--533},
  Volume                   = {518},

  Abstract                 = {The theory of reinforcement learning provides a normative account1, deeply rooted in psychological2 and neuroscientific3 perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems4,5, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms3. While reinforcement learning agents have achieved some successes in a variety of domains6,7,8, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks9,10,11 to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games12. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks.},
  Comment                  = {Deep reinforcement learning. Citations - 11182},
  Owner                    = {manoj},
  Publisher                = {Nature Publishing Group},
  Timestamp                = {2020.08.27},
  Url                      = {https://daiwk.github.io/assets/dqn.pdf}
}

@Article{neely2014forecasting,
  Title                    = {Forecasting the equity risk premium: the role of technical indicators},
  Author                   = {Neely, Christopher J and Rapach, David E and Tu, Jun and Zhou, Guofu},
  Journal                  = {Management science},
  Year                     = {2014},
  Number                   = {7},
  Pages                    = {1772--1791},
  Volume                   = {60},

  Abstract                 = {Academic research relies extensively on macroeconomic variables to forecast the U.S. equity riskpremium, with relatively little attention paid to the technical indicators widely employed by practitioners.Our paper fills this gap by comparing the forecasting ability of technical indicators with that of macroe-conomic variables. Technical indicators display statistically and economically significant in-sample andout-of-sample forecasting power, matching or exceeding that of macroeconomic variables. Furthermore,technical indicators and macroeconomic variables provide complementary information over the businesscycle: technical indicators better detect the typical decline in the equity risk premium near business-cyclepeaks, while macroeconomic variables more readily pick up the typical rise in the equity risk premiumnear cyclical troughs.  Consistent with this behavior, we show that combining information from bothtechnical indicators and macroeconomic variables significantly improves equity risk premium forecastsversus using either type of information alone.  Overall,  the substantial countercyclical fluctuations inthe equity risk premium appear well captured by the combined information in technical indicators andmacroeconomic variables.},
  Comment                  = {Forecasting. Citations - 476},
  Owner                    = {manoj},
  Publisher                = {INFORMS},
  Timestamp                = {2020.08.27},
  Url                      = {https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=4062&context=lkcsb_research}
}

@Article{nuij2013automated,
  Title                    = {An automated framework for incorporating news into stock trading strategies},
  Author                   = {Nuij, Wijnand and Milea, Viorel and Hogenboom, Frederik and Frasincar, Flavius and Kaymak, Uzay},
  Journal                  = {IEEE transactions on knowledge and data engineering},
  Year                     = {2013},
  Number                   = {4},
  Pages                    = {823--835},
  Volume                   = {26},

  Abstract                 = {—In this paper we present a framework for automatic exploitation of news in stock trading strategies. Events are extracted from news messages presented in free text without annotations. We test the introduced framework by deriving trading strategies based on technical indicators and impacts of the extracted events. The strategies take the form of rules that combine technical trading indicators with a news variable, and are revealed through the use of genetic programming. We find that the news variable is often included in the optimal trading rules, indicating the added value of news for predictive purposes and validating our proposed framework for automatically incorporating news in stock trading strategies.},
  Comment                  = {Incorporate stock into stock trading. CItations - 60},
  Owner                    = {manoj},
  Publisher                = {IEEE},
  Timestamp                = {2020.08.27},
  Url                      = {https://personal.eur.nl/frasincar/papers/TKDE2014/tkde2014.pdf}
}

@Article{said1984testing,
  Title                    = {Testing for unit roots in autoregressive-moving average models of unknown order},
  Author                   = {Said, Said E and Dickey, David A},
  Journal                  = {Biometrika},
  Year                     = {1984},
  Number                   = {3},
  Pages                    = {599--607},
  Volume                   = {71},

  Abstract                 = {Recently, methods for detecting unit roots in autoregressive and autoregressive-moving average time series have been proposed. The presence of a unit root indicates that the time series is not stationary but that differencing will reduce it to stationarity. The tests proposed to date require specification of the number of autoregressive and moving average coefficients in the model. In this paper we develop a test for unit roots which is based on an approximation of an autoregressive-moving average model by an autoregression.},
  Comment                  = {Autoregressive moving average model. CItations - 3889},
  Owner                    = {manoj},
  Publisher                = {Oxford University Press},
  Timestamp                = {2020.08.27},
  Url                      = {http://www.ssc.wisc.edu/~bhansen/718/SaidDickey1984.pdf}
}

@Article{samek2013time,
  Title                    = {Time series prediction using artificial neural networks: single and multi-dimensional data},
  Author                   = {Samek, David and Varacha, Pavel},
  Journal                  = {International Journal of Mathematical Models and Methods in Applied Sciences},
  Year                     = {2013},
  Number                   = {1},
  Pages                    = {38--46},
  Volume                   = {7},

  Abstract                 = {The  paper  studies  time  series  prediction  using artificial  neural  networks.  The  special  attention  is  paid  to  the influence  of  size  of  the  input  vector  length.  Furthermore,  the prediction  of  standard  single-dimensional  data  signal  and  the prediction of multi-dimensional data signal are compared. The tested artificial  networks  are  as  follows:  multilayer  feed-forward  neural network,  recurrent  Elman  neural network, adaptive linear network and radial basis function neural network.},
  Comment                  = {Time series prediction using ANN. CItations - 7},
  Owner                    = {manoj},
  Publisher                = {North Atlantic University Union (NAUN)},
  Timestamp                = {2020.08.27},
  Url                      = {https://pdfs.semanticscholar.org/a4e9/02f7b7b6bf7a740620aff5c896018bf6b103.pdf}
}

@Article{schaul2015prioritized,
  Title                    = {Prioritized experience replay},
  Author                   = {Schaul, Tom and Quan, John and Antonoglou, Ioannis and Silver, David},
  Journal                  = {arXiv preprint arXiv:1511.05952},
  Year                     = {2015},

  Abstract                 = {Experience replay lets online reinforcement learning agents remember and reuseexperiences from the past.  In prior work, experience transitions were uniformlysampled from a replay memory. However, this approach simply replays transitionsat the same frequency that they were originally experienced, regardless of theirsignificance.  In this paper we develop a framework for prioritizing experience,so as to replay important transitions more frequently,  and therefore learn moreefficiently.  We use prioritized experience replay in Deep Q-Networks (DQN), areinforcement learning algorithm that achieved human-level performance acrossmany Atari games. DQN with prioritized experience replay achieves a new state-of-the-art, outperforming DQN with uniform replay on 41 out of 49 games.},
  Comment                  = {Learning from experience. CItations - 1432},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {https://arxiv.org/pdf/1511.05952}
}

@Article{schulman2017proximal,
  Title                    = {Proximal policy optimization algorithms},
  Author                   = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  Journal                  = {arXiv preprint arXiv:1707.06347},
  Year                     = {2017},

  Abstract                 = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a “surrogate” objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  Comment                  = {Policy optimization. Citations - 2677},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {https://arxiv.org/pdf/1707.06347}
}

@Article{sharpe1998morningstar,
  Title                    = {Morningstar's risk-adjusted ratings},
  Author                   = {Sharpe, William F},
  Journal                  = {Financial Analysts Journal},
  Year                     = {1998},
  Number                   = {4},
  Pages                    = {21--33},
  Volume                   = {54},

  Abstract                 = {The last decade has seen the rapid growth of investment via mutual funds across the globe. This has led to a demand for simple measures of the performance of such funds. In the United States, the most popular is the "risk-adjusted rating" (RAR) produced by Morningstar, Incorporated. This measure differs significantly from more traditional ones such as various forms of the Sharpe ratio. This paper investigates the properties of Morningstar's measure. We show that the RAR measure has characteristics similar to those of an expected utility function based on an underlying bilinear utility function. This is of some concern, since strict adherence to a goal of maximizing expected utility with such a function could lead to extreme investment strategies. Next, we show that in practice, Morningstar varies one of the parameters of this function in a manner that frequently leads to results similar to those that would be obtained with the more traditional excess return Sharpe Ratio. Finally, we argue that neither Morningstar's measure nor the excess return Sharpe Ratio is an efficient tool for choosing mutual funds within peer groups when constructing a multi-fund portfolio --the ostensible purpose for which Morningstar's rankings are produced.},
  Comment                  = {Risk adjustment. Citations - 196},
  Owner                    = {manoj},
  Publisher                = {Taylor \& Francis},
  Timestamp                = {2020.08.27},
  Url                      = {https://web.stanford.edu/~wfsharpe/art/msrar/msrar.doc}
}

@Article{silver2016mastering,
  Title                    = {Mastering the game of Go with deep neural networks and tree search},
  Author                   = {Silver, David and Huang, Aja and Maddison, Chris J and Guez, Arthur and Sifre, Laurent and Van Den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and others},
  Journal                  = {nature},
  Year                     = {2016},
  Number                   = {7587},
  Pages                    = {484--489},
  Volume                   = {529},

  Abstract                 = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks’ to evaluate board positions and ‘policy networks’ to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8% winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
  Comment                  = {Go with DNN. Citations - 8101},
  Owner                    = {manoj},
  Publisher                = {Nature Publishing Group},
  Timestamp                = {2020.08.27},
  Url                      = {http://www.cs.cmu.edu/afs/cs.cmu.edu/academic/class/15780-s16/www/AlphaGo.nature16961.pdf}
}

@Book{sutton1998introduction,
  Title                    = {Introduction to reinforcement learning},
  Author                   = {Sutton, Richard S and Barto, Andrew G and others},
  Publisher                = {MIT press Cambridge},
  Year                     = {1998},
  Volume                   = {135},

  Abstract                 = {We are nearing an important milestone in the history of life on earth, the point at which we can construct machines with the potential for exhibiting an intelligence comparable to ours.”-- David Waltz, 1988 (recent president of AAAI) Should occur in≈ 2030 for≈ $1000 We don't yet have the needed AI “software”(designs, ideas) But the hardware will be a tremendous economic spur to development of the ideas... perhaps at nearly the same time},
  Comment                  = {Reinforcement learning. Citations - 7015},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {https://login.cs.utexas.edu/sites/default/files/legacy_files/research/documents/1%20intro%20up%20to%20RL%3ATD.pdf}
}

@InProceedings{sutton2000policy,
  Title                    = {Policy gradient methods for reinforcement learning with function approximation},
  Author                   = {Sutton, Richard S and McAllester, David A and Singh, Satinder P and Mansour, Yishay},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2000},
  Pages                    = {1057--1063},

  Abstract                 = {Function approximation is essential to reinforcement learning, but the standard approach of approximating a  value function and deter-mining a  policy from it has so far proven theoretically intractable. In this paper we explore an alternative approach in which the policy is  explicitly represented by its own function approximator, indepen-dent of the value function, and is updated according to the gradient of expected reward with respect to the policy parameters. Williams's REINFORCE method and actor-critic methods are examples of this approach. Our main new result is to show that the gradient can be written in a  form suitable for estimation from experience aided by an approximate action-value or advantage function. Using this result, we prove for the first time that a  version of policy iteration with arbitrary differentiable function approximation is convergent to a  locally optimal policy.},
  Comment                  = {Reinforcemetn learning. CItations - 3424},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {http://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf}
}

@Article{theate2020application,
  Title                    = {An application of deep reinforcement learning to algorithmic trading},
  Author                   = {Th{\'e}ate, Thibaut and Ernst, Damien},
  Journal                  = {arXiv preprint arXiv:2004.06627},
  Year                     = {2020},

  Abstract                 = {This scientific research paper presents an innovative approach based on deep reinforcement learning (DRL) to solve thealgorithmic trading problem of determining the optimal trading position at any point in time during a trading activityin stock markets.  It proposes a novel DRL trading strategy so as to maximise the resulting Sharpe ratio performanceindicator on a broad range of stock markets.  Denominated the Trading Deep Q-Network algorithm (TDQN), this newtrading strategy is inspired from the popular DQN algorithm and significantly adapted to the specific algorithmic tradingproblem at hand.  The training of the resulting reinforcement learning (RL) agent is entirely based on the generation ofartificial trajectories from a limited set of stock market historical data.  In order to objectively assess the performanceof  trading  strategies,  the  research  paper  also  proposes  a  novel,  more  rigorous  performance  assessment  methodology.Following this new performance assessment approach, promising results are reported for the TDQN strategy.},
  Comment                  = {Deep reinforcement learning. CItations - 1},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {https://arxiv.org/pdf/2004.06627}
}

@Article{tran2018temporal,
  Title                    = {Temporal attention-augmented bilinear network for financial time-series data analysis},
  Author                   = {Tran, Dat Thanh and Iosifidis, Alexandros and Kanniainen, Juho and Gabbouj, Moncef},
  Journal                  = {IEEE transactions on neural networks and learning systems},
  Year                     = {2018},
  Number                   = {5},
  Pages                    = {1407--1418},
  Volume                   = {30},

  Abstract                 = {Financial time-series forecasting has long been achallenging problem because of the inherently noisy and stochas-tic nature of the market. In the High-Frequency Trading (HFT),forecasting for trading purposes is even a more challengingtasksince an automated inference system is required to be bothaccurate and fast. In this paper, we propose a neural networklayer architecture that incorporates the idea of bilinear projectionas well as an attention mechanism that enables the layer todetect and focus on crucial temporal information. The resultingnetwork is highly interpretable, given its ability to highlight theimportance and contribution of each temporal instance, thusallowing further analysis on the time instances of interest. Ourexperiments in a large-scale Limit Order Book (LOB) datasetshow that a two-hidden-layer network utilizing our proposedlayer outperforms by a large margin all existing state-of-the-artresults coming from much deeper architectures while requiringfar fewer computations.},
  Comment                  = {Temporal attention. Citations - 51},
  Owner                    = {manoj},
  Publisher                = {IEEE},
  Timestamp                = {2020.08.27},
  Url                      = {https://arxiv.org/pdf/1712.00975.pdf}
}

@InProceedings{tsantekidis2017forecasting,
  Title                    = {Forecasting stock prices from the limit order book using convolutional neural networks},
  Author                   = {Tsantekidis, Avraam and Passalis, Nikolaos and Tefas, Anastasios and Kanniainen, Juho and Gabbouj, Moncef and Iosifidis, Alexandros},
  Booktitle                = {2017 IEEE 19th Conference on Business Informatics (CBI)},
  Year                     = {2017},
  Organization             = {IEEE},
  Pages                    = {7--12},
  Volume                   = {1},

  Abstract                 = {In  today’s  financial  markets,  where  mosttrades are performed in their entirety by electronic meansand the largest fraction of them is completely automated,an opportunity has risen from analyzing this vast amountof transactions. Since all the transactions are recordedin great detail, investors can analyze all the generateddata and detect repeated patterns of the price movements.Being able to detect them in advance, allows them totake profitable positions or avoid anomalous events in thefinancial markets. In this work we proposed a deep learningmethodology, based on Convolutional Neural Networks(CNNs), that predicts the price movements of stocks, usingas input large-scale, high-frequency time-series derivedfrom the order book of financial exchanges. The datasetthat we use contains more than 4 million limit order eventsand our comparison with other methods, like MultilayerNeural Networks and Support Vector Machines, shows thatCNNs are better suited for this kind of task.},
  Comment                  = {Forecasting using CNN. CItations - 91},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {http://users.auth.gr/passalis/assets/pdf/confs/2017_CBI_CNNLOB.pdf}
}

@Article{van2015deep,
  Title                    = {Deep reinforcement learning with double q-learning},
  Author                   = {Van Hasselt, Hado and Guez, Arthur and Silver, David},
  Journal                  = {arXiv preprint arXiv:1509.06461},
  Year                     = {2015},

  Abstract                 = {The popular Q-learning algorithm is known to overestimateaction values under certain conditions. It was not previouslyknown  whether,  in  practice,  such  overestimations  are  com-mon, whether they harm performance, and whether they cangenerally  be  prevented.  In  this  paper,  we  answer  all  thesequestions affirmatively. In particular, we first show that therecent  DQN  algorithm,  which  combines  Q-learning  with  adeep neural network, suffers from substantial overestimationsin some games in the Atari 2600 domain. We then show thatthe idea behind the Double Q-learning algorithm, which wasintroduced  in  a  tabular  setting,  can  be  generalized  to  workwith large-scale function approximation. We propose a spe-cific adaptation to the DQN algorithm and show that the re-sulting algorithm not only reduces the observed overestima-tions, as hypothesized, but that this also leads to much betterperformance on several games.},
  Comment                  = {Deep reinforcement learning with q-learning, Citations - 2376},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {https://arxiv.org/pdf/1509.06461}
}

@Article{vincent2010stacked,
  Title                    = {Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.},
  Author                   = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine and Bottou, L{\'e}on},
  Journal                  = {Journal of machine learning research},
  Year                     = {2010},
  Number                   = {12},
  Volume                   = {11},

  Abstract                 = {We explore an original strategy for building deep networks,based on stacking layers ofdenoisingautoencoderswhich are trained locally to denoise corrupted versions of their inputs. The resultingalgorithm is a straightforward variation on the stacking ofordinary autoencoders.  It is howevershown on a benchmark of classification problems to yield significantly lower classification error,thus bridging the performance gap with deep belief networks(DBN), and in several cases surpass-ing it.  Higher level representations learnt in this purely unsupervised fashion also help boost theperformance of subsequent SVM classifiers.  Qualitative experiments show that, contrary to ordi-nary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from naturalimage patches and larger stroke detectors from digit images. This work clearly establishes the valueof using a denoising criterion as a tractable unsupervised objective to guide the learning of usefulhigher level representations.},
  Comment                  = {Autoencoders. Citations - 5084},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf?source=post_page---------------------------}
}

@Article{walker1931periodicity,
  Title                    = {On periodicity in series of related terms},
  Author                   = {Walker, Gilbert Thomas},
  Journal                  = {Proceedings of the Royal Society of London. Series A, Containing Papers of a Mathematical and Physical Character},
  Year                     = {1931},
  Number                   = {818},
  Pages                    = {518--532},
  Volume                   = {131},

  Abstract                 = {An important extension of our ideas regarding periodicity was made in 1927 when Yule pointed out that, instead of regarding a series of annual sunspot numbers as consisting merely of a harmonic series to which a series of random terms were added, we might suppose a certain amount of causal relationship between the successive annual numbers. In that case the system might be regarded as a physical system possessing one or more natural oscillations of its own, all subject to damping; and the effect of annual random disturbances would be to produce a fairly smooth curve with periods varying in amplitude and length, essentially as the sunspot numbers vary. If we call the departures from their mean of our series u1, u2.., Yule showed that the consequence of a single natural period is an equation like ux = kux-1 - ux-2 + vx, where vx represents the “accidental” external “disturbance”; and if there are two natural periods, ux = k1 (ux-1 + ux-3) - k2ux-2 - ux-4 + vx},
  Comment                  = {Periodicity. Citations - 539},
  Owner                    = {manoj},
  Publisher                = {The Royal Society London},
  Timestamp                = {2020.08.27},
  Url                      = {https://royalsocietypublishing.org/doi/abs/10.1098/rspa.1931.0069}
}

@InProceedings{wang2016dueling,
  Title                    = {Dueling network architectures for deep reinforcement learning},
  Author                   = {Wang, Ziyu and Schaul, Tom and Hessel, Matteo and Hasselt, Hado and Lanctot, Marc and Freitas, Nando},
  Booktitle                = {International conference on machine learning},
  Year                     = {2016},
  Pages                    = {1995--2003},

  Abstract                 = {In recent years there have been many successesof  using  deep  representations  in  reinforcementlearning.   Still,  many  of  these  applications  useconventional architectures, such as convolutionalnetworks, LSTMs, or auto-encoders.  In this pa-per,  we present a new neural network architec-ture for model-free reinforcement learning.  Ourdueling network represents two separate estima-tors: one for the state value function and one forthe  state-dependent  action  advantage  function.The main benefit of this factoring is to general-ize learning across actions without imposing anychange to the underlying reinforcement learningalgorithm.   Our  results  show  that  this  architec-ture leads to better policy evaluation in the pres-ence of many similar-valued actions.  Moreover,the dueling architecture enables our RL agent tooutperform the state-of-the-art on the Atari 2600domain.},
  Comment                  = {Deep reinforcement learning. Citations - 1301},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {http://proceedings.mlr.press/v48/wangf16.pdf}
}

@Article{williams2007partially,
  Title                    = {Partially observable Markov decision processes for spoken dialog systems},
  Author                   = {Williams, Jason D and Young, Steve},
  Journal                  = {Computer Speech \& Language},
  Year                     = {2007},
  Number                   = {2},
  Pages                    = {393--422},
  Volume                   = {21},

  Abstract                 = {In a spoken dialog system, determining which action a machine should take in a given situation is a difficult problem because automatic speech recognition is unreliable and hence the state of the conversation can never be known with certainty. Much of the research in spoken dialog systems centres on mitigating this uncertainty and recent work has focussed on three largely disparate techniques: parallel dialog state hypotheses, local use of confidence scores, and automated planning. While in isolation each of these approaches can improve action selection, taken together they currently lack a unified statistical framework that admits global optimization. In this paper we cast a spoken dialog system as a partially observable Markov decision process (POMDP). We show how this formulation unifies and extends existing techniques to form a single principled framework. A number of illustrations are used to show qualitatively the potential benefits of POMDPs compared to existing techniques, and empirical results from dialog simulations are presented which demonstrate significant quantitative gains. Finally, some of the key challenges to advancing this method – in particular scalability – are briefly outlined.},
  Comment                  = {Markov decision process. Citations - 891},
  Owner                    = {manoj},
  Publisher                = {Elsevier},
  Timestamp                = {2020.08.27},
  Url                      = {http://svr-www.eng.cam.ac.uk/~sjy/papers/wiyo07-j.pdf}
}

@InProceedings{yue2015beyond,
  Title                    = {Beyond short snippets: Deep networks for video classification},
  Author                   = {Yue-Hei Ng, Joe and Hausknecht, Matthew and Vijayanarasimhan, Sudheendra and Vinyals, Oriol and Monga, Rajat and Toderici, George},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2015},
  Pages                    = {4694--4702},

  Abstract                 = {Convolutional neural networks (CNNs) have been exten-sively applied for image recognition problems giving state-of-the-art  results  on  recognition,  detection,  segmentationand retrieval. In this work we propose and evaluate severaldeep neural network architectures to combine image infor-mation across a video over longer time periods than previ-ously attempted.  We propose two methods capable of han-dling full length videos.  The first method explores variousconvolutional  temporal  feature  pooling  architectures,  ex-amining the various design choices which need to be madewhen adapting a CNN for this task.  The second proposedmethod explicitly models the video as an ordered sequenceof frames.  For this purpose we employ a recurrent neuralnetwork that uses Long Short-Term Memory (LSTM) cellswhich are connected to the output of the underlying CNN.Our best networks exhibit significant performance improve-ments over previously published results on the Sports 1 mil-lion dataset (73.1% vs.  60.9%) and the UCF-101 datasetswith (88.6% vs. 88.0%) and without additional optical flowinformation (82.6% vs. 73.0%).},
  Comment                  = {DNN for video classification. Citations - 1666},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Ng_Beyond_Short_Snippets_2015_CVPR_paper.pdf}
}

