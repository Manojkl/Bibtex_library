<!DOCTYPE HTML>
<html>
<head>
<title>JabRef references</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<script type="text/javascript">
<!--
// QuickSearch script for JabRef HTML export 
// Version: 3.0
//
// Copyright (c) 2006-2011, Mark Schenk
//
// This software is distributed under a Creative Commons Attribution 3.0 License
// http://creativecommons.org/licenses/by/3.0/
//
// Features:
// - intuitive find-as-you-type searching
//    ~ case insensitive
//    ~ ignore diacritics (optional)
//
// - search with/without Regular Expressions
// - match BibTeX key
//

// Search settings
var searchAbstract = true;	// search in abstract
var searchComment = true;	// search in comment

var noSquiggles = true; 	// ignore diacritics when searching
var searchRegExp = false; 	// enable RegExp searches


if (window.addEventListener) {
	window.addEventListener("load",initSearch,false); }
else if (window.attachEvent) {
	window.attachEvent("onload", initSearch); }

function initSearch() {
	// check for quick search table and searchfield
	if (!document.getElementById('qs_table')||!document.getElementById('quicksearch')) { return; }

	// load all the rows and sort into arrays
	loadTableData();
	
	//find the query field
	qsfield = document.getElementById('qs_field');

	// previous search term; used for speed optimisation
	prevSearch = '';

	//find statistics location
	stats = document.getElementById('stat');
	setStatistics(-1);
	
	// set up preferences
	initPreferences();

	// shows the searchfield
	document.getElementById('quicksearch').style.display = 'block';
	document.getElementById('qs_field').onkeyup = quickSearch;
}

function loadTableData() {
	// find table and appropriate rows
	searchTable = document.getElementById('qs_table');
	var allRows = searchTable.getElementsByTagName('tbody')[0].getElementsByTagName('tr');

	// split all rows into entryRows and infoRows (e.g. abstract, comment, bibtex)
	entryRows = new Array(); infoRows = new Array(); absRows = new Array(); revRows = new Array();

	// get data from each row
	entryRowsData = new Array(); absRowsData = new Array(); revRowsData = new Array(); 
	
	BibTeXKeys = new Array();
	
	for (var i=0, k=0, j=0; i<allRows.length;i++) {
		if (allRows[i].className.match(/entry/)) {
			entryRows[j] = allRows[i];
			entryRowsData[j] = stripDiacritics(getTextContent(allRows[i]));
			allRows[i].id ? BibTeXKeys[j] = allRows[i].id : allRows[i].id = 'autokey_'+j;
			j ++;
		} else {
			infoRows[k++] = allRows[i];
			// check for abstract/comment
			if (allRows[i].className.match(/abstract/)) {
				absRows.push(allRows[i]);
				absRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			} else if (allRows[i].className.match(/comment/)) {
				revRows.push(allRows[i]);
				revRowsData[j-1] = stripDiacritics(getTextContent(allRows[i]));
			}
		}
	}
	//number of entries and rows
	numEntries = entryRows.length;
	numInfo = infoRows.length;
	numAbs = absRows.length;
	numRev = revRows.length;
}

function quickSearch(){
	
	tInput = qsfield;

	if (tInput.value.length == 0) {
		showAll();
		setStatistics(-1);
		qsfield.className = '';
		return;
	} else {
		t = stripDiacritics(tInput.value);

		if(!searchRegExp) { t = escapeRegExp(t); }
			
		// only search for valid RegExp
		try {
			textRegExp = new RegExp(t,"i");
			closeAllInfo();
			qsfield.className = '';
		}
			catch(err) {
			prevSearch = tInput.value;
			qsfield.className = 'invalidsearch';
			return;
		}
	}
	
	// count number of hits
	var hits = 0;

	// start looping through all entry rows
	for (var i = 0; cRow = entryRows[i]; i++){

		// only show search the cells if it isn't already hidden OR if the search term is getting shorter, then search all
		if(cRow.className.indexOf('noshow')==-1 || tInput.value.length <= prevSearch.length){
			var found = false; 

			if (entryRowsData[i].search(textRegExp) != -1 || BibTeXKeys[i].search(textRegExp) != -1){ 
				found = true;
			} else {
				if(searchAbstract && absRowsData[i]!=undefined) {
					if (absRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
				if(searchComment && revRowsData[i]!=undefined) {
					if (revRowsData[i].search(textRegExp) != -1){ found=true; } 
				}
			}
			
			if (found){
				cRow.className = 'entry show';
				hits++;
			} else {
				cRow.className = 'entry noshow';
			}
		}
	}

	// update statistics
	setStatistics(hits)
	
	// set previous search value
	prevSearch = tInput.value;
}


// Strip Diacritics from text
// http://stackoverflow.com/questions/990904/javascript-remove-accents-in-strings

// String containing replacement characters for stripping accents 
var stripstring = 
    'AAAAAAACEEEEIIII'+
    'DNOOOOO.OUUUUY..'+
    'aaaaaaaceeeeiiii'+
    'dnooooo.ouuuuy.y'+
    'AaAaAaCcCcCcCcDd'+
    'DdEeEeEeEeEeGgGg'+
    'GgGgHhHhIiIiIiIi'+
    'IiIiJjKkkLlLlLlL'+
    'lJlNnNnNnnNnOoOo'+
    'OoOoRrRrRrSsSsSs'+
    'SsTtTtTtUuUuUuUu'+
    'UuUuWwYyYZzZzZz.';

function stripDiacritics(str){

    if(noSquiggles==false){
        return str;
    }

    var answer='';
    for(var i=0;i<str.length;i++){
        var ch=str[i];
        var chindex=ch.charCodeAt(0)-192;   // Index of character code in the strip string
        if(chindex>=0 && chindex<stripstring.length){
            // Character is within our table, so we can strip the accent...
            var outch=stripstring.charAt(chindex);
            // ...unless it was shown as a '.'
            if(outch!='.')ch=outch;
        }
        answer+=ch;
    }
    return answer;
}

// http://stackoverflow.com/questions/3446170/escape-string-for-use-in-javascript-regex
// NOTE: must escape every \ in the export code because of the JabRef Export...
function escapeRegExp(str) {
  return str.replace(/[-\[\]\/\{\}\(\)\*\+\?\.\\\^\$\|]/g, "\\$&");
}

function toggleInfo(articleid,info) {

	var entry = document.getElementById(articleid);
	var abs = document.getElementById('abs_'+articleid);
	var rev = document.getElementById('rev_'+articleid);
	var bib = document.getElementById('bib_'+articleid);
	
	if (abs && info == 'abstract') {
		abs.className.indexOf('noshow') == -1?abs.className = 'abstract noshow':abs.className = 'abstract show';
	} else if (rev && info == 'comment') {
		rev.className.indexOf('noshow') == -1?rev.className = 'comment noshow':rev.className = 'comment show';
	} else if (bib && info == 'bibtex') {
		bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex show';
	} else { 
		return;
	}

	// check if one or the other is available
	var revshow; var absshow; var bibshow;
	(abs && abs.className.indexOf('noshow') == -1)? absshow = true: absshow = false;
	(rev && rev.className.indexOf('noshow') == -1)? revshow = true: revshow = false;	
	(bib && bib.className.indexOf('noshow') == -1)? bibshow = true: bibshow = false;
	
	// highlight original entry
	if(entry) {
		if (revshow || absshow || bibshow) {
		entry.className = 'entry highlight show';
		} else {
		entry.className = 'entry show';
		}
	}
	
	// When there's a combination of abstract/comment/bibtex showing, need to add class for correct styling
	if(absshow) {
		(revshow||bibshow)?abs.className = 'abstract nextshow':abs.className = 'abstract';
	} 
	if (revshow) {
		bibshow?rev.className = 'comment nextshow': rev.className = 'comment';
	}	
	
}

function setStatistics (hits) {
	if(hits < 0) { hits=numEntries; }
	if(stats) { stats.firstChild.data = hits + '/' + numEntries}
}

function getTextContent(node) {
	// Function written by Arve Bersvendsen
	// http://www.virtuelvis.com
	
	if (node.nodeType == 3) {
	return node.nodeValue;
	} // text node
	if (node.nodeType == 1 && node.className != "infolinks") { // element node
	var text = [];
	for (var chld = node.firstChild;chld;chld=chld.nextSibling) {
		text.push(getTextContent(chld));
	}
	return text.join("");
	} return ""; // some other node, won't contain text nodes.
}

function showAll(){
	closeAllInfo();
	for (var i = 0; i < numEntries; i++){ entryRows[i].className = 'entry show'; }
}

function closeAllInfo(){
	for (var i=0; i < numInfo; i++){
		if (infoRows[i].className.indexOf('noshow') ==-1) {
			infoRows[i].className = infoRows[i].className + ' noshow';
		}
	}
}

function clearQS() {
	qsfield.value = '';
	showAll();
}

function redoQS(){
	showAll();
	quickSearch(qsfield);
}

function updateSetting(obj){
	var option = obj.id;
	var checked = obj.value;

	switch(option)
	 {
	 case "opt_searchAbs":
	   searchAbstract=!searchAbstract;
	   redoQS();
	   break;
	 case "opt_searchRev":
	   searchComment=!searchComment;
	   redoQS();
	   break;
	 case "opt_useRegExp":
	   searchRegExp=!searchRegExp;
	   redoQS();
	   break;
	 case "opt_noAccents":
	   noSquiggles=!noSquiggles;
	   loadTableData();
	   redoQS();
	   break;
	 }
}

function initPreferences(){
	if(searchAbstract){document.getElementById("opt_searchAbs").checked = true;}
	if(searchComment){document.getElementById("opt_searchRev").checked = true;}
	if(noSquiggles){document.getElementById("opt_noAccents").checked = true;}
	if(searchRegExp){document.getElementById("opt_useRegExp").checked = true;}
	
	if(numAbs==0) {document.getElementById("opt_searchAbs").parentNode.style.display = 'none';}
	if(numRev==0) {document.getElementById("opt_searchRev").parentNode.style.display = 'none';}
}

function toggleSettings(){
	var togglebutton = document.getElementById('showsettings');
	var settings = document.getElementById('settings');
	
	if(settings.className == "hidden"){
		settings.className = "show";
		togglebutton.innerText = "close settings";
		togglebutton.textContent = "close settings";
	}else{
		settings.className = "hidden";
		togglebutton.innerText = "settings...";		
		togglebutton.textContent = "settings...";
	}
}

-->
</script>
<style type="text/css">
body { background-color: white; font-family: Arial, sans-serif; font-size: 13px; line-height: 1.2; padding: 1em; color: #2E2E2E; margin: auto 2em; }

form#quicksearch { width: auto; border-style: solid; border-color: gray; border-width: 1px 0px; padding: 0.7em 0.5em; display:none; position:relative; }
span#searchstat {padding-left: 1em;}

div#settings { margin-top:0.7em; /* border-bottom: 1px transparent solid; background-color: #efefef; border: 1px grey solid; */ }
div#settings ul {margin: 0; padding: 0; }
div#settings li {margin: 0; padding: 0 1em 0 0; display: inline; list-style: none; }
div#settings li + li { border-left: 2px #efefef solid; padding-left: 0.5em;}
div#settings input { margin-bottom: 0px;}

div#settings.hidden {display:none;}

#showsettings { border: 1px grey solid; padding: 0 0.5em; float:right; line-height: 1.6em; text-align: right; }
#showsettings:hover { cursor: pointer; }

.invalidsearch { background-color: red; }
input[type="button"] { background-color: #efefef; border: 1px #2E2E2E solid;}

table { width: 100%; empty-cells: show; border-spacing: 0em 0.2em; margin: 1em 0em; border-style: none; }
th, td { border: 1px gray solid; border-width: 1px 1px; padding: 0.5em; vertical-align: top; text-align: left; }
th { background-color: #efefef; }
td + td, th + th { border-left: none; }

td a { color: navy; text-decoration: none; }
td a:hover  { text-decoration: underline; }

tr.noshow { display: none;}
tr.highlight td { background-color: #EFEFEF; border-top: 2px #2E2E2E solid; font-weight: bold; }
tr.abstract td, tr.comment td, tr.bibtex td { background-color: #EFEFEF; text-align: justify; border-bottom: 2px #2E2E2E solid; }
tr.nextshow td { border-bottom: 1px gray solid; }

tr.bibtex pre { width: 100%; overflow: auto; white-space: pre-wrap;}
p.infolinks { margin: 0.3em 0em 0em 0em; padding: 0px; }

@media print {
	p.infolinks, #qs_settings, #quicksearch, t.bibtex { display: none !important; }
	tr { page-break-inside: avoid; }
}
</style>
</head>
<body>

<form action="" id="quicksearch">
<input type="text" id="qs_field" autocomplete="off" placeholder="Type to search..." /> <input type="button" onclick="clearQS()" value="clear" />
<span id="searchstat">Matching entries: <span id="stat">0</span></span>
<div id="showsettings" onclick="toggleSettings()">settings...</div>
<div id="settings" class="hidden">
<ul>
<li><input type="checkbox" class="search_setting" id="opt_searchAbs" onchange="updateSetting(this)"><label for="opt_searchAbs"> include abstract</label></li>
<li><input type="checkbox" class="search_setting" id="opt_searchRev" onchange="updateSetting(this)"><label for="opt_searchRev"> include comment</label></li>
<li><input type="checkbox" class="search_setting" id="opt_useRegExp" onchange="updateSetting(this)"><label for="opt_useRegExp"> use RegExp</label></li>
<li><input type="checkbox" class="search_setting" id="opt_noAccents" onchange="updateSetting(this)"><label for="opt_noAccents"> ignore accents</label></li>
</ul>
</div>
</form>
<table id="qs_table" border="1">
<thead><tr><th width="20%">Author</th><th width="30%">Title</th><th width="5%">Year</th><th width="30%">Journal/Proceedings</th><th width="10%">Reftype</th><th width="5%">DOI/URL</th></tr></thead>
<tbody><tr id="ajmal2012video" class="entry">
	<td>Ajmal, M., Ashraf, M.H., Shakir, M., Abbas, Y. and Shah, F.A.</td>
	<td>Video summarization: techniques and classification <p class="infolinks">[<a href="javascript:toggleInfo('ajmal2012video','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('ajmal2012video','comment')">Comment</a>] [<a href="javascript:toggleInfo('ajmal2012video','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>International Conference on Computer Vision and Graphics, pp. 1-13&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://link.springer.com/chapter/10.1007/978-3-642-33564-8_1">URL</a>&nbsp;</td>
</tr>
<tr id="abs_ajmal2012video" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A large number of cameras record video around the clock, producing huge volumes. Processing these huge chunks of videos demands plenty of resources like time, man power, and hardware storage etc. Video summarization plays an important role in this context. It helps in efficient storage, quick browsing, and retrieval of large collection of video data without losing important aspects. In this paper, we categorize video summariztion methods on the basis of methodology used, provide detailed description of leading methods in each category, and discuss their advantages and disadvantages. Moreover, we discuss the situation in which each method is most suitable to use. The advantage of this research is that one can quickly learn different video summarization techniques, and select the method that is the most suitable according to one’s requirements.</td>
</tr>
<tr id="rev_ajmal2012video" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video summarization. Citation - 78.</td>
</tr>
<tr id="bib_ajmal2012video" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{ajmal2012video,
  author = {Ajmal, Muhammad and Ashraf, Muhammad Husnain and Shakir, Muhammad and Abbas, Yasir and Shah, Faiz Ali},
  title = {Video summarization: techniques and classification},
  booktitle = {International Conference on Computer Vision and Graphics},
  year = {2012},
  pages = {1--13},
  url = {https://link.springer.com/chapter/10.1007/978-3-642-33564-8_1}
}
</pre></td>
</tr>
<tr id="alahi2012freak" class="entry">
	<td>Alahi, A., Ortiz, R. and Vandergheynst, P.</td>
	<td>Freak: Fast retina keypoint <p class="infolinks">[<a href="javascript:toggleInfo('alahi2012freak','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('alahi2012freak','comment')">Comment</a>] [<a href="javascript:toggleInfo('alahi2012freak','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>2012 IEEE Conference on Computer Vision and Pattern Recognition, pp. 510-517&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://infoscience.epfl.ch/record/175537/files/2069.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_alahi2012freak" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A large number of vision applications rely on matching keypoints across images. The last decade featured an arms-race towards faster and more robust keypoints and association algorithms: Scale Invariant Feature Transform (SIFT)[17], Speed-up Robust Feature (SURF)[4], and more recently Binary Robust Invariant Scalable Keypoints (BRISK)[I6] to name a few. These days, the deployment of vision algorithms on smart phones and embedded devices with low memory and computation complexity has even upped the ante: the goal is to make descriptors faster to compute, more compact while remaining robust to scale, rotation and noise. To best address the current requirements, we propose a novel keypoint descriptor inspired by the human visual system and more precisely the retina, coined Fast Retina Keypoint (FREAK). A cascade of binary strings is computed by efficiently comparing image intensities over a retinal sampling pattern. Our experiments show that FREAKs are in general faster to compute with lower memory load and also more robust than SIFT, SURF or BRISK. They are thus competitive alternatives to existing keypoints in particular for embedded applications.</td>
</tr>
<tr id="rev_alahi2012freak" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Visual tracking. CItations - 2114</td>
</tr>
<tr id="bib_alahi2012freak" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{alahi2012freak,
  author = {Alahi, Alexandre and Ortiz, Raphael and Vandergheynst, Pierre},
  title = {Freak: Fast retina keypoint},
  booktitle = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2012},
  pages = {510--517},
  url = {https://infoscience.epfl.ch/record/175537/files/2069.pdf}
}
</pre></td>
</tr>
<tr id="alahi2016social" class="entry">
	<td>Alahi, A., Goel, K., Ramanathan, V., Robicquet, A., Fei-Fei, L. and Savarese, S.</td>
	<td>Social lstm: Human trajectory prediction in crowded spaces <p class="infolinks">[<a href="javascript:toggleInfo('alahi2016social','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('alahi2016social','comment')">Comment</a>] [<a href="javascript:toggleInfo('alahi2016social','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 961-971&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Alahi_Social_LSTM_Human_CVPR_2016_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_alahi2016social" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Humans navigate complex crowded environments based on social conventions: they respect personal space, yielding right-of-way and avoid collisions. In our work, we propose a data-driven approach to learn these human-human interactions for predicting their future trajectories. This is in contrast to traditional approaches which use hand-crafted functions such as Social forces. We present a new Long Short-Term Memory (LSTM) model which jointly reasons across multiple individuals in a scene. Different from the conventional LSTM, we share the information between multiple LSTMs through a new pooling layer. This layer pools the hidden representation from LSTMs corresponding to neighboring trajectories to capture interactions within this neighborhood. We demonstrate the performance of our method on several public datasets. Our model outperforms previous forecasting methods by more than 42% . We also analyze the trajectories predicted by our model to demonstrate social behaviours such as collision avoidance and group movement, learned by our model.</td>
</tr>
<tr id="rev_alahi2016social" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Visual tracking. Citations - 967</td>
</tr>
<tr id="bib_alahi2016social" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{alahi2016social,
  author = {Alahi, Alexandre and Goel, Kratarth and Ramanathan, Vignesh and Robicquet, Alexandre and Fei-Fei, Li and Savarese, Silvio},
  title = {Social lstm: Human trajectory prediction in crowded spaces},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2016},
  pages = {961--971},
  url = {http://openaccess.thecvf.com/content_cvpr_2016/papers/Alahi_Social_LSTM_Human_CVPR_2016_paper.pdf}
}
</pre></td>
</tr>
<tr id="anderson2018bottom" class="entry">
	<td>Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S. and Zhang, L.</td>
	<td>Bottom-up and top-down attention for image captioning and visual question answering <p class="infolinks">[<a href="javascript:toggleInfo('anderson2018bottom','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('anderson2018bottom','comment')">Comment</a>] [<a href="javascript:toggleInfo('anderson2018bottom','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6077-6086&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_anderson2018bottom" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.</td>
</tr>
<tr id="rev_anderson2018bottom" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Mark R-CNN. Citations - 1235.</td>
</tr>
<tr id="bib_anderson2018bottom" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{anderson2018bottom,
  author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  title = {Bottom-up and top-down attention for image captioning and visual question answering},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2018},
  pages = {6077--6086},
  url = {http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf}
}
</pre></td>
</tr>
<tr id="Antol2015" class="entry">
	<td>Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C. and Parikh, D.</td>
	<td>Vqa: Visual question answering <p class="infolinks">[<a href="javascript:toggleInfo('Antol2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Antol2015','comment')">Comment</a>] [<a href="javascript:toggleInfo('Antol2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Proceedings of the IEEE international conference on computer vision, pp. 2425-2433&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Antol2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.</td>
</tr>
<tr id="rev_Antol2015" class="comment noshow">
	<td colspan="6"><b>Comment</b>: VQA dataset. Citations - 2048</td>
</tr>
<tr id="bib_Antol2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Antol2015,
  author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C and Parikh, Devi},
  title = {Vqa: Visual question answering},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  year = {2015},
  pages = {2425--2433},
  url = {http://openaccess.thecvf.com/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf}
}
</pre></td>
</tr>
<tr id="antol2015vqa" class="entry">
	<td>Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Lawrence Zitnick, C. and Parikh, D.</td>
	<td>Vqa: Visual question answering <p class="infolinks">[<a href="javascript:toggleInfo('antol2015vqa','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('antol2015vqa','comment')">Comment</a>] [<a href="javascript:toggleInfo('antol2015vqa','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Proceedings of the IEEE international conference on computer vision, pp. 2425-2433&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_antol2015vqa" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose the task offree-formandopen-endedVisualQuestion Answering (VQA). Given an image and a naturallanguage question about the image, the task is to provide anaccurate natural language answer. Mirroring real-worldscenarios, such as helping the visually impaired, both thequestions and answers are open-ended. Visual questions se-lectively target different areas of an image, including back-ground details and underlying context. As a result, a systemthat succeeds at VQA typically needs a more detailed un-derstanding of the image and complex reasoning than a sys-tem producing generic image captions. Moreover, VQA isamenable to automatic evaluation, since many open-endedanswers contain only a few words or a closed set of answersthat can be provided in a multiple-choice format. We pro-vide a dataset containing∼0.25M images,∼0.76M ques-tions, and∼10M answers (www.visualqa.org), anddiscuss the information it provides. Numerous baselines forVQA are provided and compared with human performance</td>
</tr>
<tr id="rev_antol2015vqa" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Question answering. Ciatations - 2070.</td>
</tr>
<tr id="bib_antol2015vqa" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{antol2015vqa,
  author = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C and Parikh, Devi},
  title = {Vqa: Visual question answering},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  year = {2015},
  pages = {2425--2433},
  url = {http://openaccess.thecvf.com/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf}
}
</pre></td>
</tr>
<tr id="Badrinarayanan2017" class="entry">
	<td>Badrinarayanan, V., Kendall, A. and Cipolla, R.</td>
	<td>Segnet: A deep convolutional encoder-decoder architecture for image segmentation <p class="infolinks">[<a href="javascript:toggleInfo('Badrinarayanan2017','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Badrinarayanan2017','comment')">Comment</a>] [<a href="javascript:toggleInfo('Badrinarayanan2017','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>IEEE transactions on pattern analysis and machine intelligence<br/>Vol. 39(12), pp. 2481-2495&nbsp;</td>
	<td>article</td>
	<td><a href="https://ieeexplore.ieee.org/iel7/34/4359286/07803544.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Badrinarayanan2017" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.</td>
</tr>
<tr id="rev_Badrinarayanan2017" class="comment noshow">
	<td colspan="6"><b>Comment</b>: SegNet. CItations - 5459</td>
</tr>
<tr id="bib_Badrinarayanan2017" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Badrinarayanan2017,
  author = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  title = {Segnet: A deep convolutional encoder-decoder architecture for image segmentation},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  publisher = {IEEE},
  year = {2017},
  volume = {39},
  number = {12},
  pages = {2481--2495},
  url = {https://ieeexplore.ieee.org/iel7/34/4359286/07803544.pdf}
}
</pre></td>
</tr>
<tr id="bahdanau2014neural" class="entry">
	<td>Bahdanau, D., Cho, K. and Bengio, Y.</td>
	<td>Neural machine translation by jointly learning to align and translate <p class="infolinks">[<a href="javascript:toggleInfo('bahdanau2014neural','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('bahdanau2014neural','comment')">Comment</a>] [<a href="javascript:toggleInfo('bahdanau2014neural','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>arXiv preprint arXiv:1409.0473&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1409.0473)">URL</a>&nbsp;</td>
</tr>
<tr id="abs_bahdanau2014neural" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree<br>well with our intuition.</td>
</tr>
<tr id="rev_bahdanau2014neural" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translations. Citations - 13991</td>
</tr>
<tr id="bib_bahdanau2014neural" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{bahdanau2014neural,
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  title = {Neural machine translation by jointly learning to align and translate},
  journal = {arXiv preprint arXiv:1409.0473},
  year = {2014},
  url = {https://arxiv.org/pdf/1409.0473)}
}
</pre></td>
</tr>
<tr id="bahdanau2016end" class="entry">
	<td>Bahdanau, D., Chorowski, J., Serdyuk, D., Brakel, P. and Bengio, Y.</td>
	<td>End-to-end attention-based large vocabulary speech recognition <p class="infolinks">[<a href="javascript:toggleInfo('bahdanau2016end','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('bahdanau2016end','comment')">Comment</a>] [<a href="javascript:toggleInfo('bahdanau2016end','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>2016 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 4945-4949&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://arxiv.org/pdf/1508.04395">URL</a>&nbsp;</td>
</tr>
<tr id="abs_bahdanau2016end" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and<br>the desired character sequence is learned automatically by an<br>attention mechanism built into the RNN. For each predicted<br>character, the attention mechanism scans the input sequence<br>and chooses relevant frames. We propose two methods to<br>speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.</td>
</tr>
<tr id="rev_bahdanau2016end" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Speech recognition. Citations - 776</td>
</tr>
<tr id="bib_bahdanau2016end" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{bahdanau2016end,
  author = {Bahdanau, Dzmitry and Chorowski, Jan and Serdyuk, Dmitriy and Brakel, Philemon and Bengio, Yoshua},
  title = {End-to-end attention-based large vocabulary speech recognition},
  booktitle = {2016 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  year = {2016},
  pages = {4945--4949},
  url = {https://arxiv.org/pdf/1508.04395}
}
</pre></td>
</tr>
<tr id="bell2016inside" class="entry">
	<td>Bell, S., Lawrence Zitnick, C., Bala, K. and Girshick, R.</td>
	<td>Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks <p class="infolinks">[<a href="javascript:toggleInfo('bell2016inside','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('bell2016inside','comment')">Comment</a>] [<a href="javascript:toggleInfo('bell2016inside','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 2874-2883&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_bell2016inside" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9% to 77.9% mAP. On the new and more challenging MS COCO dataset, we improve state-of-the-art from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection Challenge, our ION model won "Best Student Entry" and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.</td>
</tr>
<tr id="rev_bell2016inside" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Inside-outside net. Citations - 772</td>
</tr>
<tr id="bib_bell2016inside" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{bell2016inside,
  author = {Bell, Sean and Lawrence Zitnick, C and Bala, Kavita and Girshick, Ross},
  title = {Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2016},
  pages = {2874--2883},
  url = {http://openaccess.thecvf.com/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf}
}
</pre></td>
</tr>
<tr id="ben2017mutan" class="entry">
	<td>Ben-Younes, H., Cadene, R., Cord, M. and Thome, N.</td>
	<td>Mutan: Multimodal tucker fusion for visual question answering <p class="infolinks">[<a href="javascript:toggleInfo('ben2017mutan','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('ben2017mutan','comment')">Comment</a>] [<a href="javascript:toggleInfo('ben2017mutan','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the IEEE international conference on computer vision, pp. 2612-2620&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Ben-younes_MUTAN_Multimodal_Tucker_ICCV_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_ben2017mutan" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Bilinear models provide an appealing framework for mixing and merging information in Visual Question Answering (VQA) tasks. They help to learn high level associations between question meaning and visual concepts in the image, but they suffer from huge dimensionality issues. We introduce MUTAN, a multimodal tensor-based Tucker decomposition to efficiently parametrize bilinear interactions between visual and textual representations. Additionally to the Tucker framework, we design a low-rank matrix-based decomposition to explicitly constrain the interaction rank. With MUTAN, we control the complexity of the merging scheme while keeping nice interpretable fusion relations. We show how the Tucker decomposition framework generalizes some of the latest VQA architectures, providing state-of-the-art results.</td>
</tr>
<tr id="rev_ben2017mutan" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Visual question answering. Citations - 249</td>
</tr>
<tr id="bib_ben2017mutan" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{ben2017mutan,
  author = {Ben-Younes, Hedi and Cadene, R&eacute;mi and Cord, Matthieu and Thome, Nicolas},
  title = {Mutan: Multimodal tucker fusion for visual question answering},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  year = {2017},
  pages = {2612--2620},
  url = {http://openaccess.thecvf.com/content_ICCV_2017/papers/Ben-younes_MUTAN_Multimodal_Tucker_ICCV_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="bordes2016learning" class="entry">
	<td>Bordes, A., Boureau, Y.-L. and Weston, J.</td>
	<td>Learning end-to-end goal-oriented dialog <p class="infolinks">[<a href="javascript:toggleInfo('bordes2016learning','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('bordes2016learning','comment')">Comment</a>] [<a href="javascript:toggleInfo('bordes2016learning','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>arXiv preprint arXiv:1605.07683&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1605.07683">URL</a>&nbsp;</td>
</tr>
<tr id="abs_bordes2016learning" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Traditional dialog systems used in goal-oriented applications require a lot of domain-specific handcrafting, which hinders scaling up to new domains. End-to-end dialog systems, in which all components are trained from the dialogs themselves, escape this limitation. But the encouraging success recently obtained in chit-chat dialog may not carry over to goal-oriented settings. This paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. Set in the context of restaurant reservation, our tasks require manipulating sentences and symbols, so as to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). We show similar result patterns on data extracted from an online concierge service.</td>
</tr>
<tr id="rev_bordes2016learning" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Conversation model. Citations - 531</td>
</tr>
<tr id="bib_bordes2016learning" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{bordes2016learning,
  author = {Bordes, Antoine and Boureau, Y-Lan and Weston, Jason},
  title = {Learning end-to-end goal-oriented dialog},
  journal = {arXiv preprint arXiv:1605.07683},
  year = {2016},
  url = {https://arxiv.org/pdf/1605.07683}
}
</pre></td>
</tr>
<tr id="Chen2014" class="entry">
	<td>Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K. and Yuille, A.L.</td>
	<td>Semantic image segmentation with deep convolutional nets and fully connected crfs <p class="infolinks">[<a href="javascript:toggleInfo('Chen2014','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Chen2014','comment')">Comment</a>] [<a href="javascript:toggleInfo('Chen2014','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>arXiv preprint arXiv:1412.7062&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1412.7062">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Chen2014" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called ”semantic image segmentation”). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our “DeepLab” system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the ’hole’ algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.</td>
</tr>
<tr id="rev_Chen2014" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Fully connected network for segmentation. Citations - 2621</td>
</tr>
<tr id="bib_Chen2014" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Chen2014,
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  title = {Semantic image segmentation with deep convolutional nets and fully connected crfs},
  journal = {arXiv preprint arXiv:1412.7062},
  year = {2014},
  url = {https://arxiv.org/pdf/1412.7062}
}
</pre></td>
</tr>
<tr id="chen2016variational" class="entry">
	<td>Chen, X., Kingma, D.P., Salimans, T., Duan, Y., Dhariwal, P., Schulman, J., Sutskever, I. and Abbeel, P.</td>
	<td>Variational lossy autoencoder <p class="infolinks">[<a href="javascript:toggleInfo('chen2016variational','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('chen2016variational','comment')">Comment</a>] [<a href="javascript:toggleInfo('chen2016variational','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>arXiv preprint arXiv:1611.02731&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1611.02731.pdf,">URL</a>&nbsp;</td>
</tr>
<tr id="abs_chen2016variational" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only "autoencodes" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution p(z) and decoding distribution p(x|z), we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.</td>
</tr>
<tr id="rev_chen2016variational" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Variational autoencoders. Citations - 373.</td>
</tr>
<tr id="bib_chen2016variational" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{chen2016variational,
  author = {Chen, Xi and Kingma, Diederik P and Salimans, Tim and Duan, Yan and Dhariwal, Prafulla and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  title = {Variational lossy autoencoder},
  journal = {arXiv preprint arXiv:1611.02731},
  year = {2016},
  url = {https://arxiv.org/pdf/1611.02731.pdf,}
}
</pre></td>
</tr>
<tr id="Chen2017" class="entry">
	<td>Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K. and Yuille, A.L.</td>
	<td>Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs <p class="infolinks">[<a href="javascript:toggleInfo('Chen2017','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Chen2017','comment')">Comment</a>] [<a href="javascript:toggleInfo('Chen2017','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>IEEE transactions on pattern analysis and machine intelligence<br/>Vol. 40(4), pp. 834-848&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1606.00915">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Chen2017" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or `atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed “DeepLab” system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.</td>
</tr>
<tr id="rev_Chen2017" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Semantic image segementation. Citations - 5527</td>
</tr>
<tr id="bib_Chen2017" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Chen2017,
  author = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  title = {Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  publisher = {IEEE},
  year = {2017},
  volume = {40},
  number = {4},
  pages = {834--848},
  url = {https://arxiv.org/pdf/1606.00915}
}
</pre></td>
</tr>
<tr id="chen2017sca" class="entry">
	<td>Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W. and Chua, T.-S.</td>
	<td>Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning <p class="infolinks">[<a href="javascript:toggleInfo('chen2017sca','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('chen2017sca','comment')">Comment</a>] [<a href="javascript:toggleInfo('chen2017sca','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 5659-5667&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_chen2017sca" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism --- a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (i.e., attentive spatial locations at multiple layers) and what (i.e., attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods.</td>
</tr>
<tr id="rev_chen2017sca" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. Citations - 609.</td>
</tr>
<tr id="bib_chen2017sca" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{chen2017sca,
  author = {Chen, Long and Zhang, Hanwang and Xiao, Jun and Nie, Liqiang and Shao, Jian and Liu, Wei and Chua, Tat-Seng},
  title = {Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2017},
  pages = {5659--5667},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="chheng2007video" class="entry">
	<td>Chheng, T.</td>
	<td>Video summarization using clustering <p class="infolinks">[<a href="javascript:toggleInfo('chheng2007video','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('chheng2007video','comment')">Comment</a>] [<a href="javascript:toggleInfo('chheng2007video','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>Department of Computer Science University of California, Irvine&nbsp;</td>
	<td>article</td>
	<td><a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.7458&rep=rep1&type=pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_chheng2007video" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we approach the problem of video summarization. Wepropose an automated algorithm to identify the unique segments of avideo. The video segments are separated using the k-means clustering.We use Euclidean distance with histograms of the corresponding segmentas the distance metric. YouTube videos are used to test our procedure</td>
</tr>
<tr id="rev_chheng2007video" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video summarization. Citation - 11.</td>
</tr>
<tr id="bib_chheng2007video" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{chheng2007video,
  author = {Chheng, Tommy},
  title = {Video summarization using clustering},
  journal = {Department of Computer Science University of California, Irvine},
  publisher = {Citeseer},
  year = {2007},
  url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.7458&amp;rep=rep1&amp;type=pdf}
}
</pre></td>
</tr>
<tr id="cho2014learning" class="entry">
	<td>Cho, K., Van Merri&euml;nboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H. and Bengio, Y.</td>
	<td>Learning phrase representations using RNN encoder-decoder for statistical machine translation <p class="infolinks">[<a href="javascript:toggleInfo('cho2014learning','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('cho2014learning','comment')">Comment</a>] [<a href="javascript:toggleInfo('cho2014learning','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>arXiv preprint arXiv:1406.1078&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1406.1078.pdf?source=post_page---------------------------">URL</a>&nbsp;</td>
</tr>
<tr id="abs_cho2014learning" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we propose a novel neural network model called RNN Encoder– Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder–Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.</td>
</tr>
<tr id="rev_cho2014learning" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translation. Citations - 10610</td>
</tr>
<tr id="bib_cho2014learning" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{cho2014learning,
  author = {Cho, Kyunghyun and Van Merri&euml;nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  title = {Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  journal = {arXiv preprint arXiv:1406.1078},
  year = {2014},
  url = {https://arxiv.org/pdf/1406.1078.pdf?source=post_page---------------------------}
}
</pre></td>
</tr>
<tr id="cho2014properties" class="entry">
	<td>Cho, K., Van Merri&euml;nboer, B., Bahdanau, D. and Bengio, Y.</td>
	<td>On the properties of neural machine translation: Encoder-decoder approaches <p class="infolinks">[<a href="javascript:toggleInfo('cho2014properties','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('cho2014properties','comment')">Comment</a>] [<a href="javascript:toggleInfo('cho2014properties','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>arXiv preprint arXiv:1409.1259&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1409.1259">URL</a>&nbsp;</td>
</tr>
<tr id="abs_cho2014properties" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct<br>translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder–Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a<br>sentence automatically.</td>
</tr>
<tr id="rev_cho2014properties" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translation. CItations - 2924</td>
</tr>
<tr id="bib_cho2014properties" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{cho2014properties,
  author = {Cho, Kyunghyun and Van Merri&euml;nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  title = {On the properties of neural machine translation: Encoder-decoder approaches},
  journal = {arXiv preprint arXiv:1409.1259},
  year = {2014},
  url = {https://arxiv.org/pdf/1409.1259}
}
</pre></td>
</tr>
<tr id="cho2014properties" class="entry">
	<td>Cho, K., Van Merri&euml;nboer, B., Bahdanau, D. and Bengio, Y.</td>
	<td>On the properties of neural machine translation: Encoder-decoder approaches <p class="infolinks">[<a href="javascript:toggleInfo('cho2014properties','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('cho2014properties','comment')">Comment</a>] [<a href="javascript:toggleInfo('cho2014properties','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>arXiv preprint arXiv:1409.1259&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1409.1259">URL</a>&nbsp;</td>
</tr>
<tr id="abs_cho2014properties" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct<br>translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder–Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a<br>sentence automatically.</td>
</tr>
<tr id="rev_cho2014properties" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translations. Citations - 2924</td>
</tr>
<tr id="bib_cho2014properties" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{cho2014properties,
  author = {Cho, Kyunghyun and Van Merri&euml;nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  title = {On the properties of neural machine translation: Encoder-decoder approaches},
  journal = {arXiv preprint arXiv:1409.1259},
  year = {2014},
  url = {https://arxiv.org/pdf/1409.1259}
}
</pre></td>
</tr>
<tr id="choi2017attentional" class="entry">
	<td>Choi, J., Jin Chang, H., Yun, S., Fischer, T., Demiris, Y. and Young Choi, J.</td>
	<td>Attentional correlation filter network for adaptive visual tracking <p class="infolinks">[<a href="javascript:toggleInfo('choi2017attentional','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('choi2017attentional','comment')">Comment</a>] [<a href="javascript:toggleInfo('choi2017attentional','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4807-4816&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Choi_Attentional_Correlation_Filter_CVPR_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_choi2017attentional" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose a new tracking framework with an attentionalmechanism that chooses a subset of the associated corre-lation filters for increased robustness and computationalefficiency. The subset of filters is adaptively selected by adeep attentional network according to the dynamic proper-ties of the tracking target. Our contributions are manifold,and are summarised as follows: (i) Introducing the Atten-tional Correlation Filter Network which allows adaptivetracking of dynamic targets. (ii) Utilising an attentional net-work which shifts the attention to the best candidate modules,as well as predicting the estimated accuracy of currently in-active modules. (iii) Enlarging the variety of correlationfilters which cover target drift, blurriness, occlusion, scalechanges, and flexible aspect ratio. (iv) Validating the robust-ness and efficiency of the attentional mechanism for visualtracking through a number of experiments. Our methodachieves similar performance to non real-time trackers, andstate-of-the-art performance amongst real-time trackers.</td>
</tr>
<tr id="rev_choi2017attentional" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Visual tracking. Citations - 208</td>
</tr>
<tr id="bib_choi2017attentional" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{choi2017attentional,
  author = {Choi, Jongwon and Jin Chang, Hyung and Yun, Sangdoo and Fischer, Tobias and Demiris, Yiannis and Young Choi, Jin},
  title = {Attentional correlation filter network for adaptive visual tracking},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2017},
  pages = {4807--4816},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Choi_Attentional_Correlation_Filter_CVPR_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="choi2018context" class="entry">
	<td>Choi, J., Jin Chang, H., Fischer, T., Yun, S., Lee, K., Jeong, J., Demiris, Y. and Young Choi, J.</td>
	<td>Context-aware deep feature compression for high-speed visual tracking <p class="infolinks">[<a href="javascript:toggleInfo('choi2018context','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('choi2018context','comment')">Comment</a>] [<a href="javascript:toggleInfo('choi2018context','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 479-488&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_Context-Aware_Deep_Feature_CVPR_2018_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_choi2018context" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose a new context-aware correlation filter basedtracking framework to achieve both high computationalspeed and state-of-the-art performance among real-timetrackers. The major contribution to the high computationalspeed lies in the proposed deep feature compression thatis achieved by a context-aware scheme utilizing multipleexpert auto-encoders; a context in our framework refersto the coarse category of the tracking target according toappearance patterns. In the pre-training phase, one expertauto-encoder is trained per category. In the tracking phase,the best expert auto-encoder is selected for a given target,and only this auto-encoder is used. To achieve high trackingperformance with the compressed feature map, we intro-duce extrinsic denoising processes and a new orthogonalityloss term for pre-training and fine-tuning of the expert auto-encoders. We validate the proposed context-aware frame-work through a number of experiments, where our methodachieves a comparable performance to state-of-the-art track-ers which cannot run in real-time, while running at a signifi-cantly fast speed of over 100 fps</td>
</tr>
<tr id="rev_choi2018context" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Visual tracking. Citations - 101.</td>
</tr>
<tr id="bib_choi2018context" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{choi2018context,
  author = {Choi, Jongwon and Jin Chang, Hyung and Fischer, Tobias and Yun, Sangdoo and Lee, Kyuewang and Jeong, Jiyeoup and Demiris, Yiannis and Young Choi, Jin},
  title = {Context-aware deep feature compression for high-speed visual tracking},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2018},
  pages = {479--488},
  url = {http://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_Context-Aware_Deep_Feature_CVPR_2018_paper.pdf}
}
</pre></td>
</tr>
<tr id="chorowski2015attention" class="entry">
	<td>Chorowski, J.K., Bahdanau, D., Serdyuk, D., Cho, K. and Bengio, Y.</td>
	<td>Attention-based models for speech recognition <p class="infolinks">[<a href="javascript:toggleInfo('chorowski2015attention','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('chorowski2015attention','comment')">Comment</a>] [<a href="javascript:toggleInfo('chorowski2015attention','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Advances in neural information processing systems, pp. 577-585&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_chorowski2015attention" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness<br>to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames,<br>which further reduces PER to 17.6% level.</td>
</tr>
<tr id="rev_chorowski2015attention" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Speech recognition. Citations - 1354</td>
</tr>
<tr id="bib_chorowski2015attention" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{chorowski2015attention,
  author = {Chorowski, Jan K and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
  title = {Attention-based models for speech recognition},
  booktitle = {Advances in neural information processing systems},
  year = {2015},
  pages = {577--585},
  url = {http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf}
}
</pre></td>
</tr>
<tr id="chung2014empirical" class="entry">
	<td>Chung, J., Gulcehre, C., Cho, K. and Bengio, Y.</td>
	<td>Empirical evaluation of gated recurrent neural networks on sequence modeling <p class="infolinks">[<a href="javascript:toggleInfo('chung2014empirical','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('chung2014empirical','comment')">Comment</a>] [<a href="javascript:toggleInfo('chung2014empirical','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>arXiv preprint arXiv:1412.3555&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1412.3555.pdf?ref=hackernoon.com">URL</a>&nbsp;</td>
</tr>
<tr id="abs_chung2014empirical" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper we compare different types of recurrent units in recurrent neural net-works (RNNs).  Especially, we focus on more sophisticated units that implementa gating mechanism, such as a long short-term memory (LSTM) unit and a re-cently proposed gated recurrent unit (GRU). We evaluate these recurrent units onthe tasks of polyphonic music modeling and speech signal modeling.  Our exper-iments revealed that these advanced recurrent units are indeed better than moretraditional recurrent units such astanhunits. Also, we found GRU to be compa-rable to LSTM.</td>
</tr>
<tr id="rev_chung2014empirical" class="comment noshow">
	<td colspan="6"><b>Comment</b>: gated RNN. Citations - 5072</td>
</tr>
<tr id="bib_chung2014empirical" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{chung2014empirical,
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  title = {Empirical evaluation of gated recurrent neural networks on sequence modeling},
  journal = {arXiv preprint arXiv:1412.3555},
  year = {2014},
  url = {https://arxiv.org/pdf/1412.3555.pdf?ref=hackernoon.com}
}
</pre></td>
</tr>
<tr id="chung2014empirical" class="entry">
	<td>Chung, J., Gulcehre, C., Cho, K. and Bengio, Y.</td>
	<td>Empirical evaluation of gated recurrent neural networks on sequence modeling <p class="infolinks">[<a href="javascript:toggleInfo('chung2014empirical','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('chung2014empirical','comment')">Comment</a>] [<a href="javascript:toggleInfo('chung2014empirical','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>arXiv preprint arXiv:1412.3555&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1412.3555.pdf?ref=hackernoon.com">URL</a>&nbsp;</td>
</tr>
<tr id="abs_chung2014empirical" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on<br>the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.</td>
</tr>
<tr id="rev_chung2014empirical" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translation. Citations - 5147</td>
</tr>
<tr id="bib_chung2014empirical" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{chung2014empirical,
  author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  title = {Empirical evaluation of gated recurrent neural networks on sequence modeling},
  journal = {arXiv preprint arXiv:1412.3555},
  year = {2014},
  url = {https://arxiv.org/pdf/1412.3555.pdf?ref=hackernoon.com}
}
</pre></td>
</tr>
<tr id="ciregan2012multi" class="entry">
	<td>Ciregan, D., Meier, U. and Schmidhuber, J.</td>
	<td>Multi-column deep neural networks for image classification <p class="infolinks">[<a href="javascript:toggleInfo('ciregan2012multi','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('ciregan2012multi','comment')">Comment</a>] [<a href="javascript:toggleInfo('ciregan2012multi','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>2012 IEEE conference on computer vision and pattern recognition, pp. 3642-3649&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://ieeexplore.ieee.org/abstract/document/6248110">URL</a>&nbsp;</td>
</tr>
<tr id="abs_ciregan2012multi" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.</td>
</tr>
<tr id="rev_ciregan2012multi" class="comment noshow">
	<td colspan="6"><b>Comment</b>: DNN for image classification. Citations - 3379.</td>
</tr>
<tr id="bib_ciregan2012multi" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{ciregan2012multi,
  author = {Ciregan, Dan and Meier, Ueli and Schmidhuber, J&uuml;rgen},
  title = {Multi-column deep neural networks for image classification},
  booktitle = {2012 IEEE conference on computer vision and pattern recognition},
  year = {2012},
  pages = {3642--3649},
  url = {https://ieeexplore.ieee.org/abstract/document/6248110}
}
</pre></td>
</tr>
<tr id="Cordts2016" class="entry">
	<td>Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R., Franke, U., Roth, S. and Schiele, B.</td>
	<td>The cityscapes dataset for semantic urban scene understanding <p class="infolinks">[<a href="javascript:toggleInfo('Cordts2016','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Cordts2016','comment')">Comment</a>] [<a href="javascript:toggleInfo('Cordts2016','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3213-3223&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Cordts2016" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark</td>
</tr>
<tr id="rev_Cordts2016" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Cityscapes dataset. CItations - 3405</td>
</tr>
<tr id="bib_Cordts2016" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Cordts2016,
  author = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  title = {The cityscapes dataset for semantic urban scene understanding},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2016},
  pages = {3213--3223},
  url = {http://openaccess.thecvf.com/content_cvpr_2016/papers/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.pdf}
}
</pre></td>
</tr>
<tr id="dai2016r" class="entry">
	<td>Dai, J., Li, Y., He, K. and Sun, J.</td>
	<td>R-fcn: Object detection via region-based fully convolutional networks <p class="infolinks">[<a href="javascript:toggleInfo('dai2016r','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('dai2016r','comment')">Comment</a>] [<a href="javascript:toggleInfo('dai2016r','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Advances in neural information processing systems, pp. 379-387&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://papers.nips.cc/paper/6465-r-fcn-object-detection-via-region-based-fully-convolutional-networks.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_dai2016r" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20 times faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn.</td>
</tr>
<tr id="rev_dai2016r" class="comment noshow">
	<td colspan="6"><b>Comment</b>: R-FCN. Citations - 2907</td>
</tr>
<tr id="bib_dai2016r" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{dai2016r,
  author = {Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
  title = {R-fcn: Object detection via region-based fully convolutional networks},
  booktitle = {Advances in neural information processing systems},
  year = {2016},
  pages = {379--387},
  url = {https://papers.nips.cc/paper/6465-r-fcn-object-detection-via-region-based-fully-convolutional-networks.pdf}
}
</pre></td>
</tr>
<tr id="Deng2009" class="entry">
	<td>Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K. and Fei-Fei, L.</td>
	<td>Imagenet: A large-scale hierarchical image database <p class="infolinks">[<a href="javascript:toggleInfo('Deng2009','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Deng2009','comment')">Comment</a>] [<a href="javascript:toggleInfo('Deng2009','bibtex')">BibTeX</a>]</p></td>
	<td>2009</td>
	<td>2009 IEEE conference on computer vision and pattern recognition, pp. 248-255&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://www.researchgate.net/profile/Li_Jia_Li/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database/links/00b495388120dbc339000000/ImageNet-a-Large-Scale-Hierarchical-Image-Database.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Deng2009" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500- 1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.</td>
</tr>
<tr id="rev_Deng2009" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Imagenet. Citations - 20648</td>
</tr>
<tr id="bib_Deng2009" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Deng2009,
  author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  title = {Imagenet: A large-scale hierarchical image database},
  booktitle = {2009 IEEE conference on computer vision and pattern recognition},
  year = {2009},
  pages = {248--255},
  url = {https://www.researchgate.net/profile/Li_Jia_Li/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database/links/00b495388120dbc339000000/ImageNet-a-Large-Scale-Hierarchical-Image-Database.pdf}
}
</pre></td>
</tr>
<tr id="devlin2015language" class="entry">
	<td>Devlin, J., Cheng, H., Fang, H., Gupta, S., Deng, L., He, X., Zweig, G. and Mitchell, M.</td>
	<td>Language models for image captioning: The quirks and what works <p class="infolinks">[<a href="javascript:toggleInfo('devlin2015language','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('devlin2015language','comment')">Comment</a>] [<a href="javascript:toggleInfo('devlin2015language','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>arXiv preprint arXiv:1505.01809&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1505.01809">URL</a>&nbsp;</td>
</tr>
<tr id="abs_devlin2015language" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.</td>
</tr>
<tr id="rev_devlin2015language" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. Citations - 216.</td>
</tr>
<tr id="bib_devlin2015language" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{devlin2015language,
  author = {Devlin, Jacob and Cheng, Hao and Fang, Hao and Gupta, Saurabh and Deng, Li and He, Xiaodong and Zweig, Geoffrey and Mitchell, Margaret},
  title = {Language models for image captioning: The quirks and what works},
  journal = {arXiv preprint arXiv:1505.01809},
  year = {2015},
  url = {https://arxiv.org/pdf/1505.01809}
}
</pre></td>
</tr>
<tr id="doersch2016tutorial" class="entry">
	<td>Doersch, C.</td>
	<td>Tutorial on variational autoencoders <p class="infolinks">[<a href="javascript:toggleInfo('doersch2016tutorial','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('doersch2016tutorial','comment')">Comment</a>] [<a href="javascript:toggleInfo('doersch2016tutorial','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>arXiv preprint arXiv:1606.05908&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1606.05908.pdf http://arxiv.org/abs/1606.05908">URL</a>&nbsp;</td>
</tr>
<tr id="abs_doersch2016tutorial" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.</td>
</tr>
<tr id="rev_doersch2016tutorial" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Variational autoencoders. Citations - 752.</td>
</tr>
<tr id="bib_doersch2016tutorial" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{doersch2016tutorial,
  author = {Doersch, Carl},
  title = {Tutorial on variational autoencoders},
  journal = {arXiv preprint arXiv:1606.05908},
  year = {2016},
  url = {https://arxiv.org/pdf/1606.05908.pdf http://arxiv.org/abs/1606.05908}
}
</pre></td>
</tr>
<tr id="Drachen2014" class="entry">
	<td>Drachen, A., Yancey, M., Maguire, J., Chu, D., Wang, I.Y., Mahlmann, T., Schubert, M. and Klabajan, D.</td>
	<td>Skill-based differences in spatio-temporal team behaviour in defence of the ancients 2 (dota 2) <p class="infolinks">[<a href="javascript:toggleInfo('Drachen2014','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Drachen2014','comment')">Comment</a>] [<a href="javascript:toggleInfo('Drachen2014','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>2014 IEEE Games Media Entertainment, pp. 1-8&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://www.academia.edu/download/57772177/1603.07738.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Drachen2014" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Multiplayer Online Battle Arena (MOBA) games are among the most played digital games in the world. In these games, teams of players fight against each other in arena environments, and the gameplay is focused on tactical combat. Mastering MOBAs requires extensive practice, as is exemplified in the popular MOBA Defence of the Ancients 2 (DotA 2). In this paper, we present three data-driven measures of spatio-temporal behavior in DotA 2: 1) Zone changes; 2) Distribution of team members and: 3) Time series clustering via a fuzzy approach. We present a method for obtaining accurate positional data from DotA 2. We investigate how behavior varies across these measures as a function of the skill level of teams, using four tiers from novice to professional players. Results indicate that spatio-temporal behavior of MOBA teams is related to team skill, with professional teams having smaller within-team distances and conducting more zone changes than amateur teams. The temporal distribution of the within-team distances of professional and high-skilled teams also generally follows patterns distinct from lower skill ranks.</td>
</tr>
<tr id="rev_Drachen2014" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Defence of ancients. Citations - 77</td>
</tr>
<tr id="bib_Drachen2014" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Drachen2014,
  author = {Drachen, Anders and Yancey, Matthew and Maguire, John and Chu, Derrek and Wang, Iris Yuhui and Mahlmann, Tobias and Schubert, Matthias and Klabajan, Diego},
  title = {Skill-based differences in spatio-temporal team behaviour in defence of the ancients 2 (dota 2)},
  booktitle = {2014 IEEE Games Media Entertainment},
  year = {2014},
  pages = {1--8},
  url = {http://www.academia.edu/download/57772177/1603.07738.pdf}
}
</pre></td>
</tr>
<tr id="engel2017neural" class="entry">
	<td>Engel, J., Resnick, C., Roberts, A., Dieleman, S., Norouzi, M., Eck, D. and Simonyan, K.</td>
	<td>Neural audio synthesis of musical notes with wavenet autoencoders <p class="infolinks">[<a href="javascript:toggleInfo('engel2017neural','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('engel2017neural','comment')">Comment</a>] [<a href="javascript:toggleInfo('engel2017neural','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>International Conference on Machine Learning, pp. 1068-1077&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://proceedings.mlr.press/v70/engel17a.html">URL</a>&nbsp;</td>
</tr>
<tr id="abs_engel2017neural" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Generative  models  in  vision  have  seen  rapidprogress  due  to  algorithmic  improvements  andthe availability of high-quality image datasets. Inthis paper, we offer contributions in both these ar-eas to enable similar progress in audio modeling.First,  we  detail  a  powerful  new  WaveNet-styleautoencoder model that conditions an autoregres-sive  decoder  on  temporal  codes  learned  fromthe raw audio waveform.  Second, we introduceNSynth, a large-scale and high-quality dataset ofmusical notes that is an order of magnitude largerthan comparable public datasets.  Using NSynth,we demonstrate improved qualitative and quanti-tative performance of the WaveNet autoencoderover a well-tuned spectral autoencoder baseline.Finally,  we show that the model learns a mani-fold of embeddings that allows for morphing be-tween instruments, meaningfully interpolating intimbre to create new types of sounds that are re-alistic and expressive.</td>
</tr>
<tr id="rev_engel2017neural" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Wavenet autoencoder. Citations - 240.</td>
</tr>
<tr id="bib_engel2017neural" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{engel2017neural,
  author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Norouzi, Mohammad and Eck, Douglas and Simonyan, Karen},
  title = {Neural audio synthesis of musical notes with wavenet autoencoders},
  booktitle = {International Conference on Machine Learning},
  year = {2017},
  pages = {1068--1077},
  url = {http://proceedings.mlr.press/v70/engel17a.html}
}
</pre></td>
</tr>
<tr id="farlow1984self" class="entry">
	<td>Farlow, S.J.</td>
	<td>Self-organizing methods in modeling: GMDH type algorithms <p class="infolinks">[<a href="javascript:toggleInfo('farlow1984self','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('farlow1984self','comment')">Comment</a>] [<a href="javascript:toggleInfo('farlow1984self','bibtex')">BibTeX</a>]</p></td>
	<td>1984</td>
	<td><br/>Vol. 54&nbsp;</td>
	<td>book</td>
	<td><a href="https://books.google.com/books?hl=en&lr=&id=G2_4Eu6hdQcC&oi=fnd&pg=PR7&dq=Cybernetics predicting device&ots=2XHmPQIyel&sig=vGmX-fdtZCFU7DCl2o_juDvkOFY">URL</a>&nbsp;</td>
</tr>
<tr id="abs_farlow1984self" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Cybernetics, Self-Organizing Systems with Positive Feedback, Cybernetic Predicting Devices,<br>Cybernetic Systems with … D. Director, Institute of Cybernetics, Ukrainian Academy of Sciences,<br>Kiev … SEARCH FOR STRUCTURE James M. Malone II PREDICTED SQUARED ERROR</td>
</tr>
<tr id="rev_farlow1984self" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Cybernaetics. Citations - 927</td>
</tr>
<tr id="bib_farlow1984self" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@book{farlow1984self,
  author = {Farlow, Stanley J},
  title = {Self-organizing methods in modeling: GMDH type algorithms},
  publisher = {CrC Press},
  year = {1984},
  volume = {54},
  url = {https://books.google.com/books?hl=en&amp;lr=&amp;id=G2_4Eu6hdQcC&amp;oi=fnd&amp;pg=PR7&amp;dq=Cybernetics predicting device&amp;ots=2XHmPQIyel&amp;sig=vGmX-fdtZCFU7DCl2o_juDvkOFY}
}
</pre></td>
</tr>
<tr id="firat2016multi" class="entry">
	<td>Firat, O., Cho, K. and Bengio, Y.</td>
	<td>Multi-way, multilingual neural machine translation with a shared attention mechanism <p class="infolinks">[<a href="javascript:toggleInfo('firat2016multi','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('firat2016multi','comment')">Comment</a>] [<a href="javascript:toggleInfo('firat2016multi','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>arXiv preprint arXiv:1601.01073&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1601.01073">URL</a>&nbsp;</td>
</tr>
<tr id="abs_firat2016multi" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to<br>translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multiway, multilingual model on ten language pairs from WMT’15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.</td>
</tr>
<tr id="rev_firat2016multi" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translation. Citations - 329</td>
</tr>
<tr id="bib_firat2016multi" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{firat2016multi,
  author = {Firat, Orhan and Cho, Kyunghyun and Bengio, Yoshua},
  title = {Multi-way, multilingual neural machine translation with a shared attention mechanism},
  journal = {arXiv preprint arXiv:1601.01073},
  year = {2016},
  url = {https://arxiv.org/pdf/1601.01073}
}
</pre></td>
</tr>
<tr id="firat2016zero" class="entry">
	<td>Firat, O., Sankaran, B., Al-Onaizan, Y., Vural, F.T.Y. and Cho, K.</td>
	<td>Zero-resource translation with multi-lingual neural machine translation <p class="infolinks">[<a href="javascript:toggleInfo('firat2016zero','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('firat2016zero','comment')">Comment</a>] [<a href="javascript:toggleInfo('firat2016zero','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>arXiv preprint arXiv:1606.04164&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1606.04164">URL</a>&nbsp;</td>
</tr>
<tr id="abs_firat2016zero" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we propose a novel finetuning algorithm for the recently introduced multiway, mulitlingual neural machine translate<br>that enables zero-resource machine translation. When used together with novel manyto-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a<br>single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivotbased translation strategy, while keeping only one additional copy of attention-related parameters.</td>
</tr>
<tr id="rev_firat2016zero" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translation. Citations - 128</td>
</tr>
<tr id="bib_firat2016zero" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{firat2016zero,
  author = {Firat, Orhan and Sankaran, Baskaran and Al-Onaizan, Yaser and Vural, Fatos T Yarman and Cho, Kyunghyun},
  title = {Zero-resource translation with multi-lingual neural machine translation},
  journal = {arXiv preprint arXiv:1606.04164},
  year = {2016},
  url = {https://arxiv.org/pdf/1606.04164}
}
</pre></td>
</tr>
<tr id="Fu2018" class="entry">
	<td>Fu, L., Feng, Y., Majeed, Y., Zhang, X., Zhang, J., Karkee, M. and Zhang, Q.</td>
	<td>Kiwifruit detection in field images using Faster R-CNN with ZFNet <p class="infolinks">[<a href="javascript:toggleInfo('Fu2018','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Fu2018','comment')">Comment</a>] [<a href="javascript:toggleInfo('Fu2018','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>IFAC-PapersOnLine<br/>Vol. 51(17), pp. 45-50&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.researchgate.net/profile/Longsheng_Fu/publication/327616829_Kiwifruit_detection_in_field_images_using_Faster_R-CNN_with_ZFNet/links/5b9ffaf745851574f7d25805/Kiwifruit-detection-in-field-images-using-Faster-R-CNN-with-ZFNet.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Fu2018" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A kiwifruit detection system for field images was developed based on the deep convolutional neural network, which has a good robustness against the subjectivity and limitation of the<br>features selected artificially. Under different lighting conditions, 2,100 sub-images with 784×</td>
</tr>
<tr id="rev_Fu2018" class="comment noshow">
	<td colspan="6"><b>Comment</b>: ZFNet. Citations - 18</td>
</tr>
<tr id="bib_Fu2018" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Fu2018,
  author = {Fu, Longsheng and Feng, Yali and Majeed, Yaqoob and Zhang, Xin and Zhang, Jing and Karkee, Manoj and Zhang, Qin},
  title = {Kiwifruit detection in field images using Faster R-CNN with ZFNet},
  journal = {IFAC-PapersOnLine},
  publisher = {Elsevier},
  year = {2018},
  volume = {51},
  number = {17},
  pages = {45--50},
  url = {https://www.researchgate.net/profile/Longsheng_Fu/publication/327616829_Kiwifruit_detection_in_field_images_using_Faster_R-CNN_with_ZFNet/links/5b9ffaf745851574f7d25805/Kiwifruit-detection-in-field-images-using-Faster-R-CNN-with-ZFNet.pdf}
}
</pre></td>
</tr>
<tr id="fukui2016multimodal" class="entry">
	<td>Fukui, A., Park, D.H., Yang, D., Rohrbach, A., Darrell, T. and Rohrbach, M.</td>
	<td>Multimodal compact bilinear pooling for visual question answering and visual grounding <p class="infolinks">[<a href="javascript:toggleInfo('fukui2016multimodal','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('fukui2016multimodal','comment')">Comment</a>] [<a href="javascript:toggleInfo('fukui2016multimodal','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>arXiv preprint arXiv:1606.01847&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1606.01847?source=post_page---------------------------">URL</a>&nbsp;</td>
</tr>
<tr id="abs_fukui2016multimodal" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.</td>
</tr>
<tr id="rev_fukui2016multimodal" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Visual question answering. Citations - 785</td>
</tr>
<tr id="bib_fukui2016multimodal" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{fukui2016multimodal,
  author = {Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},
  title = {Multimodal compact bilinear pooling for visual question answering and visual grounding},
  journal = {arXiv preprint arXiv:1606.01847},
  year = {2016},
  url = {https://arxiv.org/pdf/1606.01847?source=post_page---------------------------}
}
</pre></td>
</tr>
<tr id="Fukushima2007" class="entry">
	<td>Fukushima, K.</td>
	<td>Neocognitron <p class="infolinks">[<a href="javascript:toggleInfo('Fukushima2007','comment')">Comment</a>] [<a href="javascript:toggleInfo('Fukushima2007','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>Scholarpedia<br/>Vol. 2(1), pp. 1717&nbsp;</td>
	<td>article</td>
	<td><a href="http://var.scholarpedia.org/article/Neocognitron">URL</a>&nbsp;</td>
</tr>
<tr id="rev_Fukushima2007" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Citation - 147</td>
</tr>
<tr id="bib_Fukushima2007" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Fukushima2007,
  author = {Fukushima, Kunihiko},
  title = {Neocognitron},
  journal = {Scholarpedia},
  year = {2007},
  volume = {2},
  number = {1},
  pages = {1717},
  url = {http://var.scholarpedia.org/article/Neocognitron}
}
</pre></td>
</tr>
<tr id="galison1994ontology" class="entry">
	<td>Galison, P.</td>
	<td>The ontology of the enemy: Norbert Wiener and the cybernetic vision <p class="infolinks">[<a href="javascript:toggleInfo('galison1994ontology','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('galison1994ontology','comment')">Comment</a>] [<a href="javascript:toggleInfo('galison1994ontology','bibtex')">BibTeX</a>]</p></td>
	<td>1994</td>
	<td>Critical inquiry<br/>Vol. 21(1), pp. 228-266&nbsp;</td>
	<td>article</td>
	<td><a href="http://jerome-segal.de/Galison94.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_galison1994ontology" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The servomechanical enemy became, in the cybernetic vision of the 1940s, the prototype for<br>human … Cybernetics no longer appears as a futuristic bandwagon or as a ris- ing worldview that<br>will … would have to be filtered to gain a smoother curve that the predicting circuit could</td>
</tr>
<tr id="rev_galison1994ontology" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Cybernetic vision. Citations - 607.</td>
</tr>
<tr id="bib_galison1994ontology" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{galison1994ontology,
  author = {Galison, Peter},
  title = {The ontology of the enemy: Norbert Wiener and the cybernetic vision},
  journal = {Critical inquiry},
  publisher = {University of Chicago Press},
  year = {1994},
  volume = {21},
  number = {1},
  pages = {228--266},
  url = {http://jerome-segal.de/Galison94.pdf}
}
</pre></td>
</tr>
<tr id="gao2017video" class="entry">
	<td>Gao, L., Guo, Z., Zhang, H., Xu, X. and Shen, H.T.</td>
	<td>Video captioning with attention-based LSTM and semantic consistency <p class="infolinks">[<a href="javascript:toggleInfo('gao2017video','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('gao2017video','comment')">Comment</a>] [<a href="javascript:toggleInfo('gao2017video','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>IEEE Transactions on Multimedia<br/>Vol. 19(9), pp. 2045-2055&nbsp;</td>
	<td>article</td>
	<td><a href="https://ieeexplore.ieee.org/abstract/document/7984828">URL</a>&nbsp;</td>
</tr>
<tr id="abs_gao2017video" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recent progress in using long short-term memory (LSTM) for image captioning has motivated the exploration of their applications for video captioning. By taking a video as a sequence of features, an LSTM model is trained on video-sentence pairs and learns to associate a video to a sentence. However, most existing methods compress an entire video shot or frame into a static representation, without considering attention mechanism which allows for selecting salient features. Furthermore, existing approaches usually model the translating error, but ignore the correlations between sentence semantics and visual content. To tackle these issues, we propose a novel end-to-end framework named aLSTMs, an attention-based LSTM model with semantic consistency, to transfer videos to natural sentences. This framework integrates attention mechanism with LSTM to capture salient structures of video, and explores the correlation between multimodal representations (i.e., words and visual content) for generating sentences with rich semantic content. Specifically, we first propose an attention mechanism that uses the dynamic weighted sum of local two-dimensional convolutional neural network representations. Then, an LSTM decoder takes these visual features at time t and the word-embedding feature at time t-1 to generate important words. Finally, we use multimodal embedding to map the visual and sentence features into a joint space to guarantee the semantic consistence of the sentence description and the video visual content. Experiments on the benchmark datasets demonstrate that our method using single feature can achieve competitive or even better results than the state-of-the-art baselines for video captioning in both BLEU and METEOR.</td>
</tr>
<tr id="rev_gao2017video" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video captioning. Citations - 264</td>
</tr>
<tr id="bib_gao2017video" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{gao2017video,
  author = {Gao, Lianli and Guo, Zhao and Zhang, Hanwang and Xu, Xing and Shen, Heng Tao},
  title = {Video captioning with attention-based LSTM and semantic consistency},
  journal = {IEEE Transactions on Multimedia},
  publisher = {IEEE},
  year = {2017},
  volume = {19},
  number = {9},
  pages = {2045--2055},
  url = {https://ieeexplore.ieee.org/abstract/document/7984828}
}
</pre></td>
</tr>
<tr id="ghazvininejad2017knowledge" class="entry">
	<td>Ghazvininejad, M., Brockett, C., Chang, M.-W., Dolan, B., Gao, J., Yih, W.-t. and Galley, M.</td>
	<td>A knowledge-grounded neural conversation model <p class="infolinks">[<a href="javascript:toggleInfo('ghazvininejad2017knowledge','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('ghazvininejad2017knowledge','comment')">Comment</a>] [<a href="javascript:toggleInfo('ghazvininejad2017knowledge','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>arXiv preprint arXiv:1702.01932&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1702.01932">URL</a>&nbsp;</td>
</tr>
<tr id="abs_ghazvininejad2017knowledge" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Neural network models are capable of generating extremely natural sounding conversational interactions. Nevertheless, these models have yet to demonstrate that they can incorporate content in the form of factual information or entity-grounded opinion that would enable them to serve in more task-oriented conversational applications. This paper presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. We generalize the widely-used Seq2Seq approach by conditioning responses on both conversation history and external "facts", allowing the model to be versatile and applicable in an open-domain setting. Our approach yields significant improvements over a competitive Seq2Seq baseline. Human judges found that our outputs are significantly more informative.</td>
</tr>
<tr id="rev_ghazvininejad2017knowledge" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Conversation model. Citations - 223</td>
</tr>
<tr id="bib_ghazvininejad2017knowledge" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{ghazvininejad2017knowledge,
  author = {Ghazvininejad, Marjan and Brockett, Chris and Chang, Ming-Wei and Dolan, Bill and Gao, Jianfeng and Yih, Wen-tau and Galley, Michel},
  title = {A knowledge-grounded neural conversation model},
  journal = {arXiv preprint arXiv:1702.01932},
  year = {2017},
  url = {https://arxiv.org/pdf/1702.01932}
}
</pre></td>
</tr>
<tr id="giusti2013fast" class="entry">
	<td>Giusti, A., Cire&scedil;an, D.C., Masci, J., Gambardella, L.M. and Schmidhuber, J.</td>
	<td>Fast image scanning with deep max-pooling convolutional neural networks <p class="infolinks">[<a href="javascript:toggleInfo('giusti2013fast','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('giusti2013fast','comment')">Comment</a>] [<a href="javascript:toggleInfo('giusti2013fast','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>2013 IEEE International Conference on Image Processing, pp. 4034-4038&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="@inproceedings{giusti2013fast,
  title={Fast image scanning with deep max-pooling convolutional neural networks},
  author={Giusti, Alessandro and Cire{\c{s}}an, Dan C and Masci, Jonathan and Gambardella, Luca M and Schmidhuber, J{\"u}rgen},
  booktitle={2013 IEEE International Conference on Image Processing},
  pages={4034--4038},
  year={2013},
  organization={IEEE}
}">URL</a>&nbsp;</td>
</tr>
<tr id="abs_giusti2013fast" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Deep Neural Networks now excel at image classification, detection and segmentation. When used to scan images by means of a sliding window, however, their high computational complexity can bring even the most powerful hardware to its knees. We show how dynamic programming can speedup the process by orders of magnitude, even when max-pooling layers are present.</td>
</tr>
<tr id="rev_giusti2013fast" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Max-pooling CNN. Citations - 249.</td>
</tr>
<tr id="bib_giusti2013fast" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{giusti2013fast,
  author = {Giusti, Alessandro and Cire&scedil;an, Dan C and Masci, Jonathan and Gambardella, Luca M and Schmidhuber, J&uuml;rgen},
  title = {Fast image scanning with deep max-pooling convolutional neural networks},
  booktitle = {2013 IEEE International Conference on Image Processing},
  year = {2013},
  pages = {4034--4038},
  url = {@inproceedingsgiusti2013fast,<br>  title=Fast image scanning with deep max-pooling convolutional neural networks,<br>  author=Giusti, Alessandro and Cire&scedil;an, Dan C and Masci, Jonathan and Gambardella, Luca M and Schmidhuber, J&uuml;rgen,<br>  booktitle=2013 IEEE International Conference on Image Processing,<br>  pages=4034--4038,<br>  year=2013,<br>  organization=IEEE<br>}
}
</pre></td>
</tr>
<tr id="goodfellow2016nips" class="entry">
	<td>Goodfellow, I.</td>
	<td>NIPS 2016 tutorial: Generative adversarial networks <p class="infolinks">[<a href="javascript:toggleInfo('goodfellow2016nips','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('goodfellow2016nips','comment')">Comment</a>] [<a href="javascript:toggleInfo('goodfellow2016nips','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>arXiv preprint arXiv:1701.00160&nbsp;</td>
	<td>article</td>
	<td><a href="http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_goodfellow2016nips" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.</td>
</tr>
<tr id="rev_goodfellow2016nips" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Generative adversial network. Citations - 22336.</td>
</tr>
<tr id="bib_goodfellow2016nips" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{goodfellow2016nips,
  author = {Goodfellow, Ian},
  title = {NIPS 2016 tutorial: Generative adversarial networks},
  journal = {arXiv preprint arXiv:1701.00160},
  year = {2016},
  url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}
}
</pre></td>
</tr>
<tr id="Goyal2017" class="entry">
	<td>Goyal, R., Kahou, S.E., Michalski, V., Materzynska, J., Westphal, S., Kim, H., Haenel, V., Fruend, I., Yianilos, P., Mueller-Freitag, M. and others</td>
	<td>The" Something Something" Video Database for Learning and Evaluating Visual Common Sense. <p class="infolinks">[<a href="javascript:toggleInfo('Goyal2017','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Goyal2017','comment')">Comment</a>] [<a href="javascript:toggleInfo('Goyal2017','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td><br/>Vol. 1(4)ICCV, pp. 5&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_The_Something_Something_ICCV_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Goyal2017" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Neural networks trained on datasets such as ImageNet have led to major advances in visual object classification. One obstacle that prevents networks from reasoning more deeply about complex scenes and situations, and from integrating visual knowledge with natural language, like humans do, is their lack of common sense knowledge about the physical world. Videos, unlike still images, contain a wealth of detailed information about the physical world. However, most labelled video datasets represent high-level concepts rather than detailed physical aspects about actions and scenes. In this work, we describe our ongoing collection of the “something-something” database of video prediction tasks whose solutions require a common sense understanding of the depicted situation. The database currently contains more than 100,000 videos across 174 classes, which are defined as caption-templates. We also describe the challenges in crowd-sourcing this data at scale.</td>
</tr>
<tr id="rev_Goyal2017" class="comment noshow">
	<td colspan="6"><b>Comment</b>: VCR dataset. Citations - 191</td>
</tr>
<tr id="bib_Goyal2017" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Goyal2017,
  author = {Goyal, Raghav and Kahou, Samira Ebrahimi and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and others},
  title = {The" Something Something" Video Database for Learning and Evaluating Visual Common Sense.},
  booktitle = {ICCV},
  year = {2017},
  volume = {1},
  number = {4},
  pages = {5},
  url = {http://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_The_Something_Something_ICCV_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="graves2009offline" class="entry">
	<td>Graves, A. and Schmidhuber, J.</td>
	<td>Offline handwriting recognition with multidimensional recurrent neural networks <p class="infolinks">[<a href="javascript:toggleInfo('graves2009offline','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('graves2009offline','comment')">Comment</a>] [<a href="javascript:toggleInfo('graves2009offline','bibtex')">BibTeX</a>]</p></td>
	<td>2009</td>
	<td>Advances in neural information processing systems, pp. 545-552&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_graves2009offline" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Offline handwriting recognition---the transcription of images of handwritten text---is an interesting task, in that it combines computer vision with sequence learning. In most systems the two elements are handled separately, with sophisticated preprocessing techniques used to extract the image features and sequential models such as HMMs used to provide the transcriptions. By combining two recent innovations in neural networks---multidimensional recurrent neural networks and connectionist temporal classification---this paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input. Unlike competing systems, it does not require any alphabet specific preprocessing, and can therefore be used unchanged for any language. Evidence of its generality and power is provided by data from a recent international Arabic recognition competition, where it outperformed all entries (91.4% accuracy compared to 87.2% for the competition winner) despite the fact that neither author understands a word of Arabic.</td>
</tr>
<tr id="rev_graves2009offline" class="comment noshow">
	<td colspan="6"><b>Comment</b>: handwriting recognition with RNN.</td>
</tr>
<tr id="bib_graves2009offline" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{graves2009offline,
  author = {Graves, Alex and Schmidhuber, J&uuml;rgen},
  title = {Offline handwriting recognition with multidimensional recurrent neural networks},
  booktitle = {Advances in neural information processing systems},
  year = {2009},
  pages = {545--552},
  url = {https://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks.pdf}
}
</pre></td>
</tr>
<tr id="graves2013hybrid" class="entry">
	<td>Graves, A., Jaitly, N. and Mohamed, A.-r.</td>
	<td>Hybrid speech recognition with deep bidirectional LSTM <p class="infolinks">[<a href="javascript:toggleInfo('graves2013hybrid','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('graves2013hybrid','comment')">Comment</a>] [<a href="javascript:toggleInfo('graves2013hybrid','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>2013 IEEE workshop on automatic speech recognition and understanding, pp. 273-278&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://www.cs.toronto.edu/~graves/asru_2013.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_graves2013hybrid" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Deep Bidirectional LSTM (DBLSTM) recurrent neural networks have recently been shown to give state-of-the-art performance on the TIMIT speech database. However, the results in that work relied on recurrent-neural-network-specific objective functions, which are difficult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of DBLSTM as an acoustic model in a standard neural network-HMM hybrid system. We find that a DBLSTM-HMM hybrid gives equally good results on TIMIT as the previous work. It also outperforms both GMM and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates</td>
</tr>
<tr id="rev_graves2013hybrid" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Speech recognition. Citations - 1150</td>
</tr>
<tr id="bib_graves2013hybrid" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{graves2013hybrid,
  author = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
  title = {Hybrid speech recognition with deep bidirectional LSTM},
  booktitle = {2013 IEEE workshop on automatic speech recognition and understanding},
  year = {2013},
  pages = {273--278},
  url = {http://www.cs.toronto.edu/&nbsp;graves/asru_2013.pdf}
}
</pre></td>
</tr>
<tr id="graves2013speech" class="entry">
	<td>Graves, A., Mohamed, A.-r. and Hinton, G.</td>
	<td>Speech recognition with deep recurrent neural networks <p class="infolinks">[<a href="javascript:toggleInfo('graves2013speech','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('graves2013speech','comment')">Comment</a>] [<a href="javascript:toggleInfo('graves2013speech','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>2013 IEEE international conference on acoustics, speech and signal processing, pp. 6645-6649&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://arxiv.org/pdf/1303.5778.pdfï¼‰ï¼š“å¦‚æžœLSTMç”¨äºŽéš è— å±‚ï¼Œæˆ‘ä»¬å°†å¾—åˆ°">URL</a>&nbsp;</td>
</tr>
<tr id="abs_graves2013speech" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT<br>phoneme recognition benchmark, which to our knowledge is<br>the best recorded score.</td>
</tr>
<tr id="rev_graves2013speech" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Speech recognition. CItation - 6205</td>
</tr>
<tr id="bib_graves2013speech" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{graves2013speech,
  author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  title = {Speech recognition with deep recurrent neural networks},
  booktitle = {2013 IEEE international conference on acoustics, speech and signal processing},
  year = {2013},
  pages = {6645--6649},
  url = {https://arxiv.org/pdf/1303.5778.pdfï¼‰ï¼š“å¦‚æžœLSTMç”¨äºŽéš è— å±‚ï¼Œæˆ‘ä»¬å°†å¾—åˆ°}
}
</pre></td>
</tr>
<tr id="graves2013speech" class="entry">
	<td>Graves, A., Mohamed, A.-r. and Hinton, G.</td>
	<td>Speech recognition with deep recurrent neural networks <p class="infolinks">[<a href="javascript:toggleInfo('graves2013speech','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('graves2013speech','comment')">Comment</a>] [<a href="javascript:toggleInfo('graves2013speech','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>2013 IEEE international conference on acoustics, speech and signal processing, pp. 6645-6649&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://arxiv.org/pdf/1303.5778.pdf%C3%AF%C2%BC%E2%80%B0%C3%AF%C2%BC%C5%A1%E2%80%9C%C3%A5%C2%A6%E2%80%9A%C3%A6%C5%BE%C5%93LSTM%C3%A7%E2%80%9D%C2%A8%C3%A4%C2%BA%C5%BD%C3%A9%C5%A1%20%C3%A8%E2%80%94%20%C3%A5%C2%B1%E2%80%9A%C3%AF%C2%BC%C5%92%C3%A6%CB%86%E2%80%98%C3%A4%C2%BB%C2%AC%C3%A5%C2%B0%E2%80%A0%C3%A5%C2%BE%E2%80%94%C3%A5%CB%86%C2%B0">URL</a>&nbsp;</td>
</tr>
<tr id="abs_graves2013speech" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recurrent neural networks (RNNs) are a powerful model forsequential data. End-to-end training methods such as Connec-tionist Temporal Classification make it possible to train RNNsfor sequence labelling problems where the input-output align-ment is unknown.   The combination of these methods withthe Long Short-term Memory RNN architecture has provedparticularly fruitful, delivering state-of-the-art results in cur-sive handwriting recognition. However RNN performance inspeech recognition has so far been disappointing, with betterresults returned by deep feedforward networks. This paper in-vestigatesdeep recurrent neural networks, which combine themultiple levels of representation that have proved so effectivein deep networks with the flexible use of long range contextthat  empowers  RNNs.   When  trained  end-to-end  with  suit-able regularisation, we find that deep Long Short-term Mem-ory  RNNs  achieve  a  test  set  error  of  17.7%  on  the  TIMITphoneme recognition benchmark, which to our knowledge isthe best recorded score.</td>
</tr>
<tr id="rev_graves2013speech" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Speech recognition using DRNN. Citations - 6139</td>
</tr>
<tr id="bib_graves2013speech" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{graves2013speech,
  author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  title = {Speech recognition with deep recurrent neural networks},
  booktitle = {2013 IEEE international conference on acoustics, speech and signal processing},
  year = {2013},
  pages = {6645--6649},
  url = {https://arxiv.org/pdf/1303.5778.pdf%C3%AF%C2%BC%E2%80%B0%C3%AF%C2%BC%C5%A1%E2%80%9C%C3%A5%C2%A6%E2%80%9A%C3%A6%C5%BE%C5%93LSTM%C3%A7%E2%80%9D%C2%A8%C3%A4%C2%BA%C5%BD%C3%A9%C5%A1%20%C3%A8%E2%80%94%20%C3%A5%C2%B1%E2%80%9A%C3%AF%C2%BC%C5%92%C3%A6%CB%86%E2%80%98%C3%A4%C2%BB%C2%AC%C3%A5%C2%B0%E2%80%A0%C3%A5%C2%BE%E2%80%94%C3%A5%CB%86%C2%B0}
}
</pre></td>
</tr>
<tr id="he2017mask" class="entry">
	<td>He, K., Gkioxari, G., Doll&aacute;r, P. and Girshick, R.</td>
	<td>Mask r-cnn <p class="infolinks">[<a href="javascript:toggleInfo('he2017mask','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('he2017mask','comment')">Comment</a>] [<a href="javascript:toggleInfo('he2017mask','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the IEEE international conference on computer vision, pp. 2961-2969&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_he2017mask" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.</td>
</tr>
<tr id="rev_he2017mask" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Mask R-CNN. Citations - 7710.</td>
</tr>
<tr id="bib_he2017mask" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{he2017mask,
  author = {He, Kaiming and Gkioxari, Georgia and Doll&aacute;r, Piotr and Girshick, Ross},
  title = {Mask r-cnn},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  year = {2017},
  pages = {2961--2969},
  url = {http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="hendricks2016deep" class="entry">
	<td>Hendricks, L.A., Venugopalan, S., Rohrbach, M., Mooney, R., Saenko, K. and Darrell, T.</td>
	<td>Deep compositional captioning: Describing novel object categories without paired training data <p class="infolinks">[<a href="javascript:toggleInfo('hendricks2016deep','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('hendricks2016deep','comment')">Comment</a>] [<a href="javascript:toggleInfo('hendricks2016deep','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-10&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://openaccess.thecvf.com/content_cvpr_2016/papers/Hendricks_Deep_Compositional_Captioning_CVPR_2016_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_hendricks2016deep" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: While recent deep neural network models have achievedpromising results on the image captioning task, they relylargely on the availability of corpora with paired im-age and sentence captions to describe objects in context.In this work, we propose the Deep Compositional Cap-tioner (DCC) to address the task of generating descriptionsof novel objects which are not present in paired image-sentence datasets. Our method achieves this by leveraginglarge object recognition datasets and external text corporaand by transferring knowledge between semantically sim-ilar concepts. Current deep caption models can only de-scribe objects contained in paired image-sentence corpora,despite the fact that they are pre-trained with large objectrecognition datasets, namely ImageNet.  In contrast, ourmodel can compose sentences that describe novel objectsand their interactions with other objects. We demonstrateour model’s ability to describe novel concepts by empir-ically evaluating its performance on MSCOCO and showqualitative results on ImageNet images of objects for whichno paired image-sentence data exist. Further, we extendour approach to generate descriptions of objects in videoclips. Our results show that DCC has distinct advantagesover existing image and video captioning approaches forgenerating descriptions of new objects in context.</td>
</tr>
<tr id="rev_hendricks2016deep" class="comment noshow">
	<td colspan="6"><b>Comment</b>: VIdeo captioning. Citations - 185</td>
</tr>
<tr id="bib_hendricks2016deep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{hendricks2016deep,
  author = {Hendricks, Lisa Anne and Venugopalan, Subhashini and Rohrbach, Marcus and Mooney, Raymond and Saenko, Kate and Darrell, Trevor},
  title = {Deep compositional captioning: Describing novel object categories without paired training data},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2016},
  pages = {1--10},
  url = {https://openaccess.thecvf.com/content_cvpr_2016/papers/Hendricks_Deep_Compositional_Captioning_CVPR_2016_paper.pdf}
}
</pre></td>
</tr>
<tr id="hinton2012deep" class="entry">
	<td>Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.-r., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T.N. and others</td>
	<td>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups <p class="infolinks">[<a href="javascript:toggleInfo('hinton2012deep','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('hinton2012deep','comment')">Comment</a>] [<a href="javascript:toggleInfo('hinton2012deep','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>IEEE Signal processing magazine<br/>Vol. 29(6), pp. 82-97&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/HintonDengYuEtAl-SPM2012.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_hinton2012deep" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Most  current  speech  recognition  systems  use  hidden  Markov  models  (HMMs)  to  deal  with  the  temporal  variability  of  speech  and  Gaussian  mixture  models  (GMMs)  to  deter-mine how well each state of each HMM fits a frame  or  a  short  window  of  frames  of  coefficients  that  repre-sents the acoustic input. An alternative way to evaluate the fit is  to  use  a  feed-forward  neural  network  that  takes  several  frames  of  coefficients  as  input  and  produces  posterior  proba-bilities  over  HMM  states  as  output.  Deep  neural  networks  (DNNs)  that  have  many  hidden  layers  and  are  trained  using  new methods have been shown to outperform GMMs on a vari-ety  of  speech  recognition  benchmarks,  sometimes  by  a  large  margin. This article provides an overview of this progress and represents the shared views of four research groups that have had  recent  successes  in  using  DNNs  for  acoustic  modeling  in  speech recognition</td>
</tr>
<tr id="rev_hinton2012deep" class="comment noshow">
	<td colspan="6"><b>Comment</b>: DNN for speech. Citations - 8210</td>
</tr>
<tr id="bib_hinton2012deep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{hinton2012deep,
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  title = {Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  journal = {IEEE Signal processing magazine},
  publisher = {IEEE},
  year = {2012},
  volume = {29},
  number = {6},
  pages = {82--97},
  url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/HintonDengYuEtAl-SPM2012.pdf}
}
</pre></td>
</tr>
<tr id="hinton2012deep" class="entry">
	<td>Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A.-r., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T.N. and others</td>
	<td>Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups <p class="infolinks">[<a href="javascript:toggleInfo('hinton2012deep','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('hinton2012deep','comment')">Comment</a>] [<a href="javascript:toggleInfo('hinton2012deep','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>IEEE Signal processing magazine<br/>Vol. 29(6), pp. 82-97&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1303.5778.pdfï¼‰ï¼š“å¦‚æžœLSTMç”¨äºŽéš è— å±‚ï¼Œæˆ‘ä»¬å°†å¾—åˆ°">URL</a>&nbsp;</td>
</tr>
<tr id="abs_hinton2012deep" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recurrent neural networks (RNNs) are a powerful model for<br>sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved<br>particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in<br>speech recognition has so far been disappointing, with better<br>results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the<br>multiple levels of representation that have proved so effective<br>in deep networks with the flexible use of long range context<br>that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT<br>phoneme recognition benchmark, which to our knowledge is<br>the best recorded score.</td>
</tr>
<tr id="rev_hinton2012deep" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Speech recognition. CItation - 8264</td>
</tr>
<tr id="bib_hinton2012deep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{hinton2012deep,
  author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  title = {Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  journal = {IEEE Signal processing magazine},
  publisher = {IEEE},
  year = {2012},
  volume = {29},
  number = {6},
  pages = {82--97},
  url = {https://arxiv.org/pdf/1303.5778.pdfï¼‰ï¼š“å¦‚æžœLSTMç”¨äºŽéš è— å±‚ï¼Œæˆ‘ä»¬å°†å¾—åˆ°}
}
</pre></td>
</tr>
<tr id="hornik1989multilayer" class="entry">
	<td>Hornik, K., Stinchcombe, M., White, H. and others</td>
	<td>Multilayer feedforward networks are universal approximators. <p class="infolinks">[<a href="javascript:toggleInfo('hornik1989multilayer','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('hornik1989multilayer','comment')">Comment</a>] [<a href="javascript:toggleInfo('hornik1989multilayer','bibtex')">BibTeX</a>]</p></td>
	<td>1989</td>
	<td>Neural networks<br/>Vol. 2(5), pp. 359-366&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/notes/Sonia_Hornik.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_hornik1989multilayer" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: universal approximators: standard multilayer feedforward networks are capable of approximating<br>any measurable function to any desired degree of accuracy - there are no theoretical constraints<br>for the success of feedforward networks - lack of success is due to inadequate learning, insufficient</td>
</tr>
<tr id="rev_hornik1989multilayer" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Feedforward network. Citations - 19399.</td>
</tr>
<tr id="bib_hornik1989multilayer" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{hornik1989multilayer,
  author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert and others},
  title = {Multilayer feedforward networks are universal approximators.},
  journal = {Neural networks},
  year = {1989},
  volume = {2},
  number = {5},
  pages = {359--366},
  url = {https://www.cs.cmu.edu/&nbsp;bhiksha/courses/deeplearning/Fall.2016/notes/Sonia_Hornik.pdf}
}
</pre></td>
</tr>
<tr id="hornik1991approximation" class="entry">
	<td>Hornik, K.</td>
	<td>Approximation capabilities of multilayer feedforward networks <p class="infolinks">[<a href="javascript:toggleInfo('hornik1991approximation','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('hornik1991approximation','comment')">Comment</a>] [<a href="javascript:toggleInfo('hornik1991approximation','bibtex')">BibTeX</a>]</p></td>
	<td>1991</td>
	<td>Neural networks<br/>Vol. 4(2), pp. 251-257&nbsp;</td>
	<td>article</td>
	<td><a href="http://www.vision.jhu.edu/teaching/learning/deeplearning18/assets/Hornik-91.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_hornik1991approximation" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to V(p.) per-formance criteria, for arbitrary finite input environment measures Jl., provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can  be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives</td>
</tr>
<tr id="rev_hornik1991approximation" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Multilayer feedforward network. Citations - 4682.</td>
</tr>
<tr id="bib_hornik1991approximation" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{hornik1991approximation,
  author = {Hornik, Kurt},
  title = {Approximation capabilities of multilayer feedforward networks},
  journal = {Neural networks},
  publisher = {Elsevier},
  year = {1991},
  volume = {4},
  number = {2},
  pages = {251--257},
  url = {http://www.vision.jhu.edu/teaching/learning/deeplearning18/assets/Hornik-91.pdf}
}
</pre></td>
</tr>
<tr id="hu2017learning" class="entry">
	<td>Hu, R., Andreas, J., Rohrbach, M., Darrell, T. and Saenko, K.</td>
	<td>Learning to reason: End-to-end module networks for visual question answering <p class="infolinks">[<a href="javascript:toggleInfo('hu2017learning','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('hu2017learning','comment')">Comment</a>] [<a href="javascript:toggleInfo('hu2017learning','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the IEEE International Conference on Computer Vision, pp. 804-813&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://openaccess.thecvf.com/content_ICCV_2017/papers/Hu_Learning_to_Reason_ICCV_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_hu2017learning" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Natural language questions are inherently compositional,and many are most easily answered by reasoning about theirdecomposition into modular sub-problems. For example, toanswer“is there an equal number of balls and boxes?”wecan look for balls, look for boxes, count them, and com-pare the results. The recently proposed Neural Module Net-work (NMN) architecture [3,2] implements this approach toquestion answering by parsing questions into linguistic sub-structures and assembling question-specific deep networksfrom smaller modules that each solve one subtask. However,existing NMN implementations rely on brittle off-the-shelfparsers, and are restricted to the module configurations pro-posed by these parsers rather than learning them from data.In this paper, we propose End-to-End Module Networks(N2NMNs), which learn to reason by directly predictinginstance-specific network layouts without the aid of a parser.Our model learns to generate networkstructures(by imitat-ing expert demonstrations) while simultaneously learningnetworkparameters(using the downstream task loss). Exper-imental results on the newCLEVRdataset targeted at com-positional question answering show that N2NMNs achievean error reduction of nearly 50% relative to state-of-the-art attentional approaches, while discovering interpretablenetwork architectures specialized for each question.</td>
</tr>
<tr id="rev_hu2017learning" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Question answering. Citations - 307</td>
</tr>
<tr id="bib_hu2017learning" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{hu2017learning,
  author = {Hu, Ronghang and Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Saenko, Kate},
  title = {Learning to reason: End-to-end module networks for visual question answering},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  year = {2017},
  pages = {804--813},
  url = {https://openaccess.thecvf.com/content_ICCV_2017/papers/Hu_Learning_to_Reason_ICCV_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="jang2019video" class="entry">
	<td>Jang, Y., Song, Y., Kim, C.D., Yu, Y., Kim, Y. and Kim, G.</td>
	<td>Video question answering with spatio-temporal reasoning <p class="infolinks">[<a href="javascript:toggleInfo('jang2019video','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('jang2019video','comment')">Comment</a>] [<a href="javascript:toggleInfo('jang2019video','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>International Journal of Computer Vision<br/>Vol. 127(10), pp. 1385-1412&nbsp;</td>
	<td>article</td>
	<td><a href="https://link.springer.com/article/10.1007/s11263-019-01189-x">URL</a>&nbsp;</td>
</tr>
<tr id="abs_jang2019video" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention and show its effectiveness over conventional VQA techniques through empirical evaluations.</td>
</tr>
<tr id="rev_jang2019video" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video question answering. Citations - 2</td>
</tr>
<tr id="bib_jang2019video" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{jang2019video,
  author = {Jang, Yunseok and Song, Yale and Kim, Chris Dongjoo and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee},
  title = {Video question answering with spatio-temporal reasoning},
  journal = {International Journal of Computer Vision},
  publisher = {Springer},
  year = {2019},
  volume = {127},
  number = {10},
  pages = {1385--1412},
  url = {https://link.springer.com/article/10.1007/s11263-019-01189-x}
}
</pre></td>
</tr>
<tr id="jean2014using" class="entry">
	<td>Jean, S., Cho, K., Memisevic, R. and Bengio, Y.</td>
	<td>On using very large target vocabulary for neural machine translation <p class="infolinks">[<a href="javascript:toggleInfo('jean2014using','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('jean2014using','comment')">Comment</a>] [<a href="javascript:toggleInfo('jean2014using','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>arXiv preprint arXiv:1412.2007&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1412.2007">URL</a>&nbsp;</td>
</tr>
<tr id="abs_jean2014using" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling<br>a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling<br>that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be<br>efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole<br>target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based<br>neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured<br>by BLEU) on both the English→German and English→French translation tasks of WMT’14.</td>
</tr>
<tr id="rev_jean2014using" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translations. Citaions - 792</td>
</tr>
<tr id="bib_jean2014using" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{jean2014using,
  author = {Jean, S&eacute;bastien and Cho, Kyunghyun and Memisevic, Roland and Bengio, Yoshua},
  title = {On using very large target vocabulary for neural machine translation},
  journal = {arXiv preprint arXiv:1412.2007},
  year = {2014},
  url = {https://arxiv.org/pdf/1412.2007}
}
</pre></td>
</tr>
<tr id="ji2019video" class="entry">
	<td>Ji, Z., Xiong, K., Pang, Y. and Li, X.</td>
	<td>Video summarization with attention-based encoder-decoder networks <p class="infolinks">[<a href="javascript:toggleInfo('ji2019video','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('ji2019video','comment')">Comment</a>] [<a href="javascript:toggleInfo('ji2019video','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>IEEE Transactions on Circuits and Systems for Video Technology&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.sciencedirect.com/science/article/pii/S002002551830762X">URL</a>&nbsp;</td>
</tr>
<tr id="abs_ji2019video" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Given the explosive growth of online videos, it is becoming increasingly important to relieve the tedious work of browsing and managing the video content of interest. Video summarization aims at providing such a technique by transforming one or multiple videos into a compact one. However, conventional multi-video summarization methods often fail to produce satisfying results as they ignore the users’ search intents. To this end, this paper proposes a novel query-aware approach by formulating the multi-video summarization in a sparse coding framework, where the web images searched by a query are taken as the important preference information to reveal the query intent. To provide a user-friendly summarization, this paper also develops an event-keyframe presentation structure to present keyframes in groups of specific events related to the query by using an unsupervised multi-graph fusion method. Moreover, we release a new public dataset named MVS1K, which contains about 1000 videos from 10 queries and their video tags, manual annotations, and associated web images. Extensive experiments on the MVS1K and TVSum datasets demonstrate that our approaches produce competitively objective and subjective results.</td>
</tr>
<tr id="rev_ji2019video" class="comment noshow">
	<td colspan="6"><b>Comment</b>: VIdeo summarization. Citation - 16</td>
</tr>
<tr id="bib_ji2019video" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{ji2019video,
  author = {Ji, Zhong and Xiong, Kailin and Pang, Yanwei and Li, Xuelong},
  title = {Video summarization with attention-based encoder-decoder networks},
  journal = {IEEE Transactions on Circuits and Systems for Video Technology},
  publisher = {IEEE},
  year = {2019},
  url = {https://www.sciencedirect.com/science/article/pii/S002002551830762X}
}
</pre></td>
</tr>
<tr id="johnson2016densecap" class="entry">
	<td>Johnson, J., Karpathy, A. and Fei-Fei, L.</td>
	<td>Densecap: Fully convolutional localization networks for dense captioning <p class="infolinks">[<a href="javascript:toggleInfo('johnson2016densecap','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('johnson2016densecap','comment')">Comment</a>] [<a href="javascript:toggleInfo('johnson2016densecap','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4565-4574&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_johnson2016densecap" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.</td>
</tr>
<tr id="rev_johnson2016densecap" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. Citations - 761</td>
</tr>
<tr id="bib_johnson2016densecap" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{johnson2016densecap,
  author = {Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
  title = {Densecap: Fully convolutional localization networks for dense captioning},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2016},
  pages = {4565--4574},
  url = {http://openaccess.thecvf.com/content_cvpr_2016/papers/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf}
}
</pre></td>
</tr>
<tr id="kalchbrenner2013recurrent" class="entry">
	<td>Kalchbrenner, N. and Blunsom, P.</td>
	<td>Recurrent continuous translation models <p class="infolinks">[<a href="javascript:toggleInfo('kalchbrenner2013recurrent','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('kalchbrenner2013recurrent','comment')">Comment</a>] [<a href="javascript:toggleInfo('kalchbrenner2013recurrent','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1700-1709&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://www.aclweb.org/anthology/D13-1176.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_kalchbrenner2013recurrent" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units.<br>The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.</td>
</tr>
<tr id="rev_kalchbrenner2013recurrent" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translation. Citations - 1097</td>
</tr>
<tr id="bib_kalchbrenner2013recurrent" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{kalchbrenner2013recurrent,
  author = {Kalchbrenner, Nal and Blunsom, Phil},
  title = {Recurrent continuous translation models},
  booktitle = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  year = {2013},
  pages = {1700--1709},
  url = {https://www.aclweb.org/anthology/D13-1176.pdf}
}
</pre></td>
</tr>
<tr id="karpathy2014deep" class="entry">
	<td>Karpathy, A., Joulin, A. and Fei-Fei, L.F.</td>
	<td>Deep fragment embeddings for bidirectional image sentence mapping <p class="infolinks">[<a href="javascript:toggleInfo('karpathy2014deep','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('karpathy2014deep','comment')">Comment</a>] [<a href="javascript:toggleInfo('karpathy2014deep','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Advances in neural information processing systems, pp. 1889-1897&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://papers.nips.cc/paper/5281-deep-fragment-embeddings-for-bidirectional-image-sentence-mapping.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_karpathy2014deep" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We introduce a model for bidirectional retrieval of images and sentences througha deep, multi-modal embedding of visual and natural language data.  Unlike pre-vious models that directly map images or sentences into a common embeddingspace,  our model works on a finer level and embeds fragments of images (ob-jects) and fragments of sentences (typed dependency tree relations) into a com-mon space.  We then introduce a structured max-margin objective that allows ourmodel to explicitly associate these fragments across modalities. Extensive exper-imental evaluation shows that reasoning on both the global level of images andsentences and the finer level of their respective fragments improves performanceon image-sentence retrieval tasks. Additionally, our model provides interpretablepredictions  for  the  image-sentence  retrieval  task  since  the  inferred  inter-modalalignment of fragments is explicit.</td>
</tr>
<tr id="rev_karpathy2014deep" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. Citaions - 641.</td>
</tr>
<tr id="bib_karpathy2014deep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{karpathy2014deep,
  author = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li F},
  title = {Deep fragment embeddings for bidirectional image sentence mapping},
  booktitle = {Advances in neural information processing systems},
  year = {2014},
  pages = {1889--1897},
  url = {https://papers.nips.cc/paper/5281-deep-fragment-embeddings-for-bidirectional-image-sentence-mapping.pdf}
}
</pre></td>
</tr>
<tr id="karpathy2015deep" class="entry">
	<td>Karpathy, A. and Fei-Fei, L.</td>
	<td>Deep visual-semantic alignments for generating image descriptions <p class="infolinks">[<a href="javascript:toggleInfo('karpathy2015deep','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('karpathy2015deep','comment')">Comment</a>] [<a href="javascript:toggleInfo('karpathy2015deep','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3128-3137&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_karpathy2015deep" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.</td>
</tr>
<tr id="rev_karpathy2015deep" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. Citaitons - 3693</td>
</tr>
<tr id="bib_karpathy2015deep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{karpathy2015deep,
  author = {Karpathy, Andrej and Fei-Fei, Li},
  title = {Deep visual-semantic alignments for generating image descriptions},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2015},
  pages = {3128--3137},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf}
}
</pre></td>
</tr>
<tr id="karras2017progressive" class="entry">
	<td>Karras, T., Aila, T., Laine, S. and Lehtinen, J.</td>
	<td>Progressive growing of gans for improved quality, stability, and variation <p class="infolinks">[<a href="javascript:toggleInfo('karras2017progressive','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('karras2017progressive','comment')">Comment</a>] [<a href="javascript:toggleInfo('karras2017progressive','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>arXiv preprint arXiv:1710.10196&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1710.10196.pdf?__hstc=200028081.1bb630f9cde2cb5f07430159d50a3c91.1524009600081.1524009600082.1524009600083.1&__hssc=200028081.1.1524009600084&__hsfp=1773666937">URL</a>&nbsp;</td>
</tr>
<tr id="abs_karras2017progressive" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.</td>
</tr>
<tr id="rev_karras2017progressive" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Progressive growing of GANs. Citations - 2076.</td>
</tr>
<tr id="bib_karras2017progressive" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{karras2017progressive,
  author = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  title = {Progressive growing of gans for improved quality, stability, and variation},
  journal = {arXiv preprint arXiv:1710.10196},
  year = {2017},
  url = {https://arxiv.org/pdf/1710.10196.pdf?__hstc=200028081.1bb630f9cde2cb5f07430159d50a3c91.1524009600081.1524009600082.1524009600083.1&amp;__hssc=200028081.1.1524009600084&amp;__hsfp=1773666937}
}
</pre></td>
</tr>
<tr id="kawaguchi2017generalization" class="entry">
	<td>Kawaguchi, K., Kaelbling, L.P. and Bengio, Y.</td>
	<td>Generalization in deep learning <p class="infolinks">[<a href="javascript:toggleInfo('kawaguchi2017generalization','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('kawaguchi2017generalization','comment')">Comment</a>] [<a href="javascript:toggleInfo('kawaguchi2017generalization','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>arXiv preprint arXiv:1710.05468&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1710.05468">URL</a>&nbsp;</td>
</tr>
<tr id="abs_kawaguchi2017generalization" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.</td>
</tr>
<tr id="rev_kawaguchi2017generalization" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Genneralization. Citations - 171.</td>
</tr>
<tr id="bib_kawaguchi2017generalization" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{kawaguchi2017generalization,
  author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  title = {Generalization in deep learning},
  journal = {arXiv preprint arXiv:1710.05468},
  year = {2017},
  url = {https://arxiv.org/pdf/1710.05468}
}
</pre></td>
</tr>
<tr id="kazemi2017show" class="entry">
	<td>Kazemi, V. and Elqursh, A.</td>
	<td>Show, ask, attend, and answer: A strong baseline for visual question answering <p class="infolinks">[<a href="javascript:toggleInfo('kazemi2017show','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('kazemi2017show','comment')">Comment</a>] [<a href="javascript:toggleInfo('kazemi2017show','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>arXiv preprint arXiv:1704.03162&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1704.03162">URL</a>&nbsp;</td>
</tr>
<tr id="abs_kazemi2017show" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents a new baseline for visual question answering task. Given an image and a question in natural language, our model produces accurate answers according to the content of the image. Our model, while being architecturally simple and relatively small in terms of trainable parameters, sets a new state of the art on both unbalanced and balanced VQA benchmark. On VQA 1.0 open ended challenge, our model achieves 64.6% accuracy on the test-standard set without using additional data, an improvement of 0.4% over state of the art, and on newly released VQA 2.0, our model scores 59.7% on validation set outperforming best previously reported results by 0.5%. The results presented in this paper are especially interesting because very similar models have been tried before but significantly lower performance were reported. In light of the new results we hope to see more meaningful research on visual question answering in the future.</td>
</tr>
<tr id="rev_kazemi2017show" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Visual question answering. Citations - 88</td>
</tr>
<tr id="bib_kazemi2017show" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{kazemi2017show,
  author = {Kazemi, Vahid and Elqursh, Ali},
  title = {Show, ask, attend, and answer: A strong baseline for visual question answering},
  journal = {arXiv preprint arXiv:1704.03162},
  year = {2017},
  url = {https://arxiv.org/pdf/1704.03162}
}
</pre></td>
</tr>
<tr id="kim2017learning" class="entry">
	<td>Kim, T., Cha, M., Kim, H., Lee, J.K. and Kim, J.</td>
	<td>Learning to discover cross-domain relations with generative adversarial networks <p class="infolinks">[<a href="javascript:toggleInfo('kim2017learning','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('kim2017learning','comment')">Comment</a>] [<a href="javascript:toggleInfo('kim2017learning','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>arXiv preprint arXiv:1703.05192&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1703.05192.pdf?ref=hackernoon.com">URL</a>&nbsp;</td>
</tr>
<tr id="abs_kim2017learning" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity. Source code for official implementation is publicly available this https URL</td>
</tr>
<tr id="rev_kim2017learning" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Adversial network. Citations - 986.</td>
</tr>
<tr id="bib_kim2017learning" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{kim2017learning,
  author = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jung Kwon and Kim, Jiwon},
  title = {Learning to discover cross-domain relations with generative adversarial networks},
  journal = {arXiv preprint arXiv:1703.05192},
  year = {2017},
  url = {https://arxiv.org/pdf/1703.05192.pdf?ref=hackernoon.com}
}
</pre></td>
</tr>
<tr id="kingma2016improved" class="entry">
	<td>Kingma, D.P., Salimans, T., Jozefowicz, R., Chen, X., Sutskever, I. and Welling, M.</td>
	<td>Improved variational inference with inverse autoregressive flow <p class="infolinks">[<a href="javascript:toggleInfo('kingma2016improved','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('kingma2016improved','comment')">Comment</a>] [<a href="javascript:toggleInfo('kingma2016improved','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Advances in neural information processing systems, pp. 4743-4751&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_kingma2016improved" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The framework of normalizing flows provides a general strategy for flexible vari-ational inference of posteriors over latent variables. We propose a new type ofnormalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlierpublished flows, scales well to high-dimensional latent spaces. The proposed flowconsists of a chain of invertible transformations, where each transformation isbased on an autoregressive neural network. In experiments, we show that IAFsignificantly improves upon diagonal Gaussian approximate posteriors. In addition,we demonstrate that a novel type of variational autoencoder, coupled with IAF, iscompetitive with neural autoregressive models in terms of attained log-likelihoodon natural images, while allowing significantly faster synthesis.</td>
</tr>
<tr id="rev_kingma2016improved" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Variational autoencoders. Citations - 821.</td>
</tr>
<tr id="bib_kingma2016improved" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{kingma2016improved,
  author = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  title = {Improved variational inference with inverse autoregressive flow},
  booktitle = {Advances in neural information processing systems},
  year = {2016},
  pages = {4743--4751},
  url = {https://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow.pdf}
}
</pre></td>
</tr>
<tr id="kingma2019introduction" class="entry">
	<td>Kingma, D.P. and Welling, M.</td>
	<td>An introduction to variational autoencoders <p class="infolinks">[<a href="javascript:toggleInfo('kingma2019introduction','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('kingma2019introduction','comment')">Comment</a>] [<a href="javascript:toggleInfo('kingma2019introduction','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>arXiv preprint arXiv:1906.02691&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1906.02691">URL</a>&nbsp;</td>
</tr>
<tr id="abs_kingma2019introduction" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.</td>
</tr>
<tr id="rev_kingma2019introduction" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Variational autoencoders. Citations - 89.</td>
</tr>
<tr id="bib_kingma2019introduction" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{kingma2019introduction,
  author = {Kingma, Diederik P and Welling, Max},
  title = {An introduction to variational autoencoders},
  journal = {arXiv preprint arXiv:1906.02691},
  year = {2019},
  url = {https://arxiv.org/pdf/1906.02691}
}
</pre></td>
</tr>
<tr id="Kirillov2019" class="entry">
	<td>Kirillov, A., He, K., Girshick, R., Rother, C. and Doll&aacute;r, P.</td>
	<td>Panoptic segmentation <p class="infolinks">[<a href="javascript:toggleInfo('Kirillov2019','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Kirillov2019','comment')">Comment</a>] [<a href="javascript:toggleInfo('Kirillov2019','bibtex')">BibTeX</a>]</p></td>
	<td>2019</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9404-9413&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Kirillov_Panoptic_Segmentation_CVPR_2019_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Kirillov2019" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently<br>popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified<br>view of image segmentation. For more analysis and up-todate results, please check the arXiv version of the paper: https://arxiv.org/abs/1801.00868.</td>
</tr>
<tr id="rev_Kirillov2019" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Panoptic segmentation. CItations - 199</td>
</tr>
<tr id="bib_Kirillov2019" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Kirillov2019,
  author = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Doll&aacute;r, Piotr},
  title = {Panoptic segmentation},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2019},
  pages = {9404--9413},
  url = {http://openaccess.thecvf.com/content_CVPR_2019/papers/Kirillov_Panoptic_Segmentation_CVPR_2019_paper.pdf}
}
</pre></td>
</tr>
<tr id="klein2017opennmt" class="entry">
	<td>Klein, G., Kim, Y., Deng, Y., Senellart, J. and Rush, A.M.</td>
	<td>Opennmt: Open-source toolkit for neural machine translation <p class="infolinks">[<a href="javascript:toggleInfo('klein2017opennmt','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('klein2017opennmt','comment')">Comment</a>] [<a href="javascript:toggleInfo('klein2017opennmt','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>arXiv preprint arXiv:1701.02810&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1701.02810">URL</a>&nbsp;</td>
</tr>
<tr id="abs_klein2017opennmt" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.</td>
</tr>
<tr id="rev_klein2017opennmt" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translation. Citations - 991</td>
</tr>
<tr id="bib_klein2017opennmt" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{klein2017opennmt,
  author = {Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Senellart, Jean and Rush, Alexander M},
  title = {Opennmt: Open-source toolkit for neural machine translation},
  journal = {arXiv preprint arXiv:1701.02810},
  year = {2017},
  url = {https://arxiv.org/pdf/1701.02810}
}
</pre></td>
</tr>
<tr id="klein2017opennmt" class="entry">
	<td>Klein, G., Kim, Y., Deng, Y., Senellart, J. and Rush, A.M.</td>
	<td>Opennmt: Open-source toolkit for neural machine translation <p class="infolinks">[<a href="javascript:toggleInfo('klein2017opennmt','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>arXiv preprint arXiv:1701.02810&nbsp;</td>
	<td>article</td>
	<td>&nbsp;</td>
</tr>
<tr id="bib_klein2017opennmt" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{klein2017opennmt,
  author = {Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Senellart, Jean and Rush, Alexander M},
  title = {Opennmt: Open-source toolkit for neural machine translation},
  journal = {arXiv preprint arXiv:1701.02810},
  year = {2017}
}
</pre></td>
</tr>
<tr id="Krishna2017" class="entry">
	<td>Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S., Kalantidis, Y., Li, L.-J., Shamma, D.A. and others</td>
	<td>Visual genome: Connecting language and vision using crowdsourced dense image annotations <p class="infolinks">[<a href="javascript:toggleInfo('Krishna2017','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Krishna2017','comment')">Comment</a>] [<a href="javascript:toggleInfo('Krishna2017','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>International journal of computer vision<br/>Vol. 123(1), pp. 32-73&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1602.07332">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Krishna2017" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked "What vehicle is the person riding?", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that "the person is riding a horse-drawn carriage".<br>In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.</td>
</tr>
<tr id="rev_Krishna2017" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Visual genome dataset. Citations - 1355</td>
</tr>
<tr id="bib_Krishna2017" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Krishna2017,
  author = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  title = {Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  journal = {International journal of computer vision},
  publisher = {Springer},
  year = {2017},
  volume = {123},
  number = {1},
  pages = {32--73},
  url = {https://arxiv.org/pdf/1602.07332}
}
</pre></td>
</tr>
<tr id="kristan2018sixth" class="entry">
	<td>Kristan, M., Leonardis, A., Matas, J., Felsberg, M., Pflugfelder, R., ˇCehovin Zajc, L., Vojir, T., Bhat, G., Lukezic, A., Eldesokey, A. and others</td>
	<td>The sixth visual object tracking vot2018 challenge results <p class="infolinks">[<a href="javascript:toggleInfo('kristan2018sixth','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('kristan2018sixth','comment')">Comment</a>] [<a href="javascript:toggleInfo('kristan2018sixth','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>Proceedings of the European Conference on Computer Vision (ECCV), pp. 0-0&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11129/Kristan_The_sixth_Visual_Object_Tracking_VOT2018_challenge_results_ECCVW_2018_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_kristan2018sixth" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The Visual Object Tracking challenge VOT2018 is the sixth annual tracker benchmarking activity organized by the VOT initiative. Results of over eighty trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The evaluation included the standard VOT and other popular methodologies for short-term tracking analysis and a "real-time" experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. A long-term tracking subchallenge has been introduced to the set of standard VOT sub-challenges. The new subchallenge focuses on long-term tracking properties, namely coping with target disappearance and reappearance. A new dataset has been compiled and a performance evaluation methodology that focuses on long-term tracking capabilities has been adopted. The VOT toolkit has been updated to support both standard short-term and the new longterm tracking subchallenges. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website.</td>
</tr>
<tr id="rev_kristan2018sixth" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Visual tracking. Citations - 199</td>
</tr>
<tr id="bib_kristan2018sixth" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{kristan2018sixth,
  author = {Kristan, Matej and Leonardis, Ales and Matas, Jiri and Felsberg, Michael and Pflugfelder, Roman and ˇCehovin Zajc, Luka and Vojir, Tomas and Bhat, Goutam and Lukezic, Alan and Eldesokey, Abdelrahman and others},
  title = {The sixth visual object tracking vot2018 challenge results},
  booktitle = {Proceedings of the European Conference on Computer Vision (ECCV)},
  year = {2018},
  pages = {0--0},
  url = {http://openaccess.thecvf.com/content_ECCVW_2018/papers/11129/Kristan_The_sixth_Visual_Object_Tracking_VOT2018_challenge_results_ECCVW_2018_paper.pdf}
}
</pre></td>
</tr>
<tr id="krizhevsky2012imagenet" class="entry">
	<td>Krizhevsky, A., Sutskever, I. and Hinton, G.E.</td>
	<td>Imagenet classification with deep convolutional neural networks <p class="infolinks">[<a href="javascript:toggleInfo('krizhevsky2012imagenet','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('krizhevsky2012imagenet','comment')">Comment</a>] [<a href="javascript:toggleInfo('krizhevsky2012imagenet','bibtex')">BibTeX</a>]</p></td>
	<td>2012</td>
	<td>Advances in neural information processing systems, pp. 1097-1105&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_krizhevsky2012imagenet" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We trained a large, deep convolutional neural network to classify the 1.2 millionhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%and 17.0% which is considerably better than the previous state-of-the-art.   Theneural network, which has 60 million parameters and 650,000 neurons, consistsof five convolutional layers, some of which are followed by max-pooling layers,and three fully-connected layers with a final 1000-way softmax.  To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation.  To reduce overfitting in the fully-connectedlayers we employed a recently-developed regularization method called “dropout”that proved to be very effective.  We also entered a variant of this model in theILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%,compared to 26.2% achieved by the second-best entry</td>
</tr>
<tr id="rev_krizhevsky2012imagenet" class="comment noshow">
	<td colspan="6"><b>Comment</b>: ImageNet. Citations - 69143.</td>
</tr>
<tr id="bib_krizhevsky2012imagenet" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{krizhevsky2012imagenet,
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  title = {Imagenet classification with deep convolutional neural networks},
  booktitle = {Advances in neural information processing systems},
  year = {2012},
  pages = {1097--1105},
  url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}
</pre></td>
</tr>
<tr id="lebret2015phrase" class="entry">
	<td>Lebret, R., Pinheiro, P.O. and Collobert, R.</td>
	<td>Phrase-based image captioning <p class="infolinks">[<a href="javascript:toggleInfo('lebret2015phrase','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('lebret2015phrase','comment')">Comment</a>] [<a href="javascript:toggleInfo('lebret2015phrase','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>arXiv preprint arXiv:1502.03671&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1502.03671">URL</a>&nbsp;</td>
</tr>
<tr id="abs_lebret2015phrase" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.</td>
</tr>
<tr id="rev_lebret2015phrase" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. Citations - 103.</td>
</tr>
<tr id="bib_lebret2015phrase" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{lebret2015phrase,
  author = {Lebret, R&eacute;mi and Pinheiro, Pedro O and Collobert, Ronan},
  title = {Phrase-based image captioning},
  journal = {arXiv preprint arXiv:1502.03671},
  year = {2015},
  url = {https://arxiv.org/pdf/1502.03671}
}
</pre></td>
</tr>
<tr id="LeCun1989" class="entry">
	<td>LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W. and Jackel, L.D.</td>
	<td>Backpropagation applied to handwritten zip code recognition <p class="infolinks">[<a href="javascript:toggleInfo('LeCun1989','bibtex')">BibTeX</a>]</p></td>
	<td>1989</td>
	<td>Neural computation<br/>Vol. 1(4), pp. 541-551&nbsp;</td>
	<td>article</td>
	<td>&nbsp;</td>
</tr>
<tr id="bib_LeCun1989" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{LeCun1989,
  author = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  title = {Backpropagation applied to handwritten zip code recognition},
  journal = {Neural computation},
  publisher = {MIT Press},
  year = {1989},
  volume = {1},
  number = {4},
  pages = {541--551}
}
</pre></td>
</tr>
<tr id="lei2018tvqa" class="entry">
	<td>Lei, J., Yu, L., Bansal, M. and Berg, T.L.</td>
	<td>Tvqa: Localized, compositional video question answering <p class="infolinks">[<a href="javascript:toggleInfo('lei2018tvqa','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('lei2018tvqa','comment')">Comment</a>] [<a href="javascript:toggleInfo('lei2018tvqa','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>arXiv preprint arXiv:1809.01696&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1809.01696">URL</a>&nbsp;</td>
</tr>
<tr id="abs_lei2018tvqa" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks. However, due to data limitations, there has been much less work on video-based QA. In this paper, we present TVQA, a large-scale video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task</td>
</tr>
<tr id="rev_lei2018tvqa" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video question answering. Citations - 87.</td>
</tr>
<tr id="bib_lei2018tvqa" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{lei2018tvqa,
  author = {Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L},
  title = {Tvqa: Localized, compositional video question answering},
  journal = {arXiv preprint arXiv:1809.01696},
  year = {2018},
  url = {https://arxiv.org/pdf/1809.01696}
}
</pre></td>
</tr>
<tr id="li2015diversity" class="entry">
	<td>Li, J., Galley, M., Brockett, C., Gao, J. and Dolan, B.</td>
	<td>A diversity-promoting objective function for neural conversation models <p class="infolinks">[<a href="javascript:toggleInfo('li2015diversity','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('li2015diversity','comment')">Comment</a>] [<a href="javascript:toggleInfo('li2015diversity','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>arXiv preprint arXiv:1510.03055&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1510.03055.pdf)">URL</a>&nbsp;</td>
</tr>
<tr id="abs_li2015diversity" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., "I don't know") regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and in human evaluations.</td>
</tr>
<tr id="rev_li2015diversity" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Conversation models. CItations - 857</td>
</tr>
<tr id="bib_li2015diversity" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{li2015diversity,
  author = {Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
  title = {A diversity-promoting objective function for neural conversation models},
  journal = {arXiv preprint arXiv:1510.03055},
  year = {2015},
  url = {https://arxiv.org/pdf/1510.03055.pdf)}
}
</pre></td>
</tr>
<tr id="liang2015recurrent" class="entry">
	<td>Liang, M. and Hu, X.</td>
	<td>Recurrent convolutional neural network for object recognition <p class="infolinks">[<a href="javascript:toggleInfo('liang2015recurrent','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('liang2015recurrent','comment')">Comment</a>] [<a href="javascript:toggleInfo('liang2015recurrent','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3367-3375&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2015/papers/Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_liang2015recurrent" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In recent years, the convolutional neural network (CNN) has achieved great success in many computer vision tasks. Partially inspired by neuroscience, CNN shares many properties with the visual system of the brain. A prominent difference is that CNN is typically a feed-forward architecture while in the visual system recurrent connections are abundant. Inspired by this fact, we propose a recurrent CNN (RCNN) for object recognition by incorporating recurrent connections into each convolutional layer. Though the input is static, the activities of RCNN units evolve over time so that the activity of each unit is modulated by the activities of its neighboring units. This property enhances the ability of the model to integrate the context information, which is important for object recognition. Like other recurrent neural networks, unfolding the RCNN through time can result in an arbitrarily deep network with a fixed number of parameters. Furthermore, the unfolded network has multiple paths, which can facilitate the learning process. The model is tested on four benchmark object recognition datasets: CIFAR-10, CIFAR-100, MNIST and SVHN. With fewer trainable parameters, RCNN outperforms the state-of-the-art models on all of these datasets. Increasing the number of parameters leads to even better performance. These results demonstrate the advantage of the recurrent structure over purely feed-forward structure for object recognition.</td>
</tr>
<tr id="rev_liang2015recurrent" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Recurrent CNN. Citations - 678.</td>
</tr>
<tr id="bib_liang2015recurrent" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{liang2015recurrent,
  author = {Liang, Ming and Hu, Xiaolin},
  title = {Recurrent convolutional neural network for object recognition},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2015},
  pages = {3367--3375},
  url = {http://openaccess.thecvf.com/content_cvpr_2015/papers/Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper.pdf}
}
</pre></td>
</tr>
<tr id="Lin2014" class="entry">
	<td>Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll&aacute;r, P. and Zitnick, C.L.</td>
	<td>Microsoft coco: Common objects in context <p class="infolinks">[<a href="javascript:toggleInfo('Lin2014','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Lin2014','comment')">Comment</a>] [<a href="javascript:toggleInfo('Lin2014','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>European conference on computer vision, pp. 740-755&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://link.springer.com/content/pdf/10.1007/978-3-319-10602-1_48.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Lin2014" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed<br>statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.</td>
</tr>
<tr id="rev_Lin2014" class="comment noshow">
	<td colspan="6"><b>Comment</b>: COCO dataset. CItations - 10987</td>
</tr>
<tr id="bib_Lin2014" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Lin2014,
  author = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll&aacute;r, Piotr and Zitnick, C Lawrence},
  title = {Microsoft coco: Common objects in context},
  booktitle = {European conference on computer vision},
  year = {2014},
  pages = {740--755},
  url = {https://link.springer.com/content/pdf/10.1007/978-3-319-10602-1_48.pdf}
}
</pre></td>
</tr>
<tr id="Lin2016" class="entry">
	<td>Lin, S., Cai, L., Lin, X. and Ji, R.</td>
	<td>Masked face detection via a modified LeNet <p class="infolinks">[<a href="javascript:toggleInfo('Lin2016','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Lin2016','comment')">Comment</a>] [<a href="javascript:toggleInfo('Lin2016','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Neurocomputing<br/>Vol. 218, pp. 197-202&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.sciencedirect.com/science/article/abs/pii/S0925231216309523">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Lin2016" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Detecting masked faces in the wild has been emerging recently, which has rich applications ranging from violence video retrieval to video surveillance. Its accurate detection retains as an open problem, mainly due to the difficulties of low-resolution and arbitrary viewing angles, as well as the limitation of collecting sufficient amount of training samples. Such difficulties have been significantly challenged the design of effective handcraft features as well as robust detectors. In this paper, we tackle these problems by proposing a learn-based feature design and classifier training paradigm. More particularly, a modified LeNet, termed MLeNet, is presented, which modifies the number of units in output layer of LeNet to suit for a specific classification. Meanwhile, MLeNet further increases the number of feature maps with smaller filter size. To further reduce overfitting and improve the performance with a small quantity of training samples, we firstly increase the training dataset by horizontal reflection and then learn MLeNet via combining both pre-training and fine-tuning. We evaluate the proposed model on a real-world masked face detection dataset. Quantitative evaluations over several state-of-the-arts and alternative solution have demonstrated the accuracy and robustness of the proposed model.</td>
</tr>
<tr id="rev_Lin2016" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Citation - 12</td>
</tr>
<tr id="bib_Lin2016" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Lin2016,
  author = {Lin, Shaohui and Cai, Ling and Lin, Xianming and Ji, Rongrong},
  title = {Masked face detection via a modified LeNet},
  journal = {Neurocomputing},
  publisher = {Elsevier},
  year = {2016},
  volume = {218},
  pages = {197--202},
  url = {https://www.sciencedirect.com/science/article/abs/pii/S0925231216309523}
}
</pre></td>
</tr>
<tr id="lin2017refinenet" class="entry">
	<td>Lin, G., Milan, A., Shen, C. and Reid, I.</td>
	<td>Refinenet: Multi-path refinement networks for high-resolution semantic segmentation <p class="infolinks">[<a href="javascript:toggleInfo('lin2017refinenet','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('lin2017refinenet','comment')">Comment</a>] [<a href="javascript:toggleInfo('lin2017refinenet','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1925-1934&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_RefineNet_Multi-Path_Refinement_CVPR_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_lin2017refinenet" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recently,  very deep convolutional neural networks(CNNs) have shown outstanding performance in objectrecognition and have also been the first choice for denseclassification problems such as semantic segmentation.However, repeated subsampling operations like pooling orconvolution striding in deep CNNs lead to a significant de-crease in the initial image resolution.  Here, we presentRefineNet, a generic multi-path refinement network thatexplicitly exploits all the information available along thedown-sampling process to enable high-resolution predic-tion using long-range residual connections.  In this way,the deeper layers that capture high-level semantic featurescan be directly refined using fine-grained features from ear-lier convolutions. The individual components of RefineNetemploy residual connections following the identity map-ping mindset, which allows for effective end-to-end train-ing. Further, we introduce chained residual pooling, whichcaptures rich background context in an efficient manner. Wecarry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular,we achieve an intersection-over-union score of83.4on thechallenging PASCAL VOC 2012 dataset, which is the bestreported result to date.</td>
</tr>
<tr id="rev_lin2017refinenet" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Semantic segmentation. Citations - 1241.</td>
</tr>
<tr id="bib_lin2017refinenet" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{lin2017refinenet,
  author = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
  title = {Refinenet: Multi-path refinement networks for high-resolution semantic segmentation},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2017},
  pages = {1925--1934},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_RefineNet_Multi-Path_Refinement_CVPR_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="Liu2016" class="entry">
	<td>Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y. and Berg, A.C.</td>
	<td>Ssd: Single shot multibox detector <p class="infolinks">[<a href="javascript:toggleInfo('Liu2016','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Liu2016','comment')">Comment</a>] [<a href="javascript:toggleInfo('Liu2016','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>European conference on computer vision, pp. 21-37&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://arxiv.org/pdf/1512.02325.pdf%EF%BC%89">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Liu2016" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For   300×300  input, SSD achieves 74.3 % mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for   512×512  input, SSD achieves 76.9 % mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.</td>
</tr>
<tr id="rev_Liu2016" class="comment noshow">
	<td colspan="6"><b>Comment</b>: SSD. Citations - 9937</td>
</tr>
<tr id="bib_Liu2016" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Liu2016,
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C},
  title = {Ssd: Single shot multibox detector},
  booktitle = {European conference on computer vision},
  year = {2016},
  pages = {21--37},
  url = {https://arxiv.org/pdf/1512.02325.pdf%EF%BC%89}
}
</pre></td>
</tr>
<tr id="liu2016attention" class="entry">
	<td>Liu, C., Mao, J., Sha, F. and Yuille, A.</td>
	<td>Attention correctness in neural image captioning <p class="infolinks">[<a href="javascript:toggleInfo('liu2016attention','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('liu2016attention','comment')">Comment</a>] [<a href="javascript:toggleInfo('liu2016attention','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>arXiv preprint arXiv:1605.09553&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1605.09553">URL</a>&nbsp;</td>
</tr>
<tr id="abs_liu2016attention" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Attention mechanisms have recently been introduced in deep learning for various tasks in natural language processing and computer vision. But despite their popularity, the "correctness" of the implicitly-learned attention maps has only been assessed qualitatively by visualization of several examples. In this paper we focus on evaluating and improving the correctness of attention in neural image captioning models. Specifically, we propose a quantitative evaluation metric for the consistency between the generated attention maps and human annotations, using recently released datasets with alignment between regions in images and entities in captions. We then propose novel models with different levels of explicit supervision for learning attention maps during training. The supervision can be strong when alignment between regions and caption entities are available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing supervision of attention maps during training solidly improves both attention correctness and caption quality, showing the promise of making machine perception more human-like.</td>
</tr>
<tr id="rev_liu2016attention" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. Citations - 138</td>
</tr>
<tr id="bib_liu2016attention" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{liu2016attention,
  author = {Liu, Chenxi and Mao, Junhua and Sha, Fei and Yuille, Alan},
  title = {Attention correctness in neural image captioning},
  journal = {arXiv preprint arXiv:1605.09553},
  year = {2016},
  url = {https://arxiv.org/pdf/1605.09553}
}
</pre></td>
</tr>
<tr id="long2015fully" class="entry">
	<td>Long, J., Shelhamer, E. and Darrell, T.</td>
	<td>Fully convolutional networks for semantic segmentation <p class="infolinks">[<a href="javascript:toggleInfo('long2015fully','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('long2015fully','comment')">Comment</a>] [<a href="javascript:toggleInfo('long2015fully','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 3431-3440&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_long2015fully" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.</td>
</tr>
<tr id="rev_long2015fully" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Semantic segmentations. Citations - 17908</td>
</tr>
<tr id="bib_long2015fully" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{long2015fully,
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  title = {Fully convolutional networks for semantic segmentation},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2015},
  pages = {3431--3440},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf}
}
</pre></td>
</tr>
<tr id="lowe2015ubuntu" class="entry">
	<td>Lowe, R., Pow, N., Serban, I. and Pineau, J.</td>
	<td>The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems <p class="infolinks">[<a href="javascript:toggleInfo('lowe2015ubuntu','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('lowe2015ubuntu','comment')">Comment</a>] [<a href="javascript:toggleInfo('lowe2015ubuntu','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>arXiv preprint arXiv:1506.08909&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1506.08909">URL</a>&nbsp;</td>
</tr>
<tr id="abs_lowe2015ubuntu" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response.</td>
</tr>
<tr id="rev_lowe2015ubuntu" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Conversation model. Citations - 537</td>
</tr>
<tr id="bib_lowe2015ubuntu" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{lowe2015ubuntu,
  author = {Lowe, Ryan and Pow, Nissan and Serban, Iulian and Pineau, Joelle},
  title = {The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems},
  journal = {arXiv preprint arXiv:1506.08909},
  year = {2015},
  url = {https://arxiv.org/pdf/1506.08909}
}
</pre></td>
</tr>
<tr id="lu2017knowing" class="entry">
	<td>Lu, J., Xiong, C., Parikh, D. and Socher, R.</td>
	<td>Knowing when to look: Adaptive attention via a visual sentinel for image captioning <p class="infolinks">[<a href="javascript:toggleInfo('lu2017knowing','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('lu2017knowing','comment')">Comment</a>] [<a href="javascript:toggleInfo('lu2017knowing','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 375-383&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Lu_Knowing_When_to_CVPR_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_lu2017knowing" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as "the" and "of". Other words that may seem visual can often be predicted reliably just from the language model e.g., "sign" after "behind a red stop" or "phone" following "talking on a cell". In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.</td>
</tr>
<tr id="rev_lu2017knowing" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. Citations - 642</td>
</tr>
<tr id="bib_lu2017knowing" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{lu2017knowing,
  author = {Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
  title = {Knowing when to look: Adaptive attention via a visual sentinel for image captioning},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2017},
  pages = {375--383},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Lu_Knowing_When_to_CVPR_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="luong2015effective" class="entry">
	<td>Luong, M.-T., Pham, H. and Manning, C.D.</td>
	<td>Effective approaches to attention-based neural machine translation <p class="infolinks">[<a href="javascript:toggleInfo('luong2015effective','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>arXiv preprint arXiv:1508.04025&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1508.04025)">URL</a>&nbsp;</td>
</tr>
<tr id="bib_luong2015effective" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{luong2015effective,
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
  title = {Effective approaches to attention-based neural machine translation},
  journal = {arXiv preprint arXiv:1508.04025},
  year = {2015},
  url = {https://arxiv.org/pdf/1508.04025)}
}
</pre></td>
</tr>
<tr id="makhzani2015adversarial" class="entry">
	<td>Makhzani, A., Shlens, J., Jaitly, N., Goodfellow, I. and Frey, B.</td>
	<td>Adversarial autoencoders <p class="infolinks">[<a href="javascript:toggleInfo('makhzani2015adversarial','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('makhzani2015adversarial','comment')">Comment</a>] [<a href="javascript:toggleInfo('makhzani2015adversarial','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>arXiv preprint arXiv:1511.05644&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1511.05644.pdf]">URL</a>&nbsp;</td>
</tr>
<tr id="abs_makhzani2015adversarial" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.</td>
</tr>
<tr id="rev_makhzani2015adversarial" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Adversial autoencoders. Citations - 1368.</td>
</tr>
<tr id="bib_makhzani2015adversarial" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{makhzani2015adversarial,
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  title = {Adversarial autoencoders},
  journal = {arXiv preprint arXiv:1511.05644},
  year = {2015},
  url = {https://arxiv.org/pdf/1511.05644.pdf]}
}
</pre></td>
</tr>
<tr id="malinowski2015ask" class="entry">
	<td>Malinowski, M., Rohrbach, M. and Fritz, M.</td>
	<td>Ask your neurons: A neural-based approach to answering questions about images <p class="infolinks">[<a href="javascript:toggleInfo('malinowski2015ask','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('malinowski2015ask','comment')">Comment</a>] [<a href="javascript:toggleInfo('malinowski2015ask','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Proceedings of the IEEE international conference on computer vision, pp. 1-9&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_iccv_2015/papers/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_malinowski2015ask" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus.</td>
</tr>
<tr id="rev_malinowski2015ask" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Visual question answering. Citations - 483</td>
</tr>
<tr id="bib_malinowski2015ask" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{malinowski2015ask,
  author = {Malinowski, Mateusz and Rohrbach, Marcus and Fritz, Mario},
  title = {Ask your neurons: A neural-based approach to answering questions about images},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  year = {2015},
  pages = {1--9},
  url = {http://openaccess.thecvf.com/content_iccv_2015/papers/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.pdf}
}
</pre></td>
</tr>
<tr id="maninis2018video" class="entry">
	<td>Maninis, K.-K., Caelles, S., Chen, Y., Pont-Tuset, J., Leal-Taix&eacute;, L., Cremers, D. and Van Gool, L.</td>
	<td>Video object segmentation without temporal information <p class="infolinks">[<a href="javascript:toggleInfo('maninis2018video','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('maninis2018video','comment')">Comment</a>] [<a href="javascript:toggleInfo('maninis2018video','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>IEEE transactions on pattern analysis and machine intelligence<br/>Vol. 41(6), pp. 1515-1530&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1709.06031">URL</a>&nbsp;</td>
</tr>
<tr id="abs_maninis2018video" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Video Object Segmentation, and video processing in general, has been historically dominated by methods that rely on the temporal consistency and redundancy in consecutive video frames. When the temporal smoothness is suddenly broken, such as when an object is occluded, or some frames are missing in a sequence, the result of these methods can deteriorate significantly. This paper explores the orthogonal approach of processing each frame independently, i.e., disregarding the temporal information. In particular, it tackles the task of semi-supervised video object segmentation: the separation of an object from the background in a video, given its mask in the first frame. We present Semantic One-Shot Video Object Segmentation (OSVOSS), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic information, learned on ImageNet, to the task of foreground segmentation, and finally to learning the appearance of a single annotated object of the test sequence (hence one shot). We show that instance-level semantic information, when combined effectively, can dramatically improve the results of our previous method, OSVOS. We perform experiments on two recent single-object video segmentation databases, which show that OSVOSS is both the fastest and most accurate method in the state of the art. Experiments on multi-object video segmentation show that OSVOSS obtains competitive results.</td>
</tr>
<tr id="rev_maninis2018video" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Segmentation. Citation - 114.</td>
</tr>
<tr id="bib_maninis2018video" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{maninis2018video,
  author = {Maninis, K-K and Caelles, Sergi and Chen, Yuhua and Pont-Tuset, Jordi and Leal-Taix&eacute;, Laura and Cremers, Daniel and Van Gool, Luc},
  title = {Video object segmentation without temporal information},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  publisher = {IEEE},
  year = {2018},
  volume = {41},
  number = {6},
  pages = {1515--1530},
  url = {https://arxiv.org/pdf/1709.06031}
}
</pre></td>
</tr>
<tr id="mcculloch1943logical" class="entry">
	<td>McCulloch, W.S. and Pitts, W.</td>
	<td>A logical calculus of the ideas immanent in nervous activity <p class="infolinks">[<a href="javascript:toggleInfo('mcculloch1943logical','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('mcculloch1943logical','comment')">Comment</a>] [<a href="javascript:toggleInfo('mcculloch1943logical','bibtex')">BibTeX</a>]</p></td>
	<td>1943</td>
	<td>The bulletin of mathematical biophysics<br/>Vol. 5(4), pp. 115-133&nbsp;</td>
	<td>article</td>
	<td><a href="http://aiplaybook.a16z.com/reference-material/mcculloch-pitts-1943-neural-networks.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_mcculloch1943logical" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.</td>
</tr>
<tr id="rev_mcculloch1943logical" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Citations - 19989</td>
</tr>
<tr id="bib_mcculloch1943logical" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{mcculloch1943logical,
  author = {McCulloch, Warren S and Pitts, Walter},
  title = {A logical calculus of the ideas immanent in nervous activity},
  journal = {The bulletin of mathematical biophysics},
  publisher = {Springer},
  year = {1943},
  volume = {5},
  number = {4},
  pages = {115--133},
  url = {http://aiplaybook.a16z.com/reference-material/mcculloch-pitts-1943-neural-networks.pdf}
}
</pre></td>
</tr>
<tr id="mikolov2011extensions" class="entry">
	<td>Mikolov, T., Kombrink, S., Burget, L., &Ccaron;ernocky, J. and Khudanpur, S.</td>
	<td>Extensions of recurrent neural network language model <p class="infolinks">[<a href="javascript:toggleInfo('mikolov2011extensions','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('mikolov2011extensions','comment')">Comment</a>] [<a href="javascript:toggleInfo('mikolov2011extensions','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>2011 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 5528-5531&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_mikolov2011extensions" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present several modifications of the original recurrent neural net work language model (RNN LM). While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.</td>
</tr>
<tr id="rev_mikolov2011extensions" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Language model. Citations - 4914</td>
</tr>
<tr id="bib_mikolov2011extensions" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{mikolov2011extensions,
  author = {Mikolov, Tom&aacute;&scaron; and Kombrink, Stefan and Burget, Luk&aacute;&scaron; and &Ccaron;ernocky, Jan and Khudanpur, Sanjeev},
  title = {Extensions of recurrent neural network language model},
  booktitle = {2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  year = {2011},
  pages = {5528--5531},
  url = {http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf}
}
</pre></td>
</tr>
<tr id="mikolov2011extensions" class="entry">
	<td>Mikolov, T., Kombrink, S., Burget, L., &Ccaron;ernocky, J. and Khudanpur, S.</td>
	<td>Extensions of recurrent neural network language model <p class="infolinks">[<a href="javascript:toggleInfo('mikolov2011extensions','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('mikolov2011extensions','comment')">Comment</a>] [<a href="javascript:toggleInfo('mikolov2011extensions','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>2011 IEEE international conference on acoustics, speech and signal processing (ICASSP), pp. 5528-5531&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_mikolov2011extensions" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present several modifications of the original recurrent neural net work language model (RNN LM). While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.</td>
</tr>
<tr id="rev_mikolov2011extensions" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Language model. Citations - 4914</td>
</tr>
<tr id="bib_mikolov2011extensions" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{mikolov2011extensions,
  author = {Mikolov, Tom&aacute;&scaron; and Kombrink, Stefan and Burget, Luk&aacute;&scaron; and &Ccaron;ernocky, Jan and Khudanpur, Sanjeev},
  title = {Extensions of recurrent neural network language model},
  booktitle = {2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  year = {2011},
  pages = {5528--5531},
  url = {http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf}
}
</pre></td>
</tr>
<tr id="minsky2017perceptrons" class="entry">
	<td>Minsky, M. and Papert, S.A.</td>
	<td>Perceptrons: An introduction to computational geometry <p class="infolinks">[<a href="javascript:toggleInfo('minsky2017perceptrons','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('minsky2017perceptrons','comment')">Comment</a>] [<a href="javascript:toggleInfo('minsky2017perceptrons','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>&nbsp;</td>
	<td>book</td>
	<td><a href="https://books.google.com/books?hl=en&lr=&id=PLQ5DwAAQBAJ&oi=fnd&pg=PR5&dq=minsky perceptrons&ots=zyLGAHno_0&sig=lUsInQ-xgA6OqWEhpzfZo6h32k8">URL</a>&nbsp;</td>
</tr>
<tr id="abs_minsky2017perceptrons" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The first systematic study of parallelism in computation by two pioneers in the field. Reissue<br>of the 1988 Expanded Edition with a new foreword by Léon Bottou In 1969, ten years after<br>the discovery of the perceptron—which showed that a machine could be taught to perform</td>
</tr>
<tr id="rev_minsky2017perceptrons" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Perceptron. Citations - 9870.</td>
</tr>
<tr id="bib_minsky2017perceptrons" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@book{minsky2017perceptrons,
  author = {Minsky, Marvin and Papert, Seymour A},
  title = {Perceptrons: An introduction to computational geometry},
  publisher = {MIT press},
  year = {2017},
  url = {https://books.google.com/books?hl=en&amp;lr=&amp;id=PLQ5DwAAQBAJ&amp;oi=fnd&amp;pg=PR5&amp;dq=minsky perceptrons&amp;ots=zyLGAHno_0&amp;sig=lUsInQ-xgA6OqWEhpzfZo6h32k8}
}
</pre></td>
</tr>
<tr id="Moravcik2017" class="entry">
	<td>Morav&ccaron;\ik, M., Schmid, M., Burch, N., Lisy, V., Morrill, D., Bard, N., Davis, T., Waugh, K., Johanson, M. and Bowling, M.</td>
	<td>Deepstack: Expert-level artificial intelligence in heads-up no-limit poker <p class="infolinks">[<a href="javascript:toggleInfo('Moravcik2017','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Moravcik2017','comment')">Comment</a>] [<a href="javascript:toggleInfo('Moravcik2017','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Science<br/>Vol. 356(6337), pp. 508-513&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1701.01724">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Moravcik2017" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker is the quintessential game of imperfect information, and a longstanding challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated with statistical significance professional poker players in heads-up no-limit Texas hold’em. The approach is<br>theoretically sound and is shown to produce more difficult to exploit strategies than prior approaches.</td>
</tr>
<tr id="rev_Moravcik2017" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Deepstack poker game. Citation - 424</td>
</tr>
<tr id="bib_Moravcik2017" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Moravcik2017,
  author = {Morav&ccaron;\ik, Matej and Schmid, Martin and Burch, Neil and Lisy, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
  title = {Deepstack: Expert-level artificial intelligence in heads-up no-limit poker},
  journal = {Science},
  publisher = {American Association for the Advancement of Science},
  year = {2017},
  volume = {356},
  number = {6337},
  pages = {508--513},
  url = {https://arxiv.org/pdf/1701.01724}
}
</pre></td>
</tr>
<tr id="nguyen2017plug" class="entry">
	<td>Nguyen, A., Clune, J., Bengio, Y., Dosovitskiy, A. and Yosinski, J.</td>
	<td>Plug &amp; play generative networks: Conditional iterative generation of images in latent space <p class="infolinks">[<a href="javascript:toggleInfo('nguyen2017plug','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('nguyen2017plug','comment')">Comment</a>] [<a href="javascript:toggleInfo('nguyen2017plug','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4467-4477&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Nguyen_Plug__Play_CVPR_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_nguyen2017plug" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. 2016 showed one interesting way to synthesize novel images by performing gradient descent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227x227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models "Plug and Play Generative Networks". PPGNs are composed of (1) a generator network G that is capable of drawing a wide range of image types and (2) a replaceable "condition" network C that tells the generator what to draw. We demonstrate generation of images conditioned on a class - when C is an ImageNet classification network - and also conditioned on a caption - when C is an image captioning network. Our method also improves the state of the art of Deep Multifaceted Feature Visualization, which involves synthetically generating the set of inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While we operate on images in this paper, the approach is modality agnostic and can be applied to many types of data.</td>
</tr>
<tr id="rev_nguyen2017plug" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Generative networks. Citations - 404.</td>
</tr>
<tr id="bib_nguyen2017plug" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{nguyen2017plug,
  author = {Nguyen, Anh and Clune, Jeff and Bengio, Yoshua and Dosovitskiy, Alexey and Yosinski, Jason},
  title = {Plug &amp; play generative networks: Conditional iterative generation of images in latent space},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2017},
  pages = {4467--4477},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Nguyen_Plug__Play_CVPR_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="noh2016image" class="entry">
	<td>Noh, H., Hongsuck Seo, P. and Han, B.</td>
	<td>Image question answering using convolutional neural network with dynamic parameter prediction <p class="infolinks">[<a href="javascript:toggleInfo('noh2016image','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('noh2016image','comment')">Comment</a>] [<a href="javascript:toggleInfo('noh2016image','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 30-38&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Noh_Image_Question_Answering_CVPR_2016_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_noh2016image" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We tackle image question answering (ImageQA) problem by learning a convolutional neural network (CNN) with a dynamic parameter layer whose weights are determined adaptively based on questions. For the adaptive parameter prediction, we employ a separate parameter prediction network, which consists of gated recurrent unit (GRU) taking a question as its input and a fully-connected layer generating a set of candidate weights as its output. However, it is challenging to construct a parameter prediction network for a large number of parameters in the fully-connected dynamic parameter layer of the CNN. We reduce the complexity of this problem by incorporating a hashing technique, where the candidate weights given by the parameter prediction network are selected using a predefined hash function to determine individual weights in the dynamic parameter layer. The proposed network---joint network with the CNN for ImageQA and the parameter prediction network---is trained end-to-end through back-propagation, where its weights are initialized using a pre-trained CNN and GRU. The proposed algorithm illustrates the state-of-the-art performance on all available public ImageQA benchmarks.</td>
</tr>
<tr id="rev_noh2016image" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Question answering. Citations - 252</td>
</tr>
<tr id="bib_noh2016image" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{noh2016image,
  author = {Noh, Hyeonwoo and Hongsuck Seo, Paul and Han, Bohyung},
  title = {Image question answering using convolutional neural network with dynamic parameter prediction},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2016},
  pages = {30--38},
  url = {http://openaccess.thecvf.com/content_cvpr_2016/papers/Noh_Image_Question_Answering_CVPR_2016_paper.pdf}
}
</pre></td>
</tr>
<tr id="Oh2015" class="entry">
	<td>Oh, J., Guo, X., Lee, H., Lewis, R.L. and Singh, S.</td>
	<td>Action-conditional video prediction using deep networks in atari games <p class="infolinks">[<a href="javascript:toggleInfo('Oh2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Oh2015','comment')">Comment</a>] [<a href="javascript:toggleInfo('Oh2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Advances in neural information processing systems, pp. 2863-2871&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://papers.nips.cc/paper/5859-action-conditional-video-prediction-using-deep-networks-in-atari-games.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Oh2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future image-frames depend on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, actionconditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.</td>
</tr>
<tr id="rev_Oh2015" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Atari games by deep neural network. Citation -</td>
</tr>
<tr id="bib_Oh2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Oh2015,
  author = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard L and Singh, Satinder},
  title = {Action-conditional video prediction using deep networks in atari games},
  booktitle = {Advances in neural information processing systems},
  year = {2015},
  pages = {2863--2871},
  url = {http://papers.nips.cc/paper/5859-action-conditional-video-prediction-using-deep-networks-in-atari-games.pdf}
}
</pre></td>
</tr>
<tr id="oord2016wavenet" class="entry">
	<td>Oord, A.v.d., Dieleman, S., Zen, H., Simonyan, K., Vinyals, O., Graves, A., Kalchbrenner, N., Senior, A. and Kavukcuoglu, K.</td>
	<td>Wavenet: A generative model for raw audio <p class="infolinks">[<a href="javascript:toggleInfo('oord2016wavenet','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('oord2016wavenet','comment')">Comment</a>] [<a href="javascript:toggleInfo('oord2016wavenet','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>arXiv preprint arXiv:1609.03499&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1609.03499.pdf?utm_source=Sailthru&utm_medium=email&utm_campaign=Uncubed Entry #61 - April 3, 2019&utm_term=entry">URL</a>&nbsp;</td>
</tr>
<tr id="abs_oord2016wavenet" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.</td>
</tr>
<tr id="rev_oord2016wavenet" class="comment noshow">
	<td colspan="6"><b>Comment</b>: WaveNet. Citations - 2090</td>
</tr>
<tr id="bib_oord2016wavenet" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{oord2016wavenet,
  author = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  title = {Wavenet: A generative model for raw audio},
  journal = {arXiv preprint arXiv:1609.03499},
  year = {2016},
  url = {https://arxiv.org/pdf/1609.03499.pdf?utm_source=Sailthru&amp;utm_medium=email&amp;utm_campaign=Uncubed Entry #61 - April 3, 2019&amp;utm_term=entry}
}
</pre></td>
</tr>
<tr id="oord2018parallel" class="entry">
	<td>Oord, A., Li, Y., Babuschkin, I., Simonyan, K., Vinyals, O., Kavukcuoglu, K., Driessche, G., Lockhart, E., Cobo, L., Stimberg, F. and others</td>
	<td>Parallel wavenet: Fast high-fidelity speech synthesis <p class="infolinks">[<a href="javascript:toggleInfo('oord2018parallel','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('oord2018parallel','comment')">Comment</a>] [<a href="javascript:toggleInfo('oord2018parallel','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>International conference on machine learning, pp. 3918-3926&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://proceedings.mlr.press/v80/oord18a/oord18a.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_oord2018parallel" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today’s massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, a 1000x speed up relative to the original WaveNet, and capable of serving multiple English and Japanese voices in a production setting.</td>
</tr>
<tr id="rev_oord2018parallel" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Parallel Wavenets. Citations - 328.</td>
</tr>
<tr id="bib_oord2018parallel" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{oord2018parallel,
  author = {Oord, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and Driessche, George and Lockhart, Edward and Cobo, Luis and Stimberg, Florian and others},
  title = {Parallel wavenet: Fast high-fidelity speech synthesis},
  booktitle = {International conference on machine learning},
  year = {2018},
  pages = {3918--3926},
  url = {http://proceedings.mlr.press/v80/oord18a/oord18a.pdf}
}
</pre></td>
</tr>
<tr id="pan2016hierarchical" class="entry">
	<td>Pan, P., Xu, Z., Yang, Y., Wu, F. and Zhuang, Y.</td>
	<td>Hierarchical recurrent neural encoder for video representation with application to captioning <p class="infolinks">[<a href="javascript:toggleInfo('pan2016hierarchical','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('pan2016hierarchical','comment')">Comment</a>] [<a href="javascript:toggleInfo('pan2016hierarchical','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1029-1038&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_pan2016hierarchical" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recently, deep learning approach, especially deep Con-volutional Neural Networks (ConvNets), have achievedoverwhelming accuracy with fast processing speed for im-age classification. Incorporating temporal structure withdeep ConvNets for video representation becomes a funda-mental problem for video content analysis. In this paper,we propose a new approach, namely Hierarchical RecurrentNeural Encoder (HRNE), to exploit temporal information ofvideos. Compared to recent video representation inferenceapproaches, this paper makes the following three contribu-tions. First, our HRNE is able to efficiently exploit videotemporal structure in a longer range by reducing the lengthof input information flow, and compositing multiple consec-utive inputs at a higher level. Second, computation oper-ations are significantly lessened while attaining more non-linearity. Third, HRNE is able to uncover temporal tran-sitions between frame chunks with different granularities,i.e. it can model the temporal transitions between framesas well as the transitions between segments. We apply thenew method to video captioning where temporal informa-tion plays a crucial role. Experiments demonstrate that ourmethod outperforms the state-of-the-art on video captioningbenchmarks.</td>
</tr>
<tr id="rev_pan2016hierarchical" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video captioning. CItations - 293</td>
</tr>
<tr id="bib_pan2016hierarchical" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{pan2016hierarchical,
  author = {Pan, Pingbo and Xu, Zhongwen and Yang, Yi and Wu, Fei and Zhuang, Yueting},
  title = {Hierarchical recurrent neural encoder for video representation with application to captioning},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2016},
  pages = {1029--1038},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="pan2016hierarchical" class="entry">
	<td>Pan, P., Xu, Z., Yang, Y., Wu, F. and Zhuang, Y.</td>
	<td>Hierarchical recurrent neural encoder for video representation with application to captioning <p class="infolinks">[<a href="javascript:toggleInfo('pan2016hierarchical','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('pan2016hierarchical','comment')">Comment</a>] [<a href="javascript:toggleInfo('pan2016hierarchical','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 1029-1038&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="@inproceedings{pan2016hierarchical,
  title={Hierarchical recurrent neural encoder for video representation with application to captioning},
  author={Pan, Pingbo and Xu, Zhongwen and Yang, Yi and Wu, Fei and Zhuang, Yueting},
  booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={1029--1038},
  year={2016}
}">URL</a>&nbsp;</td>
</tr>
<tr id="abs_pan2016hierarchical" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recently, deep learning approach, especially deep Convolutional Neural Networks (ConvNets), have achieved overwhelming accuracy with fast processing speed for image classification. Incorporating temporal structure with deep ConvNets for video representation becomes a fundamental problem for video content analysis. In this paper, we propose a new approach, namely Hierarchical Recurrent Neural Encoder (HRNE), to exploit temporal information of videos. Compared to recent video representation inference approaches, this paper makes the following three contributions. First, our HRNE is able to efficiently exploit video temporal structure in a longer range by reducing the length of input information flow, and compositing multiple consecutive inputs at a higher level. Second, computation operations are significantly lessened while attaining more non-linearity. Third, HRNE is able to uncover temporal transitions between frame chunks with different granularities, i.e. it can model the temporal transitions between frames as well as the transitions between segments. We apply the new method to video captioning where temporal information plays a crucial role. Experiments demonstrate that our method outperforms the state-of-the-art on video captioning benchmarks.</td>
</tr>
<tr id="rev_pan2016hierarchical" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video captioning. Citations - 293.</td>
</tr>
<tr id="bib_pan2016hierarchical" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{pan2016hierarchical,
  author = {Pan, Pingbo and Xu, Zhongwen and Yang, Yi and Wu, Fei and Zhuang, Yueting},
  title = {Hierarchical recurrent neural encoder for video representation with application to captioning},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2016},
  pages = {1029--1038},
  url = {@inproceedingspan2016hierarchical,<br>  title=Hierarchical recurrent neural encoder for video representation with application to captioning,<br>  author=Pan, Pingbo and Xu, Zhongwen and Yang, Yi and Wu, Fei and Zhuang, Yueting,<br>  booktitle=Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,<br>  pages=1029--1038,<br>  year=2016<br>}
}
</pre></td>
</tr>
<tr id="pan2017video" class="entry">
	<td>Pan, Y., Yao, T., Li, H. and Mei, T.</td>
	<td>Video captioning with transferred semantic attributes <p class="infolinks">[<a href="javascript:toggleInfo('pan2017video','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('pan2017video','comment')">Comment</a>] [<a href="javascript:toggleInfo('pan2017video','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 6504-6512&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Pan_Video_Captioning_With_CVPR_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_pan2017video" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Automatically generating natural language descriptions of videos plays a fundamental challenge for computer vision community. Most recent progress in this problem has been achieved through employing 2-D and/or 3-D Convolutional Neural Networks (CNNs) to encode video content and Recurrent Neural Networks (RNNs) to decode a sentence. In this paper, we present Long Short-Term Memory with Transferred Semantic Attributes (LSTM-TSA)---a novel deep architecture that incorporates the transferred semantic attributes learnt from images and videos into the CNN plus RNN framework, by training them in an end-to-end manner. The design of LSTM-TSA is highly inspired by the facts that 1) semantic attributes play a significant contribution to captioning, and 2) images and videos carry complementary semantics and thus can reinforce each other for captioning. To boost video captioning, we propose a novel transfer unit to model the mutually correlated attributes learnt from images and videos. Extensive experiments are conducted on three public datasets, i.e., MSVD, M-VAD and MPII-MD. Our proposed LSTM-TSA achieves to-date the best published performance in sentence generation on MSVD: 52.8% and 74.0% in terms of BLEU@4 and CIDEr-D. Superior results are also reported on M-VAD and MPII-MD when compared to state-of-the-art methods.</td>
</tr>
<tr id="rev_pan2017video" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video captioning. CItations - 183</td>
</tr>
<tr id="bib_pan2017video" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{pan2017video,
  author = {Pan, Yingwei and Yao, Ting and Li, Houqiang and Mei, Tao},
  title = {Video captioning with transferred semantic attributes},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2017},
  pages = {6504--6512},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Pan_Video_Captioning_With_CVPR_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="panda2017multi" class="entry">
	<td>Panda, R. and Roy-Chowdhury, A.K.</td>
	<td>Multi-view surveillance video summarization via joint embedding and sparse optimization <p class="infolinks">[<a href="javascript:toggleInfo('panda2017multi','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('panda2017multi','comment')">Comment</a>] [<a href="javascript:toggleInfo('panda2017multi','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>IEEE Transactions on Multimedia<br/>Vol. 19(9), pp. 2010-2021&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1706.03121">URL</a>&nbsp;</td>
</tr>
<tr id="abs_panda2017multi" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Most traditional video summarization methods are designed to generate effective summaries for single-view videos, and thus, they cannot fully exploit the complicated intra- and inter-view correlations in summarizing multi-view videos in a camera network. In this paper, with the aim of summarizing multi-view videos, we introduce a novel unsupervised framework via joint embedding and sparse representative selection. The objective function is twofold. The first is to capture the multiview correlations via an embedding, which helps in extracting a diverse set of representatives. The second is to use a ℓ 2,1 -norm to model the sparsity while selecting representative shots for the summary. We propose to jointly optimize both of the objectives, such that embedding cannot only characterize the correlations, but also indicate the requirements of sparse representative selection. We present an efficient alternating algorithm based on half-quadratic minimization to solve the proposed non-smooth and non-convex objective with convergence analysis. A key advantage of the proposed approach with respect to the state-of-the-art is that it can summarize multi-view videos without assuming any prior correspondences/alignment between them, e.g., uncalibrated camera networks. Rigorous experiments on several multi-view datasets demonstrate that our approach clearly outperforms the state-of-the-art methods.</td>
</tr>
<tr id="rev_panda2017multi" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video summarization. Citation - 33.</td>
</tr>
<tr id="bib_panda2017multi" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{panda2017multi,
  author = {Panda, Rameswar and Roy-Chowdhury, Amit K},
  title = {Multi-view surveillance video summarization via joint embedding and sparse optimization},
  journal = {IEEE Transactions on Multimedia},
  publisher = {IEEE},
  year = {2017},
  volume = {19},
  number = {9},
  pages = {2010--2021},
  url = {https://arxiv.org/pdf/1706.03121}
}
</pre></td>
</tr>
<tr id="paszke2016enet" class="entry">
	<td>Paszke, A., Chaurasia, A., Kim, S. and Culurciello, E.</td>
	<td>Enet: A deep neural network architecture for real-time semantic segmentation <p class="infolinks">[<a href="javascript:toggleInfo('paszke2016enet','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('paszke2016enet','comment')">Comment</a>] [<a href="javascript:toggleInfo('paszke2016enet','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>arXiv preprint arXiv:1606.02147&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1606.02147.pdf)ì €">URL</a>&nbsp;</td>
</tr>
<tr id="abs_paszke2016enet" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18× faster, requires 75× less FLOPs, has 79× less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.</td>
</tr>
<tr id="rev_paszke2016enet" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Semantic segmentations. Citations - 761</td>
</tr>
<tr id="bib_paszke2016enet" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{paszke2016enet,
  author = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
  title = {Enet: A deep neural network architecture for real-time semantic segmentation},
  journal = {arXiv preprint arXiv:1606.02147},
  year = {2016},
  url = {https://arxiv.org/pdf/1606.02147.pdf)ì €}
}
</pre></td>
</tr>
<tr id="perez2017effectiveness" class="entry">
	<td>Perez, L. and Wang, J.</td>
	<td>The effectiveness of data augmentation in image classification using deep learning <p class="infolinks">[<a href="javascript:toggleInfo('perez2017effectiveness','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('perez2017effectiveness','comment')">Comment</a>] [<a href="javascript:toggleInfo('perez2017effectiveness','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>arXiv preprint arXiv:1712.04621&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1712.04621.pdf?source=post_page---------------------------">URL</a>&nbsp;</td>
</tr>
<tr id="abs_perez2017effectiveness" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: n this paper, we explore and compare multiple solutionsto the problem of data augmentation in image classification.Previous work has demonstrated the effectiveness of dataaugmentation through simple techniques, such as cropping,rotating,  and  flipping  input  images.   We  artificially  con-strain our access to data to a small subset of the ImageNetdataset, and compare each data augmentation technique inturn. One of the more successful data augmentations strate-gies is the traditional transformations mentioned above. Wealso experiment with GANs to generate images of differentstyles. Finally, we propose a method to allow a neural net tolearn augmentations that best improve the classifier, whichwe call neural augmentation. We discuss the successes andshortcomings of this method on various datasets</td>
</tr>
<tr id="rev_perez2017effectiveness" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image classification using DNN. Citations - 725.</td>
</tr>
<tr id="bib_perez2017effectiveness" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{perez2017effectiveness,
  author = {Perez, Luis and Wang, Jason},
  title = {The effectiveness of data augmentation in image classification using deep learning},
  journal = {arXiv preprint arXiv:1712.04621},
  year = {2017},
  url = {https://arxiv.org/pdf/1712.04621.pdf?source=post_page---------------------------}
}
</pre></td>
</tr>
<tr id="Redmon2016" class="entry">
	<td>Redmon, J., Divvala, S., Girshick, R. and Farhadi, A.</td>
	<td>You only look once: Unified, real-time object detection <p class="infolinks">[<a href="javascript:toggleInfo('Redmon2016','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Redmon2016','comment')">Comment</a>] [<a href="javascript:toggleInfo('Redmon2016','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 779-788&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Redmon2016" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.</td>
</tr>
<tr id="rev_Redmon2016" class="comment noshow">
	<td colspan="6"><b>Comment</b>: YOLO. CItations - 10673</td>
</tr>
<tr id="bib_Redmon2016" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Redmon2016,
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  title = {You only look once: Unified, real-time object detection},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2016},
  pages = {779--788},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf}
}
</pre></td>
</tr>
<tr id="redmon2017yolo9000" class="entry">
	<td>Redmon, J. and Farhadi, A.</td>
	<td>YOLO9000: better, faster, stronger <p class="infolinks">[<a href="javascript:toggleInfo('redmon2017yolo9000','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('redmon2017yolo9000','comment')">Comment</a>] [<a href="javascript:toggleInfo('redmon2017yolo9000','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 7263-7271&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_redmon2017yolo9000" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.</td>
</tr>
<tr id="rev_redmon2017yolo9000" class="comment noshow">
	<td colspan="6"><b>Comment</b>: YOLO9000. Citations - 5497</td>
</tr>
<tr id="bib_redmon2017yolo9000" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{redmon2017yolo9000,
  author = {Redmon, Joseph and Farhadi, Ali},
  title = {YOLO9000: better, faster, stronger},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2017},
  pages = {7263--7271},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="ren2015faster" class="entry">
	<td>Ren, S., He, K., Girshick, R. and Sun, J.</td>
	<td>Faster r-cnn: Towards real-time object detection with region proposal networks <p class="infolinks">[<a href="javascript:toggleInfo('ren2015faster','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('ren2015faster','comment')">Comment</a>] [<a href="javascript:toggleInfo('ren2015faster','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Advances in neural information processing systems, pp. 91-99&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_ren2015faster" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster_rcnn.</td>
</tr>
<tr id="rev_ren2015faster" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Faster RCNN. Citations - 21399.</td>
</tr>
<tr id="bib_ren2015faster" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{ren2015faster,
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  title = {Faster r-cnn: Towards real-time object detection with region proposal networks},
  booktitle = {Advances in neural information processing systems},
  year = {2015},
  pages = {91--99},
  url = {http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf}
}
</pre></td>
</tr>
<tr id="rennie2017self" class="entry">
	<td>Rennie, S.J., Marcheret, E., Mroueh, Y., Ross, J. and Goel, V.</td>
	<td>Self-critical sequence training for image captioning <p class="infolinks">[<a href="javascript:toggleInfo('rennie2017self','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('rennie2017self','comment')">Comment</a>] [<a href="javascript:toggleInfo('rennie2017self','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7008-7024&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_rennie2017self" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a "baseline" to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.</td>
</tr>
<tr id="rev_rennie2017self" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. Citations - 671</td>
</tr>
<tr id="bib_rennie2017self" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{rennie2017self,
  author = {Rennie, Steven J and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  title = {Self-critical sequence training for image captioning},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2017},
  pages = {7008--7024},
  url = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="Ronneberger2015" class="entry">
	<td>Ronneberger, O., Fischer, P. and Brox, T.</td>
	<td>U-net: Convolutional networks for biomedical image segmentation <p class="infolinks">[<a href="javascript:toggleInfo('Ronneberger2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Ronneberger2015','comment')">Comment</a>] [<a href="javascript:toggleInfo('Ronneberger2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>International Conference on Medical image computing and computer-assisted intervention, pp. 234-241&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://arxiv.org/pdf/1505.04597.pdf)%E5%92%8C[Tiramisu](https://arxiv.org/abs/1611.09326">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Ronneberger2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .</td>
</tr>
<tr id="rev_Ronneberger2015" class="comment noshow">
	<td colspan="6"><b>Comment</b>: U-net. Citations - 16950</td>
</tr>
<tr id="bib_Ronneberger2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Ronneberger2015,
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  title = {U-net: Convolutional networks for biomedical image segmentation},
  booktitle = {International Conference on Medical image computing and computer-assisted intervention},
  year = {2015},
  pages = {234--241},
  url = {https://arxiv.org/pdf/1505.04597.pdf)%E5%92%8C[Tiramisu](https://arxiv.org/abs/1611.09326}
}
</pre></td>
</tr>
<tr id="rumelhart1985learning" class="entry">
	<td>Rumelhart, D.E., Hinton, G.E. and Williams, R.J.</td>
	<td>Learning internal representations by error propagation <p class="infolinks">[<a href="javascript:toggleInfo('rumelhart1985learning','comment')">Comment</a>] [<a href="javascript:toggleInfo('rumelhart1985learning','bibtex')">BibTeX</a>]</p></td>
	<td>1985</td>
	<td><i>School</i>: California Univ San Diego La Jolla Inst for Cognitive Science&nbsp;</td>
	<td>techreport</td>
	<td><a href="https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="rev_rumelhart1985learning" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Internal representation by error propagation. Citations - 26607.</td>
</tr>
<tr id="bib_rumelhart1985learning" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@techreport{rumelhart1985learning,
  author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  title = {Learning internal representations by error propagation},
  school = {California Univ San Diego La Jolla Inst for Cognitive Science},
  year = {1985},
  url = {https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf}
}
</pre></td>
</tr>
<tr id="sak2014long" class="entry">
	<td>Sak, H., Senior, A. and Beaufays, F.</td>
	<td>Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition <p class="infolinks">[<a href="javascript:toggleInfo('sak2014long','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('sak2014long','comment')">Comment</a>] [<a href="javascript:toggleInfo('sak2014long','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>arXiv preprint arXiv:1402.1128&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1402.1128">URL</a>&nbsp;</td>
</tr>
<tr id="abs_sak2014long" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Long Short-Term Memory (LSTM) is a recurrent neural network<br>(RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike<br>feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models. 1</td>
</tr>
<tr id="rev_sak2014long" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Speech reocgnition. Citations - 558</td>
</tr>
<tr id="bib_sak2014long" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{sak2014long,
  author = {Sak, Ha&scedil;im and Senior, Andrew and Beaufays, Fran&ccedil;oise},
  title = {Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition},
  journal = {arXiv preprint arXiv:1402.1128},
  year = {2014},
  url = {https://arxiv.org/pdf/1402.1128}
}
</pre></td>
</tr>
<tr id="salakhutdinov2009deep" class="entry">
	<td>Salakhutdinov, R. and Hinton, G.</td>
	<td>Deep boltzmann machines <p class="infolinks">[<a href="javascript:toggleInfo('salakhutdinov2009deep','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('salakhutdinov2009deep','comment')">Comment</a>] [<a href="javascript:toggleInfo('salakhutdinov2009deep','bibtex')">BibTeX</a>]</p></td>
	<td>2009</td>
	<td>Artificial intelligence and statistics, pp. 448-455&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://www.jmlr.org/proceedings/papers/v5/salakhutdinov09a/salakhutdinov09a.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_salakhutdinov2009deep" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present a new learning algorithm for Boltz-mann machines that contain many layers of hid-den variables.  Data-dependent expectations areestimated using a variational approximation thattends  to  focus  on  a  single  mode,   and  data-independent  expectations  are  approximated  us-ing  persistent  Markov  chains.   The  use  of  twoquite different techniques for estimating the twotypes of expectation that enter into the gradientof the log-likelihood makes it practical to learnBoltzmann  machines with  multiple  hidden lay-ers and millions of parameters. The learning canbe made more efficient by using a layer-by-layer“pre-training”  phase  that  allows  variational  in-ference  to  be  initialized  with  a  single  bottom-up pass.  We present results on the MNIST andNORB  datasets  showing  that  deep  Boltzmannmachines learn good generative models and per-form well on handwritten digit and visual objectrecognition tasks.</td>
</tr>
<tr id="rev_salakhutdinov2009deep" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Deep boltzman machine. Citations - 2147.</td>
</tr>
<tr id="bib_salakhutdinov2009deep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{salakhutdinov2009deep,
  author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
  title = {Deep boltzmann machines},
  booktitle = {Artificial intelligence and statistics},
  year = {2009},
  pages = {448--455},
  url = {http://www.jmlr.org/proceedings/papers/v5/salakhutdinov09a/salakhutdinov09a.pdf}
}
</pre></td>
</tr>
<tr id="santoro2017simple" class="entry">
	<td>Santoro, A., Raposo, D., Barrett, D.G., Malinowski, M., Pascanu, R., Battaglia, P. and Lillicrap, T.</td>
	<td>A simple neural network module for relational reasoning <p class="infolinks">[<a href="javascript:toggleInfo('santoro2017simple','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('santoro2017simple','comment')">Comment</a>] [<a href="javascript:toggleInfo('santoro2017simple','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Advances in neural information processing systems, pp. 4967-4976&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://papers.nips.cc/paper/7082-a-simple-neural-network-module-for-relational-reasoning.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_santoro2017simple" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Relational reasoning is a central component of generally intelligent behavior, buthas proven difficult for neural networks to learn. In this paper we describe how touse Relation Networks (RNs) as a simple plug-and-play module to solve problemsthat fundamentally hinge on relational reasoning. We tested RN-augmented net-works on three tasks: visual question answering using a challenging dataset calledCLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoningabout dynamic physical systems.  Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a generalcapacity to solve relational questions, but can gain this capacity when augmentedwith RNs.  Thus, by simply augmenting convolutions, LSTMs, and MLPs withRNs, we can remove computational burden from network components that arenot well-suited to handle relational reasoning, reduce overall network complexity,and gain a general ability to reason about the relations between entities and theirproperties.</td>
</tr>
<tr id="rev_santoro2017simple" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Visual question answering. Citations - 774</td>
</tr>
<tr id="bib_santoro2017simple" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{santoro2017simple,
  author = {Santoro, Adam and Raposo, David and Barrett, David G and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  title = {A simple neural network module for relational reasoning},
  booktitle = {Advances in neural information processing systems},
  year = {2017},
  pages = {4967--4976},
  url = {https://papers.nips.cc/paper/7082-a-simple-neural-network-module-for-relational-reasoning.pdf}
}
</pre></td>
</tr>
<tr id="seide2011conversational" class="entry">
	<td>Seide, F., Li, G. and Yu, D.</td>
	<td>Conversational speech transcription using context-dependent deep neural networks <p class="infolinks">[<a href="javascript:toggleInfo('seide2011conversational','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('seide2011conversational','comment')">Comment</a>] [<a href="javascript:toggleInfo('seide2011conversational','bibtex')">BibTeX</a>]</p></td>
	<td>2011</td>
	<td>Twelfth annual conference of the international speech communication association&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CD-DNN-HMM-SWB-Interspeech2011-Pub.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_seide2011conversational" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We apply the recently proposed Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, to speech-to-texttranscription. For single-pass speaker-independent recognitionon the RT03S Fisher portion of phone-call transcription bench-mark (Switchboard), the word-error rate is reduced from 27.4%,obtained by discriminatively trained Gaussian-mixture HMMs,to 18.5%—a 33% relative improvement.CD-DNN-HMMs combine classic artificial-neural-networkHMMs with traditional tied-state triphones and deep-belief-network pre-training. They had previously been shown to re-duce errors by 16% relatively when trained on tens of hours ofdata using hundreds of tied states. This paper takes CD-DNN-HMMs further and applies them to transcription using over 300hours of training data, over 9000 tied states, and up to 9 hiddenlayers, and demonstrates how sparseness can be exploited.On four less well-matched transcription tasks, we observerelative error reductions of 22–28%.</td>
</tr>
<tr id="rev_seide2011conversational" class="comment noshow">
	<td colspan="6"><b>Comment</b>: DNN. Citations - 919.</td>
</tr>
<tr id="bib_seide2011conversational" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{seide2011conversational,
  author = {Seide, Frank and Li, Gang and Yu, Dong},
  title = {Conversational speech transcription using context-dependent deep neural networks},
  booktitle = {Twelfth annual conference of the international speech communication association},
  year = {2011},
  url = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CD-DNN-HMM-SWB-Interspeech2011-Pub.pdf}
}
</pre></td>
</tr>
<tr id="sennrich2015neural" class="entry">
	<td>Sennrich, R., Haddow, B. and Birch, A.</td>
	<td>Neural machine translation of rare words with subword units <p class="infolinks">[<a href="javascript:toggleInfo('sennrich2015neural','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('sennrich2015neural','comment')">Comment</a>] [<a href="javascript:toggleInfo('sennrich2015neural','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>arXiv preprint arXiv:1508.07909&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1508.07909.pdf)">URL</a>&nbsp;</td>
</tr>
<tr id="abs_sennrich2015neural" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words,<br>for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and<br>loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation<br>techniques, including simple character n -gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks<br>English →German and English →Russian by up to 1.1 and 1.3 BLEU, respectively</td>
</tr>
<tr id="rev_sennrich2015neural" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translations. Citations - 2667</td>
</tr>
<tr id="bib_sennrich2015neural" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{sennrich2015neural,
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  title = {Neural machine translation of rare words with subword units},
  journal = {arXiv preprint arXiv:1508.07909},
  year = {2015},
  url = {https://arxiv.org/pdf/1508.07909.pdf)}
}
</pre></td>
</tr>
<tr id="sennrich2017nematus" class="entry">
	<td>Sennrich, R., Firat, O., Cho, K., Birch, A., Haddow, B., Hitschler, J., Junczys-Dowmunt, M., L&auml;ubli, S., Barone, A.V.M., Mokry, J. and others</td>
	<td>Nematus: a toolkit for neural machine translation <p class="infolinks">[<a href="javascript:toggleInfo('sennrich2017nematus','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('sennrich2017nematus','comment')">Comment</a>] [<a href="javascript:toggleInfo('sennrich2017nematus','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>arXiv preprint arXiv:1703.04357&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1703.04357">URL</a>&nbsp;</td>
</tr>
<tr id="abs_sennrich2017nematus" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present Nematus, a toolkit for Neural Machine Translation. The toolkit prioritizes high translation accuracy, usability, and extensibility. Nematus has been used to build top-performing submissions to shared translation tasks at WMT and IWSLT, and has been used to train systems for production environments.</td>
</tr>
<tr id="rev_sennrich2017nematus" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translation. Citations - 316</td>
</tr>
<tr id="bib_sennrich2017nematus" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{sennrich2017nematus,
  author = {Sennrich, Rico and Firat, Orhan and Cho, Kyunghyun and Birch, Alexandra and Haddow, Barry and Hitschler, Julian and Junczys-Dowmunt, Marcin and L&auml;ubli, Samuel and Barone, Antonio Valerio Miceli and Mokry, Jozef and others},
  title = {Nematus: a toolkit for neural machine translation},
  journal = {arXiv preprint arXiv:1703.04357},
  year = {2017},
  url = {https://arxiv.org/pdf/1703.04357}
}
</pre></td>
</tr>
<tr id="shang2015neural" class="entry">
	<td>Shang, L., Lu, Z. and Li, H.</td>
	<td>Neural responding machine for short-text conversation <p class="infolinks">[<a href="javascript:toggleInfo('shang2015neural','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('shang2015neural','comment')">Comment</a>] [<a href="javascript:toggleInfo('shang2015neural','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>arXiv preprint arXiv:1503.02364&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1503.02364">URL</a>&nbsp;</td>
</tr>
<tr id="abs_shang2015neural" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the<br>generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.</td>
</tr>
<tr id="rev_shang2015neural" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Conversation modelling. Citations - 788</td>
</tr>
<tr id="bib_shang2015neural" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{shang2015neural,
  author = {Shang, Lifeng and Lu, Zhengdong and Li, Hang},
  title = {Neural responding machine for short-text conversation},
  journal = {arXiv preprint arXiv:1503.02364},
  year = {2015},
  url = {https://arxiv.org/pdf/1503.02364}
}
</pre></td>
</tr>
<tr id="shen2015minimum" class="entry">
	<td>Shen, S., Cheng, Y., He, Z., He, W., Wu, H., Sun, M. and Liu, Y.</td>
	<td>Minimum risk training for neural machine translation <p class="infolinks">[<a href="javascript:toggleInfo('shen2015minimum','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('shen2015minimum','comment')">Comment</a>] [<a href="javascript:toggleInfo('shen2015minimum','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>arXiv preprint arXiv:1512.02433&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1512.02433">URL</a>&nbsp;</td>
</tr>
<tr id="abs_shen2015minimum" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks.</td>
</tr>
<tr id="rev_shen2015minimum" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translations. Citations - 303</td>
</tr>
<tr id="bib_shen2015minimum" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{shen2015minimum,
  author = {Shen, Shiqi and Cheng, Yong and He, Zhongjun and He, Wei and Wu, Hua and Sun, Maosong and Liu, Yang},
  title = {Minimum risk training for neural machine translation},
  journal = {arXiv preprint arXiv:1512.02433},
  year = {2015},
  url = {https://arxiv.org/pdf/1512.02433}
}
</pre></td>
</tr>
<tr id="shen2018natural" class="entry">
	<td>Shen, J., Pang, R., Weiss, R.J., Schuster, M., Jaitly, N., Yang, Z., Chen, Z., Zhang, Y., Wang, Y., Skerrv-Ryan, R. and others</td>
	<td>Natural tts synthesis by conditioning wavenet on mel spectrogram predictions <p class="infolinks">[<a href="javascript:toggleInfo('shen2018natural','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('shen2018natural','comment')">Comment</a>] [<a href="javascript:toggleInfo('shen2018natural','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4779-4783&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://arxiv.org/pdf/1712.05884.pdf）-A">URL</a>&nbsp;</td>
</tr>
<tr id="abs_shen2018natural" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.</td>
</tr>
<tr id="rev_shen2018natural" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Wavenet. Citations - 614.</td>
</tr>
<tr id="bib_shen2018natural" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{shen2018natural,
  author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and others},
  title = {Natural tts synthesis by conditioning wavenet on mel spectrogram predictions},
  booktitle = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  year = {2018},
  pages = {4779--4783},
  url = {https://arxiv.org/pdf/1712.05884.pdf）-A}
}
</pre></td>
</tr>
<tr id="Silver2017" class="entry">
	<td>Silver, D., Schrittwieser, J., Simonyan, K., Antonoglou, I., Huang, A., Guez, A., Hubert, T., Baker, L., Lai, M., Bolton, A. and others</td>
	<td>Mastering the game of go without human knowledge <p class="infolinks">[<a href="javascript:toggleInfo('Silver2017','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Silver2017','comment')">Comment</a>] [<a href="javascript:toggleInfo('Silver2017','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>nature<br/>Vol. 550(7676), pp. 354-359&nbsp;</td>
	<td>article</td>
	<td><a href="https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Silver2017" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from selfplay. Here, we introduce an algorithm based solely on reinforcement learning, without human data, guidance, or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.</td>
</tr>
<tr id="rev_Silver2017" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Go game. CItation - 3950</td>
</tr>
<tr id="bib_Silver2017" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Silver2017,
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  title = {Mastering the game of go without human knowledge},
  journal = {nature},
  publisher = {Nature Publishing Group},
  year = {2017},
  volume = {550},
  number = {7676},
  pages = {354--359},
  url = {https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf}
}
</pre></td>
</tr>
<tr id="sutskever2014sequence" class="entry">
	<td>Sutskever, I., Vinyals, O. and Le, Q.V.</td>
	<td>Sequence to sequence learning with neural networks <p class="infolinks">[<a href="javascript:toggleInfo('sutskever2014sequence','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('sutskever2014sequence','comment')">Comment</a>] [<a href="javascript:toggleInfo('sutskever2014sequence','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>Advances in neural information processing systems, pp. 3104-3112&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_sutskever2014sequence" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its<br>BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but<br>not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.</td>
</tr>
<tr id="rev_sutskever2014sequence" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translation. Citations - 12084</td>
</tr>
<tr id="bib_sutskever2014sequence" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{sutskever2014sequence,
  author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  title = {Sequence to sequence learning with neural networks},
  booktitle = {Advances in neural information processing systems},
  year = {2014},
  pages = {3104--3112},
  url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf}
}
</pre></td>
</tr>
<tr id="Szegedy2013" class="entry">
	<td>Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. and Fergus, R.</td>
	<td>Intriguing properties of neural networks <p class="infolinks">[<a href="javascript:toggleInfo('Szegedy2013','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Szegedy2013','comment')">Comment</a>] [<a href="javascript:toggleInfo('Szegedy2013','bibtex')">BibTeX</a>]</p></td>
	<td>2013</td>
	<td>arXiv preprint arXiv:1312.6199&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1312.6199.pdf?source=post_page---------------------------">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Szegedy2013" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network’s prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.</td>
</tr>
<tr id="rev_Szegedy2013" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Properties of neural net. Citations - 5096</td>
</tr>
<tr id="bib_Szegedy2013" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Szegedy2013,
  author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  title = {Intriguing properties of neural networks},
  journal = {arXiv preprint arXiv:1312.6199},
  year = {2013},
  url = {https://arxiv.org/pdf/1312.6199.pdf?source=post_page---------------------------}
}
</pre></td>
</tr>
<tr id="Szegedy2015" class="entry">
	<td>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V. and Rabinovich, A.</td>
	<td>Going deeper with convolutions <p class="infolinks">[<a href="javascript:toggleInfo('Szegedy2015','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('Szegedy2015','comment')">Comment</a>] [<a href="javascript:toggleInfo('Szegedy2015','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1-9&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://arxiv.org/pdf/1508.04025)">URL</a>&nbsp;</td>
</tr>
<tr id="abs_Szegedy2015" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little<br>work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of<br>5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble<br>model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.<br>1</td>
</tr>
<tr id="rev_Szegedy2015" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translations. Citations - 4344</td>
</tr>
<tr id="bib_Szegedy2015" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{Szegedy2015,
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title = {Going deeper with convolutions},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2015},
  pages = {1--9},
  url = {https://arxiv.org/pdf/1508.04025)}
}
</pre></td>
</tr>
<tr id="tapaswi2016movieqa" class="entry">
	<td>Tapaswi, M., Zhu, Y., Stiefelhagen, R., Torralba, A., Urtasun, R. and Fidler, S.</td>
	<td>Movieqa: Understanding stories in movies through question-answering <p class="infolinks">[<a href="javascript:toggleInfo('tapaswi2016movieqa','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('tapaswi2016movieqa','comment')">Comment</a>] [<a href="javascript:toggleInfo('tapaswi2016movieqa','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4631-4640&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Tapaswi_MovieQA_Understanding_Stories_CVPR_2016_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_tapaswi2016movieqa" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We introduce the MovieQA dataset which aims to eval-uate automatic story comprehension from both video andtext. The dataset consists of 14,944 questions about 408movies with high semantic diversity. The questions rangefrom simpler “Who” did “What” to “Whom”, to “Why”and “How” certain events occurred. Each question comeswith a set of five possible answers; a correct one and fourdeceiving answers provided by human annotators.  Ourdataset is unique in that it contains multiple sources ofinformation – video clips, plots, subtitles, scripts, andDVS [32]. We analyze our data through various statisticsand methods.  We further extend existing QA techniquesto show that question-answering with such open-ended se-mantics is hard. We make this data set public along with anevaluation benchmark to encourage inspiring work in thischallenging domain.</td>
</tr>
<tr id="rev_tapaswi2016movieqa" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video question answering. Citations - 319</td>
</tr>
<tr id="bib_tapaswi2016movieqa" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{tapaswi2016movieqa,
  author = {Tapaswi, Makarand and Zhu, Yukun and Stiefelhagen, Rainer and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  title = {Movieqa: Understanding stories in movies through question-answering},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2016},
  pages = {4631--4640},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Tapaswi_MovieQA_Understanding_Stories_CVPR_2016_paper.pdf}
}
</pre></td>
</tr>
<tr id="teney2018tips" class="entry">
	<td>Teney, D., Anderson, P., He, X. and Van Den Hengel, A.</td>
	<td>Tips and tricks for visual question answering: Learnings from the 2017 challenge <p class="infolinks">[<a href="javascript:toggleInfo('teney2018tips','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('teney2018tips','comment')">Comment</a>] [<a href="javascript:toggleInfo('teney2018tips','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4223-4232&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Teney_Tips_and_Tricks_CVPR_2018_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_teney2018tips" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Deep Learning has had a transformative impact on Com-puter Vision, but for all of the success there is also a signif-icant cost. This is that the models and procedures used areso complex and intertwined that it is often impossible to dis-tinguish the impact of the individual design and engineer-ing choices each model embodies. This ambiguity divertsprogress in the field, and leads to a situation where devel-oping a state-of-the-art model is as much an art as a sci-ence. As a step towards addressing this problem we presenta massive exploration of the effects of the myriad architec-tural and hyperparameter choices that must be made in gen-erating a state-of-the-art model. The model is of particularinterest because it won the 2017 Visual Question AnsweringChallenge. We provide a detailed analysis of the impact ofeach choice on model performance, in the hope that it willinform others in developing models, but also that it mightset a precedent that will accelerate scientific progress in thefield.</td>
</tr>
<tr id="rev_teney2018tips" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Visual question answering. Citations - 186.</td>
</tr>
<tr id="bib_teney2018tips" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{teney2018tips,
  author = {Teney, Damien and Anderson, Peter and He, Xiaodong and Van Den Hengel, Anton},
  title = {Tips and tricks for visual question answering: Learnings from the 2017 challenge},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2018},
  pages = {4223--4232},
  url = {http://openaccess.thecvf.com/content_cvpr_2018/papers/Teney_Tips_and_Tricks_CVPR_2018_paper.pdf}
}
</pre></td>
</tr>
<tr id="van2016heiga" class="entry">
	<td>Van Den Oord, A. and Dieleman, S.</td>
	<td>Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio <p class="infolinks">[<a href="javascript:toggleInfo('van2016heiga','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>arXiv preprint arXiv:1609.03499<br/>Vol. 2(3)&nbsp;</td>
	<td>article</td>
	<td>&nbsp;</td>
</tr>
<tr id="bib_van2016heiga" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{van2016heiga,
  author = {Van Den Oord, Aaron and Dieleman, Sander},
  title = {Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio},
  journal = {arXiv preprint arXiv:1609.03499},
  year = {2016},
  volume = {2},
  number = {3}
}
</pre></td>
</tr>
<tr id="vendrov2015order" class="entry">
	<td>Vendrov, I., Kiros, R., Fidler, S. and Urtasun, R.</td>
	<td>Order-embeddings of images and language <p class="infolinks">[<a href="javascript:toggleInfo('vendrov2015order','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('vendrov2015order','comment')">Comment</a>] [<a href="javascript:toggleInfo('vendrov2015order','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>arXiv preprint arXiv:1511.06361&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1511.06361">URL</a>&nbsp;</td>
</tr>
<tr id="abs_vendrov2015order" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.</td>
</tr>
<tr id="rev_vendrov2015order" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. Citation - 350.</td>
</tr>
<tr id="bib_vendrov2015order" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{vendrov2015order,
  author = {Vendrov, Ivan and Kiros, Ryan and Fidler, Sanja and Urtasun, Raquel},
  title = {Order-embeddings of images and language},
  journal = {arXiv preprint arXiv:1511.06361},
  year = {2015},
  url = {https://arxiv.org/pdf/1511.06361}
}
</pre></td>
</tr>
<tr id="venugopalan2014translating" class="entry">
	<td>Venugopalan, S., Xu, H., Donahue, J., Rohrbach, M., Mooney, R. and Saenko, K.</td>
	<td>Translating videos to natural language using deep recurrent neural networks <p class="infolinks">[<a href="javascript:toggleInfo('venugopalan2014translating','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('venugopalan2014translating','comment')">Comment</a>] [<a href="javascript:toggleInfo('venugopalan2014translating','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>arXiv preprint arXiv:1412.4729&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1412.4729">URL</a>&nbsp;</td>
</tr>
<tr id="abs_venugopalan2014translating" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.</td>
</tr>
<tr id="rev_venugopalan2014translating" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video captioning. CItations - 722</td>
</tr>
<tr id="bib_venugopalan2014translating" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{venugopalan2014translating,
  author = {Venugopalan, Subhashini and Xu, Huijuan and Donahue, Jeff and Rohrbach, Marcus and Mooney, Raymond and Saenko, Kate},
  title = {Translating videos to natural language using deep recurrent neural networks},
  journal = {arXiv preprint arXiv:1412.4729},
  year = {2014},
  url = {https://arxiv.org/pdf/1412.4729}
}
</pre></td>
</tr>
<tr id="venugopalan2015sequence" class="entry">
	<td>Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T. and Saenko, K.</td>
	<td>Sequence to sequence-video to text <p class="infolinks">[<a href="javascript:toggleInfo('venugopalan2015sequence','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('venugopalan2015sequence','comment')">Comment</a>] [<a href="javascript:toggleInfo('venugopalan2015sequence','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Proceedings of the IEEE international conference on computer vision, pp. 4534-4542&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_iccv_2015/papers/Venugopalan_Sequence_to_Sequence_ICCV_2015_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_venugopalan2015sequence" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Real-world videos often have complex dynamics; andmethods for generating open-domain video descriptionsshould be sensitive to temporal structure and allow both in-put (sequence of frames) and output (sequence of words) ofvariable length. To approach this problem, we propose anovel end-to-end sequence-to-sequence model to generatecaptions for videos. For this we exploit recurrent neural net-works, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. OurLSTM model is trained on video-sentence pairs and learnsto associate a sequence of video frames to a sequence ofwords in order to generate a description of the event in thevideo clip. Our model naturally is able to learn the tem-poral structure of the sequence of frames as well as the se-quence model of the generated sentences, i.e. a languagemodel. We evaluate several variants of our model that ex-ploit different visual features on a standard set of YouTubevideos and two movie description datasets (M-VAD andMPII-MD).</td>
</tr>
<tr id="rev_venugopalan2015sequence" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video captioning. Citations - 919</td>
</tr>
<tr id="bib_venugopalan2015sequence" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{venugopalan2015sequence,
  author = {Venugopalan, Subhashini and Rohrbach, Marcus and Donahue, Jeffrey and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
  title = {Sequence to sequence-video to text},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  year = {2015},
  pages = {4534--4542},
  url = {http://openaccess.thecvf.com/content_iccv_2015/papers/Venugopalan_Sequence_to_Sequence_ICCV_2015_paper.pdf}
}
</pre></td>
</tr>
<tr id="venugopalan2015sequence" class="entry">
	<td>Venugopalan, S., Rohrbach, M., Donahue, J., Mooney, R., Darrell, T. and Saenko, K.</td>
	<td>Sequence to sequence-video to text <p class="infolinks">[<a href="javascript:toggleInfo('venugopalan2015sequence','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('venugopalan2015sequence','comment')">Comment</a>] [<a href="javascript:toggleInfo('venugopalan2015sequence','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Proceedings of the IEEE international conference on computer vision, pp. 4534-4542&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_iccv_2015/papers/Venugopalan_Sequence_to_Sequence_ICCV_2015_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_venugopalan2015sequence" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Real-world videos often have complex dynamics; methods for generating open-domain video descriptions should be senstive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).</td>
</tr>
<tr id="rev_venugopalan2015sequence" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video captioning. Citations - 919</td>
</tr>
<tr id="bib_venugopalan2015sequence" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{venugopalan2015sequence,
  author = {Venugopalan, Subhashini and Rohrbach, Marcus and Donahue, Jeffrey and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
  title = {Sequence to sequence-video to text},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  year = {2015},
  pages = {4534--4542},
  url = {http://openaccess.thecvf.com/content_iccv_2015/papers/Venugopalan_Sequence_to_Sequence_ICCV_2015_paper.pdf}
}
</pre></td>
</tr>
<tr id="vinyals2015neural" class="entry">
	<td>Vinyals, O. and Le, Q.</td>
	<td>A neural conversational model <p class="infolinks">[<a href="javascript:toggleInfo('vinyals2015neural','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('vinyals2015neural','comment')">Comment</a>] [<a href="javascript:toggleInfo('vinyals2015neural','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>arXiv preprint arXiv:1506.05869&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1506.05869.pdf)">URL</a>&nbsp;</td>
</tr>
<tr id="abs_vinyals2015neural" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.</td>
</tr>
<tr id="rev_vinyals2015neural" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Conversation modelling. Citations - 1385</td>
</tr>
<tr id="bib_vinyals2015neural" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{vinyals2015neural,
  author = {Vinyals, Oriol and Le, Quoc},
  title = {A neural conversational model},
  journal = {arXiv preprint arXiv:1506.05869},
  year = {2015},
  url = {https://arxiv.org/pdf/1506.05869.pdf)}
}
</pre></td>
</tr>
<tr id="vinyals2016show" class="entry">
	<td>Vinyals, O., Toshev, A., Bengio, S. and Erhan, D.</td>
	<td>Show and tell: Lessons learned from the 2015 mscoco image captioning challenge <p class="infolinks">[<a href="javascript:toggleInfo('vinyals2016show','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('vinyals2016show','comment')">Comment</a>] [<a href="javascript:toggleInfo('vinyals2016show','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>IEEE transactions on pattern analysis and machine intelligence<br/>Vol. 39(4), pp. 652-663&nbsp;</td>
	<td>article</td>
	<td><a href="https://ieeexplore.ieee.org/iel7/34/4359286/07505636.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_vinyals2016show" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research.</td>
</tr>
<tr id="rev_vinyals2016show" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. CItations - 496</td>
</tr>
<tr id="bib_vinyals2016show" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{vinyals2016show,
  author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  title = {Show and tell: Lessons learned from the 2015 mscoco image captioning challenge},
  journal = {IEEE transactions on pattern analysis and machine intelligence},
  publisher = {IEEE},
  year = {2016},
  volume = {39},
  number = {4},
  pages = {652--663},
  url = {https://ieeexplore.ieee.org/iel7/34/4359286/07505636.pdf}
}
</pre></td>
</tr>
<tr id="Wang2016" class="entry">
	<td>Wang, F.-Y., Zhang, J.J., Zheng, X., Wang, X., Yuan, Y., Dai, X., Zhang, J. and Yang, L.</td>
	<td>Where does AlphaGo go: From church-turing thesis to AlphaGo thesis and beyond <p class="infolinks">[<a href="javascript:toggleInfo('Wang2016','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>IEEE/CAA Journal of Automatica Sinica<br/>Vol. 3(2), pp. 113-120&nbsp;</td>
	<td>article</td>
	<td><a href="http://159.226.21.68/bitstream/173211/11604/1/Where%20does%20AlphaGo%20go%EF%BC%9A%20from%20Church-Turing%20Thesis%20to%20AlphaGo%20Thesis%20and%20beyond.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="bib_Wang2016" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{Wang2016,
  author = {Wang, Fei-Yue and Zhang, Jun Jason and Zheng, Xinhu and Wang, Xiao and Yuan, Yong and Dai, Xiaoxiao and Zhang, Jie and Yang, Liuqing},
  title = {Where does AlphaGo go: From church-turing thesis to AlphaGo thesis and beyond},
  journal = {IEEE/CAA Journal of Automatica Sinica},
  publisher = {IEEE},
  year = {2016},
  volume = {3},
  number = {2},
  pages = {113--120},
  url = {http://159.226.21.68/bitstream/173211/11604/1/Where%20does%20AlphaGo%20go%EF%BC%9A%20from%20Church-Turing%20Thesis%20to%20AlphaGo%20Thesis%20and%20beyond.pdf}
}
</pre></td>
</tr>
<tr id="wang2017gated" class="entry">
	<td>Wang, W., Yang, N., Wei, F., Chang, B. and Zhou, M.</td>
	<td>Gated self-matching networks for reading comprehension and question answering <p class="infolinks">[<a href="javascript:toggleInfo('wang2017gated','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('wang2017gated','comment')">Comment</a>] [<a href="javascript:toggleInfo('wang2017gated','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 189-198&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://www.aclweb.org/anthology/P17-1018.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_wang2017gated" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In  this  paper,  we  present  the  gated  self-matching  networks  for  reading  compre-hension  style  question  answering,  whichaims to answer questions from a given pas-sage. We first match the question and pas-sage with gated attention-based recurrentnetworks to obtain the question-aware pas-sage  representation.   Then  we  propose  aself-matching attention mechanism to re-fine  the  representation  by  matching  thepassage  against  itself,  which  effectivelyencodes information from the whole pas-sage.  We finally employ the pointer net-works  to  locate  the  positions  of  answersfrom the passages.  We conduct extensiveexperiments on the SQuAD dataset.  Thesingle model achieves 71.3% on the evalu-ation metrics of exact match on the hiddentest set, while the ensemble model furtherboosts the results to 75.9%. At the time ofsubmission of the paper, our model holdsthe first place on the SQuAD leaderboardfor both single and ensemble model</td>
</tr>
<tr id="rev_wang2017gated" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Question answering. Citations - 429</td>
</tr>
<tr id="bib_wang2017gated" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{wang2017gated,
  author = {Wang, Wenhui and Yang, Nan and Wei, Furu and Chang, Baobao and Zhou, Ming},
  title = {Gated self-matching networks for reading comprehension and question answering},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  year = {2017},
  pages = {189--198},
  url = {https://www.aclweb.org/anthology/P17-1018.pdf}
}
</pre></td>
</tr>
<tr id="wang2017r" class="entry">
	<td>Wang, S., Yu, M., Guo, X., Wang, Z., Klinger, T., Zhang, W., Chang, S., Tesauro, G., Zhou, B. and Jiang, J.</td>
	<td>R &circ; 3: Reinforced reader-ranker for open-domain question answering <p class="infolinks">[<a href="javascript:toggleInfo('wang2017r','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('wang2017r','comment')">Comment</a>] [<a href="javascript:toggleInfo('wang2017r','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>arXiv preprint arXiv:1709.00023&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1709.00023">URL</a>&nbsp;</td>
</tr>
<tr id="abs_wang2017r" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In recent years researchers have achieved considerable success applying neural network methods to question answering (QA). These approaches have achieved state of the art results in simplified closed-domain settings such as the SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected passage, from which the answer to a given question may be extracted. More recently, researchers have begun to tackle open-domain QA, in which the model is given a question and access to a large corpus (e.g., wikipedia) instead of a pre-selected passage (Chen et al., 2017a). This setting is more complex as it requires large-scale search for relevant passages by an information retrieval component, combined with a reading comprehension model that "reads" the passages to generate an answer to the question. Performance in this setting lags considerably behind closed-domain performance. In this paper, we present a novel open-domain QA system called Reinforced Ranker-Reader (R3), based on two algorithmic innovations. First, we propose a new pipeline for open-domain QA with a Ranker component, which learns to rank retrieved passages in terms of likelihood of generating the ground-truth answer to a given question. Second, we propose a novel method that jointly trains the Ranker along with an answer-generation Reader model, based on reinforcement learning. We report extensive experimental results showing that our method significantly improves on the state of the art for multiple open-domain QA datasets.</td>
</tr>
<tr id="rev_wang2017r" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Question answering. Citations - 150</td>
</tr>
<tr id="bib_wang2017r" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{wang2017r,
  author = {Wang, Shuohang and Yu, Mo and Guo, Xiaoxiao and Wang, Zhiguo and Klinger, Tim and Zhang, Wei and Chang, Shiyu and Tesauro, Gerald and Zhou, Bowen and Jiang, Jing},
  title = {R &circ; 3: Reinforced reader-ranker for open-domain question answering},
  journal = {arXiv preprint arXiv:1709.00023},
  year = {2017},
  url = {https://arxiv.org/pdf/1709.00023}
}
</pre></td>
</tr>
<tr id="wang2018video" class="entry">
	<td>Wang, X., Chen, W., Wu, J., Wang, Y.-F. and Yang Wang, W.</td>
	<td>Video captioning via hierarchical reinforcement learning <p class="infolinks">[<a href="javascript:toggleInfo('wang2018video','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('wang2018video','comment')">Comment</a>] [<a href="javascript:toggleInfo('wang2018video','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 4213-4222&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Video_Captioning_via_CVPR_2018_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_wang2018video" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Video captioning is the task of automatically generating a textual description of the actions in a video. Although previous work (e.g. sequence-to-sequence model) has shown promising results in abstracting a coarse description of a short video, it is still very challenging to caption a video containing multiple fine-grained actions with a detailed description. This paper aims to address the challenge by proposing a novel hierarchical reinforcement learning framework for video captioning, where a high-level Manager module learns to design sub-goals and a low-level Worker module recognizes the primitive actions to fulfill the sub-goal. With this compositional framework to reinforce video captioning at different levels, our approach significantly outperforms all the baseline methods on a newly introduced large-scale dataset for fine-grained video captioning. Furthermore, our non-ensemble model has already achieved the state-of-the-art results on the widely-used MSR-VTT dataset.</td>
</tr>
<tr id="rev_wang2018video" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video captioning. Citations - 103.</td>
</tr>
<tr id="bib_wang2018video" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{wang2018video,
  author = {Wang, Xin and Chen, Wenhu and Wu, Jiawei and Wang, Yuan-Fang and Yang Wang, William},
  title = {Video captioning via hierarchical reinforcement learning},
  booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  year = {2018},
  pages = {4213--4222},
  url = {http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Video_Captioning_via_CVPR_2018_paper.pdf}
}
</pre></td>
</tr>
<tr id="werbos1982applications" class="entry">
	<td>Werbos, P.J.</td>
	<td>Applications of advances in nonlinear sensitivity analysis <p class="infolinks">[<a href="javascript:toggleInfo('werbos1982applications','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('werbos1982applications','comment')">Comment</a>] [<a href="javascript:toggleInfo('werbos1982applications','bibtex')">BibTeX</a>]</p></td>
	<td>1982</td>
	<td>System modeling and optimization, pp. 762-770&nbsp;</td>
	<td>incollection</td>
	<td><a href="https://link.springer.com/chapter/10.1007/BFb0006203">URL</a>&nbsp;</td>
</tr>
<tr id="abs_werbos1982applications" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost. The areas of application include the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling. The details, references and derivations can be obtained by requesting „Sensitivity Analysis Methods for Nonlinear Systems“ from Forecast Analysis and Evaluation Team, Quality Assurance, OSS/EIA, Room 7413, Department of Energy, Washington, DC 20461.</td>
</tr>
<tr id="rev_werbos1982applications" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Nonlinear sensitivity. Citations - 284.</td>
</tr>
<tr id="bib_werbos1982applications" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@incollection{werbos1982applications,
  author = {Werbos, Paul J},
  title = {Applications of advances in nonlinear sensitivity analysis},
  booktitle = {System modeling and optimization},
  publisher = {Springer},
  year = {1982},
  pages = {762--770},
  url = {https://link.springer.com/chapter/10.1007/BFb0006203}
}
</pre></td>
</tr>
<tr id="williams2007partially" class="entry">
	<td>Williams, J.D. and Young, S.</td>
	<td>Partially observable Markov decision processes for spoken dialog systems <p class="infolinks">[<a href="javascript:toggleInfo('williams2007partially','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('williams2007partially','comment')">Comment</a>] [<a href="javascript:toggleInfo('williams2007partially','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>Computer Speech &amp; Language<br/>Vol. 21(2), pp. 393-422&nbsp;</td>
	<td>article</td>
	<td><a href="http://svr-www.eng.cam.ac.uk/~sjy/papers/wiyo07-j.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_williams2007partially" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In a spoken dialog system, determining which action a machine should take in a given situation is a difficult problem because automatic speech recognition is unreliable and hence the state of the conversation can never be known with certainty. Much of the research in spoken dialog systems centres on mitigating this uncertainty and recent work has focussed on three largely disparate techniques: parallel dialog state hypotheses, local use of confidence scores, and automated planning. While in isolation each of these approaches can improve action selection, taken together they currently lack a unified statistical framework that admits global optimization. In this paper we cast a spoken dialog system as a partially observable Markov decision process (POMDP). We show how this formulation unifies and extends existing techniques to form a single principled framework. A number of illustrations are used to show qualitatively the potential benefits of POMDPs compared to existing techniques, and empirical results from dialog simulations are presented which demonstrate significant quantitative gains. Finally, some of the key challenges to advancing this method – in particular scalability – are briefly outlined.</td>
</tr>
<tr id="rev_williams2007partially" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Markov decision process. Citations - 891</td>
</tr>
<tr id="bib_williams2007partially" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{williams2007partially,
  author = {Williams, Jason D and Young, Steve},
  title = {Partially observable Markov decision processes for spoken dialog systems},
  journal = {Computer Speech &amp; Language},
  publisher = {Elsevier},
  year = {2007},
  volume = {21},
  number = {2},
  pages = {393--422},
  url = {http://svr-www.eng.cam.ac.uk/&nbsp;sjy/papers/wiyo07-j.pdf}
}
</pre></td>
</tr>
<tr id="williams2007partially" class="entry">
	<td>Williams, J.D. and Young, S.</td>
	<td>Partially observable Markov decision processes for spoken dialog systems <p class="infolinks">[<a href="javascript:toggleInfo('williams2007partially','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('williams2007partially','comment')">Comment</a>] [<a href="javascript:toggleInfo('williams2007partially','bibtex')">BibTeX</a>]</p></td>
	<td>2007</td>
	<td>Computer Speech &amp; Language<br/>Vol. 21(2), pp. 393-422&nbsp;</td>
	<td>article</td>
	<td><a href="http://svr-www.eng.cam.ac.uk/~sjy/papers/wiyo07-j.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_williams2007partially" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: In a spoken dialog system, determining which action a machine should take in a given situation is a difficult problem because automatic speech recognition is unreliable and hence the state of the conversation can never be known with certainty. Much of the research in spoken dialog systems centres on mitigating this uncertainty and recent work has focussed on three largely disparate techniques: parallel dialog state hypotheses, local use of confidence scores, and automated planning. While in isolation each of these approaches can improve action selection, taken together they currently lack a unified statistical framework that admits global optimization. In this paper we cast a spoken dialog system as a partially observable Markov decision process (POMDP). We show how this formulation unifies and extends existing techniques to form a single principled framework. A number of illustrations are used to show qualitatively the potential benefits of POMDPs compared to existing techniques, and empirical results from dialog simulations are presented which demonstrate significant quantitative gains. Finally, some of the key challenges to advancing this method – in particular scalability – are briefly outlined.</td>
</tr>
<tr id="rev_williams2007partially" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Markov decision process. Citations - 891</td>
</tr>
<tr id="bib_williams2007partially" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{williams2007partially,
  author = {Williams, Jason D and Young, Steve},
  title = {Partially observable Markov decision processes for spoken dialog systems},
  journal = {Computer Speech &amp; Language},
  publisher = {Elsevier},
  year = {2007},
  volume = {21},
  number = {2},
  pages = {393--422},
  url = {http://svr-www.eng.cam.ac.uk/&nbsp;sjy/papers/wiyo07-j.pdf}
}
</pre></td>
</tr>
<tr id="wu2016google" class="entry">
	<td>Wu, Y., Schuster, M., Chen, Z., Le, Q.V., Norouzi, M., Macherey, W., Krikun, M., Cao, Y., Gao, Q., Macherey, K. and others</td>
	<td>Google's neural machine translation system: Bridging the gap between human and machine translation <p class="infolinks">[<a href="javascript:toggleInfo('wu2016google','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('wu2016google','comment')">Comment</a>] [<a href="javascript:toggleInfo('wu2016google','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>arXiv preprint arXiv:1609.08144&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1609.08144.pdf (7">URL</a>&nbsp;</td>
</tr>
<tr id="abs_wu2016google" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference – sometimes prohibitively so in the case of very large data sets and large models. Several authors have also charged that NMT systems lack robustness, particularly when input sentences contain rare words. These issues have hindered NMT’s use in practical deployments and services, where both accuracy and<br>speed are essential. In this work, we present GNMT, Google’s Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using residual connections as well as attention connections from the decoder network to the encoder. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (“wordpieces”) for both input and output. This method provides a good balance between the flexibility of “character”-delimited models and the efficiency of “word”-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. To directly optimize the translation BLEU scores, we consider refining the models by using reinforcement learning, but we found that the improvement in the BLEU scores did not reflect in the human evaluation. On the WMT’14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google’s phrase-based production system.</td>
</tr>
<tr id="rev_wu2016google" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translation. Citation - 3090</td>
</tr>
<tr id="bib_wu2016google" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{wu2016google,
  author = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  title = {Google's neural machine translation system: Bridging the gap between human and machine translation},
  journal = {arXiv preprint arXiv:1609.08144},
  year = {2016},
  url = {https://arxiv.org/pdf/1609.08144.pdf (7}
}
</pre></td>
</tr>
<tr id="xiang2015learning" class="entry">
	<td>Xiang, Y., Alahi, A. and Savarese, S.</td>
	<td>Learning to track: Online multi-object tracking by decision making <p class="infolinks">[<a href="javascript:toggleInfo('xiang2015learning','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('xiang2015learning','comment')">Comment</a>] [<a href="javascript:toggleInfo('xiang2015learning','bibtex')">BibTeX</a>]</p></td>
	<td>2015</td>
	<td>Proceedings of the IEEE international conference on computer vision, pp. 4705-4713&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xiang_Learning_to_Track_ICCV_2015_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_xiang2015learning" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Online Multi-Object Tracking (MOT) has wide appli-cations in time-critical video analysis scenarios, such asrobot navigation and autonomous driving.  In tracking-by-detection, a major challenge of online MOT is how torobustly associate noisy object detections on a new videoframe with previously tracked objects.  In this work, weformulate the online MOT problem as decision making inMarkov Decision Processes (MDPs), where the lifetime ofan object is modeled with a MDP. Learning a similarityfunction for data association is equivalent to learning a pol-icy for the MDP, and the policy learning is approached ina reinforcement learning fashion which benefits from bothadvantages of offline-learning and online-learning for dataassociation. Moreover, our framework can naturally handlethe birth/death and appearance/disappearance of targets bytreating them as state transitions in the MDP while leverag-ing existing online single object tracking methods. We con-duct experiments on the MOT Benchmark [24] to verify theeffectiveness of our method.</td>
</tr>
<tr id="rev_xiang2015learning" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Object tracking. Citations - 440.</td>
</tr>
<tr id="bib_xiang2015learning" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{xiang2015learning,
  author = {Xiang, Yu and Alahi, Alexandre and Savarese, Silvio},
  title = {Learning to track: Online multi-object tracking by decision making},
  booktitle = {Proceedings of the IEEE international conference on computer vision},
  year = {2015},
  pages = {4705--4713},
  url = {https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xiang_Learning_to_Track_ICCV_2015_paper.pdf}
}
</pre></td>
</tr>
<tr id="xiong2016dynamic" class="entry">
	<td>Xiong, C., Zhong, V. and Socher, R.</td>
	<td>Dynamic coattention networks for question answering <p class="infolinks">[<a href="javascript:toggleInfo('xiong2016dynamic','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('xiong2016dynamic','comment')">Comment</a>] [<a href="javascript:toggleInfo('xiong2016dynamic','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>arXiv preprint arXiv:1611.01604&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1611.01604">URL</a>&nbsp;</td>
</tr>
<tr id="abs_xiong2016dynamic" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.</td>
</tr>
<tr id="rev_xiong2016dynamic" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Question answering. Citations - 448</td>
</tr>
<tr id="bib_xiong2016dynamic" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{xiong2016dynamic,
  author = {Xiong, Caiming and Zhong, Victor and Socher, Richard},
  title = {Dynamic coattention networks for question answering},
  journal = {arXiv preprint arXiv:1611.01604},
  year = {2016},
  url = {https://arxiv.org/pdf/1611.01604}
}
</pre></td>
</tr>
<tr id="xiong2016dynamic" class="entry">
	<td>Xiong, C., Merity, S. and Socher, R.</td>
	<td>Dynamic memory networks for visual and textual question answering <p class="infolinks">[<a href="javascript:toggleInfo('xiong2016dynamic','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('xiong2016dynamic','comment')">Comment</a>] [<a href="javascript:toggleInfo('xiong2016dynamic','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>International conference on machine learning, pp. 2397-2406&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf?luicode=10000011&lfid=231522type=1&t=10&q=#ICML2016#&featurecode=newtitle
麻麻！这个小姐姐撩我！我要娶她٩( 'ω' )و&u=http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_xiong2016dynamic" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown<br>whether the architecture achieves strong results<br>for question answering when supporting facts are<br>not marked during training or whether it could<br>be applied to other modalities such as images.<br>Based on an analysis of the DMN, we propose<br>several improvements to its memory and input<br>modules. Together with these changes we introduce a novel input module for images in order<br>to be able to answer visual questions. Our new<br>DMN+ model improves the state of the art on<br>both the Visual Question Answering dataset and<br>the bAbI-10k text question-answering dataset<br>without supporting fact supervision.</td>
</tr>
<tr id="rev_xiong2016dynamic" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Question answering. Citation - 541</td>
</tr>
<tr id="bib_xiong2016dynamic" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{xiong2016dynamic,
  author = {Xiong, Caiming and Merity, Stephen and Socher, Richard},
  title = {Dynamic memory networks for visual and textual question answering},
  booktitle = {International conference on machine learning},
  year = {2016},
  pages = {2397--2406},
  url = {http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf?luicode=10000011&amp;lfid=231522type=1&amp;t=10&amp;q=#ICML2016#&amp;featurecode=newtitle<br>麻麻！这个小姐姐撩我！我要娶她٩( 'ω' )و&amp;u=http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf}
}
</pre></td>
</tr>
<tr id="xu2017video" class="entry">
	<td>Xu, D., Zhao, Z., Xiao, J., Wu, F., Zhang, H., He, X. and Zhuang, Y.</td>
	<td>Video question answering via gradually refined attention over appearance and motion <p class="infolinks">[<a href="javascript:toggleInfo('xu2017video','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('xu2017video','comment')">Comment</a>] [<a href="javascript:toggleInfo('xu2017video','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the 25th ACM international conference on Multimedia, pp. 1645-1653&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://www.comp.nus.edu.sg/~xiangnan/papers/mm17-videoQA.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_xu2017video" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recently image question answering (ImageQA) has gained lotsof attention in the research community. However, as its naturalextension, video question answering (VideoQA) is less explored.Although both tasks look similar, VideoQA is more challengingmainly because of the complexity and diversity of videos. As such,simply extending the ImageQA methods to videos is insufficient andsuboptimal. Particularly, working with the video needs to model itsinherent temporal structure and analyze the diverse informationit contains. In this paper, we consider exploiting the appearanceand motion information resided in the video with a novel attentionmechanism. More specifically, we propose an end-to-end modelwhich gradually refines its attention over the appearance and mo-tion features of the video using the question as guidance. Thequestion is processed word by word until the model generates thefinal optimized attention. The weighted representation of the video,as well as other contextual information, are used to generate theanswer. Extensive experiments show the advantages of our modelcompared to other baseline models. We also demonstrate the effec-tiveness of our model by analyzing the refined attention weightsduring the question answering procedure.</td>
</tr>
<tr id="rev_xu2017video" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video question answering. Citations - 59.</td>
</tr>
<tr id="bib_xu2017video" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{xu2017video,
  author = {Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  title = {Video question answering via gradually refined attention over appearance and motion},
  booktitle = {Proceedings of the 25th ACM international conference on Multimedia},
  year = {2017},
  pages = {1645--1653},
  url = {https://www.comp.nus.edu.sg/&nbsp;xiangnan/papers/mm17-videoQA.pdf}
}
</pre></td>
</tr>
<tr id="xue2017unifying" class="entry">
	<td>Xue, H., Zhao, Z. and Cai, D.</td>
	<td>Unifying the video and question attentions for open-ended video question answering <p class="infolinks">[<a href="javascript:toggleInfo('xue2017unifying','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('xue2017unifying','comment')">Comment</a>] [<a href="javascript:toggleInfo('xue2017unifying','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>IEEE Transactions on Image Processing<br/>Vol. 26(12), pp. 5656-5666&nbsp;</td>
	<td>article</td>
	<td><a href="https://ieeexplore.ieee.org/abstract/document/8017608/">URL</a>&nbsp;</td>
</tr>
<tr id="abs_xue2017unifying" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Video question answering is an important task toward scene understanding and visual data retrieval. However, current visual question answering works mainly focus on a single static image, which is distinct from the dynamic and sequential visual data in the real world. Their approaches cannot utilize the temporal information in videos. In this paper, we introduce the task of free-form open-ended video question answering. The open-ended answers enable wider applications compared with the common multiple-choice tasks in Visual-QA. We first propose a data set for open-ended Video-QA with the automatic question generation approaches. Then, we propose our sequential video attention and temporal question attention models. These two models apply the attention mechanism on videos and questions, while preserving the sequential and temporal structures of the guides. The two models are integrated into the model of unified attention. After the video and the question are encoded, the answers are generated wordwisely from our models by a decoder. In the end, we evaluate our models on the proposed data set. The experimental results demonstrate the effectiveness of our proposed model.</td>
</tr>
<tr id="rev_xue2017unifying" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video question answering. Citations - 25.</td>
</tr>
<tr id="bib_xue2017unifying" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{xue2017unifying,
  author = {Xue, Hongyang and Zhao, Zhou and Cai, Deng},
  title = {Unifying the video and question attentions for open-ended video question answering},
  journal = {IEEE Transactions on Image Processing},
  publisher = {IEEE},
  year = {2017},
  volume = {26},
  number = {12},
  pages = {5656--5666},
  url = {https://ieeexplore.ieee.org/abstract/document/8017608/}
}
</pre></td>
</tr>
<tr id="yang2017improved" class="entry">
	<td>Yang, Z., Hu, Z., Salakhutdinov, R. and Berg-Kirkpatrick, T.</td>
	<td>Improved variational autoencoders for text modeling using dilated convolutions <p class="infolinks">[<a href="javascript:toggleInfo('yang2017improved','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('yang2017improved','comment')">Comment</a>] [<a href="javascript:toggleInfo('yang2017improved','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>arXiv preprint arXiv:1702.08139&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1702.08139">URL</a>&nbsp;</td>
</tr>
<tr id="abs_yang2017improved" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.</td>
</tr>
<tr id="rev_yang2017improved" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Variational autoencoders. Citations - 198.</td>
</tr>
<tr id="bib_yang2017improved" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{yang2017improved,
  author = {Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and Berg-Kirkpatrick, Taylor},
  title = {Improved variational autoencoders for text modeling using dilated convolutions},
  journal = {arXiv preprint arXiv:1702.08139},
  year = {2017},
  url = {https://arxiv.org/pdf/1702.08139}
}
</pre></td>
</tr>
<tr id="yao2017boosting" class="entry">
	<td>Yao, T., Pan, Y., Li, Y., Qiu, Z. and Mei, T.</td>
	<td>Boosting image captioning with attributes <p class="infolinks">[<a href="javascript:toggleInfo('yao2017boosting','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('yao2017boosting','comment')">Comment</a>] [<a href="javascript:toggleInfo('yao2017boosting','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the IEEE International Conference on Computer Vision, pp. 4894-4902&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Yao_Boosting_Image_Captioning_ICCV_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_yao2017boosting" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Automatically describing an image with a natural lan-guage has been an emerging challenge in both fields ofcomputer vision and natural language processing. In thispaper, we present Long Short-Term Memory with Attributes(LSTM-A) - a novel architecture that integrates attributesinto the successful Convolutional Neural Networks (CNNs)plus Recurrent Neural Networks (RNNs) image captioningframework, by training them in an end-to-end manner. Par-ticularly, the learning of attributes is strengthened by in-tegrating inter-attribute correlations into Multiple InstanceLearning (MIL). To incorporate attributes into captioning,we construct variants of architectures by feeding imagerepresentations and attributes into RNNs in different waysto explore the mutual but also fuzzy relationship betweenthem. Extensive experiments are conducted on COCO im-age captioning dataset and our framework shows clear im-provements when compared to state-of-the-art deep mod-els.  More remarkably, we obtain METEOR/CIDEr-D of25.5%/100.2% on testing data of widely used and publiclyavailable splits in [10] when extracting image representa-tions by GoogleNet and achieve superior performance onCOCO captioning Leaderboard.</td>
</tr>
<tr id="rev_yao2017boosting" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. Citations - 347</td>
</tr>
<tr id="bib_yao2017boosting" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{yao2017boosting,
  author = {Yao, Ting and Pan, Yingwei and Li, Yehao and Qiu, Zhaofan and Mei, Tao},
  title = {Boosting image captioning with attributes},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  year = {2017},
  pages = {4894--4902},
  url = {http://openaccess.thecvf.com/content_ICCV_2017/papers/Yao_Boosting_Image_Captioning_ICCV_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="yao2017boosting" class="entry">
	<td>Yao, T., Pan, Y., Li, Y., Qiu, Z. and Mei, T.</td>
	<td>Boosting image captioning with attributes <p class="infolinks">[<a href="javascript:toggleInfo('yao2017boosting','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('yao2017boosting','comment')">Comment</a>] [<a href="javascript:toggleInfo('yao2017boosting','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>Proceedings of the IEEE International Conference on Computer Vision, pp. 4894-4902&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_ICCV_2017/papers/Yao_Boosting_Image_Captioning_ICCV_2017_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_yao2017boosting" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. Particularly, the learning of attributes is strengthened by integrating inter-attribute correlations into Multiple Instance Learning (MIL). To incorporate attributes into captioning, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework shows clear improvements when compared to state-of-the-art deep models. More remarkably, we obtain METEOR/CIDEr-D of 25.5%/100.2% on testing data of widely used and publicly available splits in [10] when extracting image representations by GoogleNet and achieve superior performance on COCO captioning Leaderboard.</td>
</tr>
<tr id="rev_yao2017boosting" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. Citations - 347.</td>
</tr>
<tr id="bib_yao2017boosting" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{yao2017boosting,
  author = {Yao, Ting and Pan, Yingwei and Li, Yehao and Qiu, Zhaofan and Mei, Tao},
  title = {Boosting image captioning with attributes},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  year = {2017},
  pages = {4894--4902},
  url = {http://openaccess.thecvf.com/content_ICCV_2017/papers/Yao_Boosting_Image_Captioning_ICCV_2017_paper.pdf}
}
</pre></td>
</tr>
<tr id="yao2018exploring" class="entry">
	<td>Yao, T., Pan, Y., Li, Y. and Mei, T.</td>
	<td>Exploring visual relationship for image captioning <p class="infolinks">[<a href="javascript:toggleInfo('yao2018exploring','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('yao2018exploring','comment')">Comment</a>] [<a href="javascript:toggleInfo('yao2018exploring','bibtex')">BibTeX</a>]</p></td>
	<td>2018</td>
	<td>Proceedings of the European conference on computer vision (ECCV), pp. 684-699&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_ECCV_2018/papers/Ting_Yao_Exploring_Visual_Relationship_ECCV_2018_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_yao2018exploring" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: It is always well believed that modeling relationships be-tween objects would be helpful for representing and eventually describingan image. Nevertheless, there has not been evidence in support of the ideaon image description generation. In this paper, we introduce a new designto explore the connections between objects for image captioning underthe umbrella of attention-based encoder-decoder framework. Specifically,we present Graph Convolutional Networks plus Long Short-Term Mem-ory (dubbed as GCN-LSTM) architecture that novelly integrates bothsemantic and spatial object relationships into image encoder.Technical-ly, we build graphs over the detected objects in an image based on theirspatial and semantic connections. The representations of each region pro-posed on objects are then refined by leveraging graph structure throughGCN. With the learnt region-level features, our GCN-LSTM capitalizeson LSTM-based captioning framework with attention mechanismfor sen-tence generation. Extensive experiments are conducted on COCOimagecaptioning dataset, and superior results are reported when comparingto state-of-the-art approaches. More remarkably, GCN-LSTM increasesCIDEr-D performance from 120.1% to 128.7% on COCO testing set.</td>
</tr>
<tr id="rev_yao2018exploring" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. CItations - 103</td>
</tr>
<tr id="bib_yao2018exploring" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{yao2018exploring,
  author = {Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
  title = {Exploring visual relationship for image captioning},
  booktitle = {Proceedings of the European conference on computer vision (ECCV)},
  year = {2018},
  pages = {684--699},
  url = {http://openaccess.thecvf.com/content_ECCV_2018/papers/Ting_Yao_Exploring_Visual_Relationship_ECCV_2018_paper.pdf}
}
</pre></td>
</tr>
<tr id="you2016image" class="entry">
	<td>You, Q., Jin, H., Wang, Z., Fang, C. and Luo, J.</td>
	<td>Image captioning with semantic attention <p class="infolinks">[<a href="javascript:toggleInfo('you2016image','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('you2016image','comment')">Comment</a>] [<a href="javascript:toggleInfo('you2016image','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4651-4659&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/You_Image_Captioning_With_CVPR_2016_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_you2016image" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.</td>
</tr>
<tr id="rev_you2016image" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Image captioning. Citations - 934</td>
</tr>
<tr id="bib_you2016image" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{you2016image,
  author = {You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
  title = {Image captioning with semantic attention},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2016},
  pages = {4651--4659},
  url = {http://openaccess.thecvf.com/content_cvpr_2016/papers/You_Image_Captioning_With_CVPR_2016_paper.pdf}
}
</pre></td>
</tr>
<tr id="yu2014deep" class="entry">
	<td>Yu, L., Hermann, K.M., Blunsom, P. and Pulman, S.</td>
	<td>Deep learning for answer sentence selection <p class="infolinks">[<a href="javascript:toggleInfo('yu2014deep','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('yu2014deep','comment')">Comment</a>] [<a href="javascript:toggleInfo('yu2014deep','bibtex')">BibTeX</a>]</p></td>
	<td>2014</td>
	<td>arXiv preprint arXiv:1412.1632&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1412.1632">URL</a>&nbsp;</td>
</tr>
<tr id="abs_yu2014deep" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Answer sentence selection is the task of identifying sentences that contain the answer to a given question. This is an important problem in its own right as well as in the larger context of open domain question answering. We propose a novel approach to solving this task via means of distributed representations, and learn to match questions with answers by considering their semantic encoding. This contrasts prior work on this task, which typically relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources. Our approach does not require any feature engineering nor does it involve specialist linguistic data, making this model easily applicable to a wide range of domains and languages. Experimental results on a standard benchmark dataset from TREC demonstrate that---despite its simplicity---our model matches state of the art performance on the answer sentence selection task.</td>
</tr>
<tr id="rev_yu2014deep" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Question answering. Citations - 369</td>
</tr>
<tr id="bib_yu2014deep" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{yu2014deep,
  author = {Yu, Lei and Hermann, Karl Moritz and Blunsom, Phil and Pulman, Stephen},
  title = {Deep learning for answer sentence selection},
  journal = {arXiv preprint arXiv:1412.1632},
  year = {2014},
  url = {https://arxiv.org/pdf/1412.1632}
}
</pre></td>
</tr>
<tr id="yu2016video" class="entry">
	<td>Yu, H., Wang, J., Huang, Z., Yang, Y. and Xu, W.</td>
	<td>Video paragraph captioning using hierarchical recurrent neural networks <p class="infolinks">[<a href="javascript:toggleInfo('yu2016video','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('yu2016video','comment')">Comment</a>] [<a href="javascript:toggleInfo('yu2016video','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4584-4593&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="http://openaccess.thecvf.com/content_cvpr_2016/papers/Yu_Video_Paragraph_Captioning_CVPR_2016_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_yu2016video" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We present an approach that exploits hierarchical Recur-rent Neural Networks (RNNs) to tackle the video captioningproblem, i.e., generating one or multiple sentences to de-scribe a realistic video. Our hierarchical framework con-tains a sentence generator and a paragraph generator. Thesentence generator produces one simple short sentence thatdescribes a specific short video interval. It exploits bothtemporal- and spatial-attention mechanisms to selectivelyfocus on visual elements during generation. The paragraphgenerator captures the inter-sentence dependency by takingas input the sentential embedding produced by the sentencegenerator, combining it with the paragraph history, andoutputting the new initial state for the sentence generator.We evaluate our approach on two large-scale benchmarkdatasets: YouTubeClips and TACoS-MultiLevel. The exper-iments demonstrate that our approach significantly outper-forms the current state-of-the-art methods with BLEU@4scores 0.499 and 0.305 respectively.</td>
</tr>
<tr id="rev_yu2016video" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video captioning. Citations - 413</td>
</tr>
<tr id="bib_yu2016video" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{yu2016video,
  author = {Yu, Haonan and Wang, Jiang and Huang, Zhiheng and Yang, Yi and Xu, Wei},
  title = {Video paragraph captioning using hierarchical recurrent neural networks},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2016},
  pages = {4584--4593},
  url = {http://openaccess.thecvf.com/content_cvpr_2016/papers/Yu_Video_Paragraph_Captioning_CVPR_2016_paper.pdf}
}
</pre></td>
</tr>
<tr id="yuan2016grammatical" class="entry">
	<td>Yuan, Z. and Briscoe, T.</td>
	<td>Grammatical error correction using neural machine translation <p class="infolinks">[<a href="javascript:toggleInfo('yuan2016grammatical','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('yuan2016grammatical','comment')">Comment</a>] [<a href="javascript:toggleInfo('yuan2016grammatical','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 380-386&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://www.aclweb.org/anthology/N16-1042.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_yuan2016grammatical" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: This paper presents the first study using neural machine translation (NMT) for grammatical error correction (GEC). We propose a twostep approach to handle the rare word problem in NMT, which has been proved to be useful and effective for the GEC task. Our best NMTbased system trained on the CLC outperforms our SMT-based system when testing on the publicly available FCE test set. The same system achieves an F0.5 score of 39.90% on the CoNLL-2014 shared task test set, outperforming the state-of-the-art and demonstrating that the NMT-based GEC system generalises effectively.</td>
</tr>
<tr id="rev_yuan2016grammatical" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Machine translations. Citations - 118</td>
</tr>
<tr id="bib_yuan2016grammatical" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{yuan2016grammatical,
  author = {Yuan, Zheng and Briscoe, Ted},
  title = {Grammatical error correction using neural machine translation},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  year = {2016},
  pages = {380--386},
  url = {https://www.aclweb.org/anthology/N16-1042.pdf}
}
</pre></td>
</tr>
<tr id="zeng2016leveraging" class="entry">
	<td>Zeng, K.-H., Chen, T.-H., Chuang, C.-Y., Liao, Y.-H., Niebles, J.C. and Sun, M.</td>
	<td>Leveraging video descriptions to learn video question answering <p class="infolinks">[<a href="javascript:toggleInfo('zeng2016leveraging','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('zeng2016leveraging','comment')">Comment</a>] [<a href="javascript:toggleInfo('zeng2016leveraging','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>arXiv preprint arXiv:1611.04021&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1611.04021">URL</a>&nbsp;</td>
</tr>
<tr id="abs_zeng2016leveraging" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose a scalable approach to learn video-based question answering (QA): answer a "free-form natural language question" about a video content. Our approach automatically harvests a large number of videos and descriptions freely available online. Then, a large number of candidate QA pairs are automatically generated from descriptions rather than manually annotated. Next, we use these candidate QA pairs to train a number of video-based QA methods extended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et al. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect candidate QA pairs, we propose a self-paced learning procedure to iteratively identify them and mitigate their effects in training. Finally, we evaluate performance on manually generated video-based QA pairs. The results show that our self-paced learning procedure is effective, and the extended SS model outperforms various baselines.</td>
</tr>
<tr id="rev_zeng2016leveraging" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video question answering. Citations - 74</td>
</tr>
<tr id="bib_zeng2016leveraging" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{zeng2016leveraging,
  author = {Zeng, Kuo-Hao and Chen, Tseng-Hung and Chuang, Ching-Yao and Liao, Yuan-Hong and Niebles, Juan Carlos and Sun, Min},
  title = {Leveraging video descriptions to learn video question answering},
  journal = {arXiv preprint arXiv:1611.04021},
  year = {2016},
  url = {https://arxiv.org/pdf/1611.04021}
}
</pre></td>
</tr>
<tr id="zhang2016summary" class="entry">
	<td>Zhang, K., Chao, W.-L., Sha, F. and Grauman, K.</td>
	<td>Summary transfer: Exemplar-based subset selection for video summarization <p class="infolinks">[<a href="javascript:toggleInfo('zhang2016summary','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('zhang2016summary','comment')">Comment</a>] [<a href="javascript:toggleInfo('zhang2016summary','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 1059-1067&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhang_Summary_Transfer_Exemplar-Based_CVPR_2016_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_zhang2016summary" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Video summarization has unprecedented importance to help us digest, browse, and search today's ever-growing video collections. We propose a novel subset selection technique that leverages supervision in the form of human-created summaries to perform automatic keyframe-based video summarization. The main idea is to nonparametrically transfer summary structures from annotated videos to unseen test videos. We show how to extend our method to exploit semantic side information about the video's category/genre to guide the transfer process by those training videos semantically consistent with the test input. We also show how to generalize our method to subshot-based summarization, which not only reduces computational costs but also provides more flexible ways of defining visual similarity across subshots spanning several frames. We conduct extensive evaluation on several benchmarks and demonstrate promising results, outperforming existing methods in several settings.</td>
</tr>
<tr id="rev_zhang2016summary" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video summarization. Citation - 146.</td>
</tr>
<tr id="bib_zhang2016summary" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{zhang2016summary,
  author = {Zhang, Ke and Chao, Wei-Lun and Sha, Fei and Grauman, Kristen},
  title = {Summary transfer: Exemplar-based subset selection for video summarization},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  year = {2016},
  pages = {1059--1067},
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhang_Summary_Transfer_Exemplar-Based_CVPR_2016_paper.pdf}
}
</pre></td>
</tr>
<tr id="zhang2016video" class="entry">
	<td>Zhang, K., Chao, W.-L., Sha, F. and Grauman, K.</td>
	<td>Video summarization with long short-term memory <p class="infolinks">[<a href="javascript:toggleInfo('zhang2016video','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('zhang2016video','comment')">Comment</a>] [<a href="javascript:toggleInfo('zhang2016video','bibtex')">BibTeX</a>]</p></td>
	<td>2016</td>
	<td>European conference on computer vision, pp. 766-782&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://arxiv.org/pdf/1605.08110">URL</a>&nbsp;</td>
</tr>
<tr id="abs_zhang2016video" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose a novel supervised learning technique for summarizing videos by automatically selecting keyframes or key subshots. Casting the task as a structured prediction problem, our main idea is to use Long Short-Term Memory (LSTM) to model the variable-range temporal dependency among video frames, so as to derive both representative and compact video summaries. The proposed model successfully accounts for the sequential structure crucial to generating meaningful video summaries, leading to state-of-the-art results on two benchmark datasets. In addition to advances in modeling techniques, we introduce a strategy to address the need for a large amount of annotated data for training complex learning approaches to summarization. There, our main idea is to exploit auxiliary annotated video summarization datasets, in spite of their heterogeneity in visual styles and contents. Specifically, we show that domain adaptation techniques can improve learning by reducing the discrepancies in the original datasets’ statistical properties.</td>
</tr>
<tr id="rev_zhang2016video" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video summarization. Citation - 333.</td>
</tr>
<tr id="bib_zhang2016video" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{zhang2016video,
  author = {Zhang, Ke and Chao, Wei-Lun and Sha, Fei and Grauman, Kristen},
  title = {Video summarization with long short-term memory},
  booktitle = {European conference on computer vision},
  year = {2016},
  pages = {766--782},
  url = {https://arxiv.org/pdf/1605.08110}
}
</pre></td>
</tr>
<tr id="zhao2017video" class="entry">
	<td>Zhao, Z., Yang, Q., Cai, D., He, X., Zhuang, Y., Zhao, Z., Yang, Q., Cai, D., He, X. and Zhuang, Y.</td>
	<td>Video Question Answering via Hierarchical Spatio-Temporal Attention Networks. <p class="infolinks">[<a href="javascript:toggleInfo('zhao2017video','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('zhao2017video','comment')">Comment</a>] [<a href="javascript:toggleInfo('zhao2017video','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>IJCAI, pp. 3518-3524&nbsp;</td>
	<td>inproceedings</td>
	<td><a href="https://pdfs.semanticscholar.org/834d/688a4a8fb893e081fcf20ff52f989b21cc17.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_zhao2017video" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: Open-ended  video  question  answering  is  a  chal-lenging  problem  in  visual  information  retrieval,which automatically generates the natural languageanswer from the referenced video content accord-ing  to  the  question.However,  the  existing  vi-sual  question  answering  works  only  focus  on  thestatic  image,  which  may  be  ineffectively  appliedto  video  question  answering  due  to  the  lack  ofmodeling  the  temporal  dynamics  of  video  con-tents.In  this  paper,  we  consider  the  problemof open-ended video question answering from theviewpoint  of  spatio-temporal  attentional  encoder-decoder learning framework.   We propose the hi-erarchical  spatio-temporal  attention  network  forlearning  the  joint  representation  of  the  dynamicvideo  contents  according  to  the  given  question.We  then  develop  the  spatio-temporal  attentionalencoder-decoder  learning  method  with  multi-stepreasoning  process  for  open-ended  video  questionanswering.  We construct a large-scale video ques-tion answering dataset.  The extensive experimentsshow the effectiveness of our method</td>
</tr>
<tr id="rev_zhao2017video" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video question answering. Citations - 51.</td>
</tr>
<tr id="bib_zhao2017video" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@inproceedings{zhao2017video,
  author = {Zhao, Zhou and Yang, Qifan and Cai, Deng and He, Xiaofei and Zhuang, Yueting and Zhao, Zhou and Yang, Qifan and Cai, Deng and He, Xiaofei and Zhuang, Yueting},
  title = {Video Question Answering via Hierarchical Spatio-Temporal Attention Networks.},
  booktitle = {IJCAI},
  year = {2017},
  pages = {3518--3524},
  url = {https://pdfs.semanticscholar.org/834d/688a4a8fb893e081fcf20ff52f989b21cc17.pdf}
}
</pre></td>
</tr>
<tr id="zhu2017uncovering" class="entry">
	<td>Zhu, L., Xu, Z., Yang, Y. and Hauptmann, A.G.</td>
	<td>Uncovering the temporal context for video question answering <p class="infolinks">[<a href="javascript:toggleInfo('zhu2017uncovering','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('zhu2017uncovering','comment')">Comment</a>] [<a href="javascript:toggleInfo('zhu2017uncovering','bibtex')">BibTeX</a>]</p></td>
	<td>2017</td>
	<td>International Journal of Computer Vision<br/>Vol. 124(3), pp. 409-421&nbsp;</td>
	<td>article</td>
	<td><a href="https://arxiv.org/pdf/1511.04670">URL</a>&nbsp;</td>
</tr>
<tr id="abs_zhu2017uncovering" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: n  this  work,  we  introduce  Video  Question  Answeringin temporal domain to infer the past, describe the presentand predict the future.  We present an encoder-decoder ap-proach using Recurrent Neural Networks to learn tempo-ral structures of videos and introduce a dual-channel rank-ing loss to answer multiple-choice questions.   We exploreapproaches for finer understanding of video content usingquestion form of “fill-in-the-blank”,  and managed to col-lect  109,895  video  clips  with  duration  over  1,000  hoursfrom  TACoS,  MPII-MD,  MEDTest  14  datasets,  while  thecorresponding  390,744  questions  are  generated  from  an-notations.  Extensive experiments demonstrate that our ap-proach significantly outperforms the compared baselines.</td>
</tr>
<tr id="rev_zhu2017uncovering" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Video captioning. Citations - 135.</td>
</tr>
<tr id="bib_zhu2017uncovering" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{zhu2017uncovering,
  author = {Zhu, Linchao and Xu, Zhongwen and Yang, Yi and Hauptmann, Alexander G},
  title = {Uncovering the temporal context for video question answering},
  journal = {International Journal of Computer Vision},
  publisher = {Springer},
  year = {2017},
  volume = {124},
  number = {3},
  pages = {409--421},
  url = {https://arxiv.org/pdf/1511.04670}
}
</pre></td>
</tr>
<tr id="" class="entry">
	<td></td>
	<td> <p class="infolinks">[<a href="javascript:toggleInfo('','abstract')">Abstract</a>] [<a href="javascript:toggleInfo('','comment')">Comment</a>] [<a href="javascript:toggleInfo('','bibtex')">BibTeX</a>]</p></td>
	<td></td>
	<td>&nbsp;</td>
	<td>article</td>
	<td><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">URL</a>&nbsp;</td>
</tr>
<tr id="abs_" class="abstract noshow">
	<td colspan="6"><b>Abstract</b>: We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.</td>
</tr>
<tr id="rev_" class="comment noshow">
	<td colspan="6"><b>Comment</b>: Inceptionnet. Citations - 23666</td>
</tr>
<tr id="bib_" class="bibtex noshow">
<td colspan="6"><b>BibTeX</b>:
<pre>
@article{,,
  url = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf}
}
</pre></td>
</tr>
</tbody>
</table>
<footer>
 <small>Created by <a href="http://jabref.sourceforge.net">JabRef</a> on 10/09/2020.</small>
</footer>
<!-- file generated by JabRef -->
</body>
</html>