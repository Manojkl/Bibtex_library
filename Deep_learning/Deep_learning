% This file was created with JabRef 2.10.
% Encoding: UTF-8


@InProceedings{antol2015vqa,
  Title                    = {Vqa: Visual question answering},
  Author                   = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C and Parikh, Devi},
  Booktitle                = {Proceedings of the IEEE international conference on computer vision},
  Year                     = {2015},
  Pages                    = {2425--2433},

  Abstract                 = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.},
  Comment                  = {VQA dataset. Citations - 2048},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {http://openaccess.thecvf.com/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf}
}

@Article{badrinarayanan2017segnet,
  Title                    = {Segnet: A deep convolutional encoder-decoder architecture for image segmentation},
  Author                   = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  Journal                  = {IEEE transactions on pattern analysis and machine intelligence},
  Year                     = {2017},
  Number                   = {12},
  Pages                    = {2481--2495},
  Volume                   = {39},

  Abstract                 = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
  Comment                  = {SegNet. CItations - 5459},
  Owner                    = {manoj},
  Publisher                = {IEEE},
  Timestamp                = {2020.08.26},
  Url                      = {https://ieeexplore.ieee.org/iel7/34/4359286/07803544.pdf}
}

@Article{chen2017deeplab,
  Title                    = {Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs},
  Author                   = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  Journal                  = {IEEE transactions on pattern analysis and machine intelligence},
  Year                     = {2017},
  Number                   = {4},
  Pages                    = {834--848},
  Volume                   = {40},

  Abstract                 = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or `atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed “DeepLab” system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
  Comment                  = {Semantic image segementation. Citations - 5527},
  Owner                    = {manoj},
  Publisher                = {IEEE},
  Timestamp                = {2020.08.26},
  Url                      = {https://arxiv.org/pdf/1606.00915}
}

@Article{chen2014semantic,
  Title                    = {Semantic image segmentation with deep convolutional nets and fully connected crfs},
  Author                   = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  Journal                  = {arXiv preprint arXiv:1412.7062},
  Year                     = {2014},

  Abstract                 = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called ”semantic image segmentation”). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our “DeepLab” system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the ’hole’ algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
  Comment                  = {Fully connected network for segmentation. Citations - 2621},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://arxiv.org/pdf/1412.7062}
}

@InProceedings{cordts2016cityscapes,
  Title                    = {The cityscapes dataset for semantic urban scene understanding},
  Author                   = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2016},
  Pages                    = {3213--3223},

  Abstract                 = {Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark},
  Comment                  = {Cityscapes dataset. CItations - 3405},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2016/papers/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.pdf}
}

@InProceedings{deng2009imagenet,
  Title                    = {Imagenet: A large-scale hierarchical image database},
  Author                   = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  Booktitle                = {2009 IEEE conference on computer vision and pattern recognition},
  Year                     = {2009},
  Organization             = {Ieee},
  Pages                    = {248--255},

  Abstract                 = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500- 1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  Comment                  = {Imagenet. Citations - 20648},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://www.researchgate.net/profile/Li_Jia_Li/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database/links/00b495388120dbc339000000/ImageNet-a-Large-Scale-Hierarchical-Image-Database.pdf}
}

@InProceedings{drachen2014skill,
  Title                    = {Skill-based differences in spatio-temporal team behaviour in defence of the ancients 2 (dota 2)},
  Author                   = {Drachen, Anders and Yancey, Matthew and Maguire, John and Chu, Derrek and Wang, Iris Yuhui and Mahlmann, Tobias and Schubert, Matthias and Klabajan, Diego},
  Booktitle                = {2014 IEEE Games Media Entertainment},
  Year                     = {2014},
  Organization             = {IEEE},
  Pages                    = {1--8},

  Abstract                 = {Multiplayer Online Battle Arena (MOBA) games are among the most played digital games in the world. In these games, teams of players fight against each other in arena environments, and the gameplay is focused on tactical combat. Mastering MOBAs requires extensive practice, as is exemplified in the popular MOBA Defence of the Ancients 2 (DotA 2). In this paper, we present three data-driven measures of spatio-temporal behavior in DotA 2: 1) Zone changes; 2) Distribution of team members and: 3) Time series clustering via a fuzzy approach. We present a method for obtaining accurate positional data from DotA 2. We investigate how behavior varies across these measures as a function of the skill level of teams, using four tiers from novice to professional players. Results indicate that spatio-temporal behavior of MOBA teams is related to team skill, with professional teams having smaller within-team distances and conducting more zone changes than amateur teams. The temporal distribution of the within-team distances of professional and high-skilled teams also generally follows patterns distinct from lower skill ranks.},
  Comment                  = {Defence of ancients. Citations - 77},
  Owner                    = {manoj},
  Timestamp                = {2020.08.21},
  Url                      = {http://www.academia.edu/download/57772177/1603.07738.pdf}
}

@Article{fu2018kiwifruit,
  Title                    = {Kiwifruit detection in field images using Faster R-CNN with ZFNet},
  Author                   = {Fu, Longsheng and Feng, Yali and Majeed, Yaqoob and Zhang, Xin and Zhang, Jing and Karkee, Manoj and Zhang, Qin},
  Journal                  = {IFAC-PapersOnLine},
  Year                     = {2018},
  Number                   = {17},
  Pages                    = {45--50},
  Volume                   = {51},

  Abstract                 = {A kiwifruit detection system for field images was developed based on the deep convolutional neural network, which has a good robustness against the subjectivity and limitation of the
features selected artificially. Under different lighting conditions, 2,100 sub-images with 784×},
  Comment                  = {ZFNet. Citations - 18},
  Owner                    = {manoj},
  Publisher                = {Elsevier},
  Timestamp                = {2020.08.26},
  Url                      = {https://www.researchgate.net/profile/Longsheng_Fu/publication/327616829_Kiwifruit_detection_in_field_images_using_Faster_R-CNN_with_ZFNet/links/5b9ffaf745851574f7d25805/Kiwifruit-detection-in-field-images-using-Faster-R-CNN-with-ZFNet.pdf}
}

@Article{fukushima2007neocognitron,
  Title                    = {Neocognitron},
  Author                   = {Fukushima, Kunihiko},
  Journal                  = {Scholarpedia},
  Year                     = {2007},
  Number                   = {1},
  Pages                    = {1717},
  Volume                   = {2},

  Comment                  = {Citation - 147},
  Owner                    = {manoj},
  Timestamp                = {2020.08.20},
  Url                      = {http://var.scholarpedia.org/article/Neocognitron}
}

@InProceedings{goyal2017something,
  Title                    = {The" Something Something" Video Database for Learning and Evaluating Visual Common Sense.},
  Author                   = {Goyal, Raghav and Kahou, Samira Ebrahimi and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and others},
  Booktitle                = {ICCV},
  Year                     = {2017},
  Number                   = {4},
  Pages                    = {5},
  Volume                   = {1},

  Abstract                 = {Neural networks trained on datasets such as ImageNet have led to major advances in visual object classification. One obstacle that prevents networks from reasoning more deeply about complex scenes and situations, and from integrating visual knowledge with natural language, like humans do, is their lack of common sense knowledge about the physical world. Videos, unlike still images, contain a wealth of detailed information about the physical world. However, most labelled video datasets represent high-level concepts rather than detailed physical aspects about actions and scenes. In this work, we describe our ongoing collection of the “something-something” database of video prediction tasks whose solutions require a common sense understanding of the depicted situation. The database currently contains more than 100,000 videos across 174 classes, which are defined as caption-templates. We also describe the challenges in crowd-sourcing this data at scale.},
  Comment                  = {VCR dataset. Citations - 191},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {http://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_The_Something_Something_ICCV_2017_paper.pdf}
}

@InProceedings{kirillov2019panoptic,
  Title                    = {Panoptic segmentation},
  Author                   = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Doll{\'a}r, Piotr},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2019},
  Pages                    = {9404--9413},

  Abstract                 = {We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently
popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified
view of image segmentation. For more analysis and up-todate results, please check the arXiv version of the paper: https://arxiv.org/abs/1801.00868.},
  Comment                  = {Panoptic segmentation. CItations - 199},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {http://openaccess.thecvf.com/content_CVPR_2019/papers/Kirillov_Panoptic_Segmentation_CVPR_2019_paper.pdf}
}

@Article{krishna2017visual,
  Title                    = {Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  Author                   = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  Journal                  = {International journal of computer vision},
  Year                     = {2017},
  Number                   = {1},
  Pages                    = {32--73},
  Volume                   = {123},

  Abstract                 = {Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked "What vehicle is the person riding?", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that "the person is riding a horse-drawn carriage".
In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.},
  Comment                  = {Visual genome dataset. Citations - 1355},
  Owner                    = {manoj},
  Publisher                = {Springer},
  Timestamp                = {2020.08.26},
  Url                      = {https://arxiv.org/pdf/1602.07332}
}

@Article{lecun1989backpropagation,
  Title                    = {Backpropagation applied to handwritten zip code recognition},
  Author                   = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  Journal                  = {Neural computation},
  Year                     = {1989},
  Number                   = {4},
  Pages                    = {541--551},
  Volume                   = {1},

  Owner                    = {manoj},
  Publisher                = {MIT Press},
  Timestamp                = {2020.08.20}
}

@Article{lin2016masked,
  Title                    = {Masked face detection via a modified LeNet},
  Author                   = {Lin, Shaohui and Cai, Ling and Lin, Xianming and Ji, Rongrong},
  Journal                  = {Neurocomputing},
  Year                     = {2016},
  Pages                    = {197--202},
  Volume                   = {218},

  Abstract                 = {Detecting masked faces in the wild has been emerging recently, which has rich applications ranging from violence video retrieval to video surveillance. Its accurate detection retains as an open problem, mainly due to the difficulties of low-resolution and arbitrary viewing angles, as well as the limitation of collecting sufficient amount of training samples. Such difficulties have been significantly challenged the design of effective handcraft features as well as robust detectors. In this paper, we tackle these problems by proposing a learn-based feature design and classifier training paradigm. More particularly, a modified LeNet, termed MLeNet, is presented, which modifies the number of units in output layer of LeNet to suit for a specific classification. Meanwhile, MLeNet further increases the number of feature maps with smaller filter size. To further reduce overfitting and improve the performance with a small quantity of training samples, we firstly increase the training dataset by horizontal reflection and then learn MLeNet via combining both pre-training and fine-tuning. We evaluate the proposed model on a real-world masked face detection dataset. Quantitative evaluations over several state-of-the-arts and alternative solution have demonstrated the accuracy and robustness of the proposed model.},
  Comment                  = {Citation - 12},
  Owner                    = {manoj},
  Publisher                = {Elsevier},
  Timestamp                = {2020.08.20},
  Url                      = {https://www.sciencedirect.com/science/article/abs/pii/S0925231216309523}
}

@InProceedings{lin2014microsoft,
  Title                    = {Microsoft coco: Common objects in context},
  Author                   = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  Booktitle                = {European conference on computer vision},
  Year                     = {2014},
  Organization             = {Springer},
  Pages                    = {740--755},

  Abstract                 = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed
statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  Comment                  = {COCO dataset. CItations - 10987},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://link.springer.com/content/pdf/10.1007/978-3-319-10602-1_48.pdf}
}

@InProceedings{liu2016ssd,
  Title                    = {Ssd: Single shot multibox detector},
  Author                   = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C},
  Booktitle                = {European conference on computer vision},
  Year                     = {2016},
  Organization             = {Springer},
  Pages                    = {21--37},

  Abstract                 = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For   300×300  input, SSD achieves 74.3 % mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for   512×512  input, SSD achieves 76.9 % mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
  Comment                  = {SSD. Citations - 9937},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://arxiv.org/pdf/1512.02325.pdf%EF%BC%89}
}

@Article{moravvcik2017deepstack,
  Title                    = {Deepstack: Expert-level artificial intelligence in heads-up no-limit poker},
  Author                   = {Morav{\v{c}}{\'\i}k, Matej and Schmid, Martin and Burch, Neil and Lis{\`y}, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
  Journal                  = {Science},
  Year                     = {2017},
  Number                   = {6337},
  Pages                    = {508--513},
  Volume                   = {356},

  Abstract                 = {Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker is the quintessential game of imperfect information, and a longstanding challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated with statistical significance professional poker players in heads-up no-limit Texas hold’em. The approach is
theoretically sound and is shown to produce more difficult to exploit strategies than prior approaches.},
  Comment                  = {Deepstack poker game. Citation - 424},
  Owner                    = {manoj},
  Publisher                = {American Association for the Advancement of Science},
  Timestamp                = {2020.08.21},
  Url                      = {https://arxiv.org/pdf/1701.01724}
}

@InProceedings{oh2015action,
  Title                    = {Action-conditional video prediction using deep networks in atari games},
  Author                   = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard L and Singh, Satinder},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2015},
  Pages                    = {2863--2871},

  Abstract                 = {Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future image-frames depend on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, actionconditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.},
  Comment                  = {Atari games by deep neural network. Citation -},
  Owner                    = {manoj},
  Timestamp                = {2020.08.21},
  Url                      = {http://papers.nips.cc/paper/5859-action-conditional-video-prediction-using-deep-networks-in-atari-games.pdf}
}

@InProceedings{redmon2016you,
  Title                    = {You only look once: Unified, real-time object detection},
  Author                   = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2016},
  Pages                    = {779--788},

  Abstract                 = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
  Comment                  = {YOLO. CItations - 10673},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf}
}

@InProceedings{ronneberger2015u,
  Title                    = {U-net: Convolutional networks for biomedical image segmentation},
  Author                   = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  Booktitle                = {International Conference on Medical image computing and computer-assisted intervention},
  Year                     = {2015},
  Organization             = {Springer},
  Pages                    = {234--241},

  Abstract                 = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  Comment                  = {U-net. Citations - 16950},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://arxiv.org/pdf/1505.04597.pdf)%E5%92%8C[Tiramisu](https://arxiv.org/abs/1611.09326}
}

@Article{silver2017mastering,
  Title                    = {Mastering the game of go without human knowledge},
  Author                   = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  Journal                  = {nature},
  Year                     = {2017},
  Number                   = {7676},
  Pages                    = {354--359},
  Volume                   = {550},

  Abstract                 = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from selfplay. Here, we introduce an algorithm based solely on reinforcement learning, without human data, guidance, or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.},
  Comment                  = {Go game. CItation - 3950},
  Owner                    = {manoj},
  Publisher                = {Nature Publishing Group},
  Timestamp                = {2020.08.21},
  Url                      = {https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf}
}

@InProceedings{szegedy2015going,
  Title                    = {Going deeper with convolutions},
  Author                   = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2015},
  Pages                    = {1--9},

  Owner                    = {manoj},
  Timestamp                = {2020.08.26}
}

@Article{szegedy2013intriguing,
  Title                    = {Intriguing properties of neural networks},
  Author                   = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  Journal                  = {arXiv preprint arXiv:1312.6199},
  Year                     = {2013},

  Abstract                 = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network’s prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  Comment                  = {Properties of neural net. Citations - 5096},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://arxiv.org/pdf/1312.6199.pdf?source=post_page---------------------------}
}

@Article{wang2016does,
  Title                    = {Where does AlphaGo go: From church-turing thesis to AlphaGo thesis and beyond},
  Author                   = {Wang, Fei-Yue and Zhang, Jun Jason and Zheng, Xinhu and Wang, Xiao and Yuan, Yong and Dai, Xiaoxiao and Zhang, Jie and Yang, Liuqing},
  Journal                  = {IEEE/CAA Journal of Automatica Sinica},
  Year                     = {2016},
  Number                   = {2},
  Pages                    = {113--120},
  Volume                   = {3},

  Owner                    = {manoj},
  Publisher                = {IEEE},
  Timestamp                = {2020.08.21},
  Url                      = {http://159.226.21.68/bitstream/173211/11604/1/Where%20does%20AlphaGo%20go%EF%BC%9A%20from%20Church-Turing%20Thesis%20to%20AlphaGo%20Thesis%20and%20beyond.pdf}
}

@Article{,
  Abstract                 = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  Comment                  = {Inceptionnet. Citations - 23666},
  Owner                    = {manoj},
  Timestamp                = {2020.08.20},
  Url                      = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf}
}

