% This file was created with JabRef 2.10.
% Encoding: UTF-8


@InProceedings{ajmal2012video,
  Title                    = {Video summarization: techniques and classification},
  Author                   = {Ajmal, Muhammad and Ashraf, Muhammad Husnain and Shakir, Muhammad and Abbas, Yasir and Shah, Faiz Ali},
  Booktitle                = {International Conference on Computer Vision and Graphics},
  Year                     = {2012},
  Organization             = {Springer},
  Pages                    = {1--13},

  Abstract                 = {A large number of cameras record video around the clock, producing huge volumes. Processing these huge chunks of videos demands plenty of resources like time, man power, and hardware storage etc. Video summarization plays an important role in this context. It helps in efficient storage, quick browsing, and retrieval of large collection of video data without losing important aspects. In this paper, we categorize video summariztion methods on the basis of methodology used, provide detailed description of leading methods in each category, and discuss their advantages and disadvantages. Moreover, we discuss the situation in which each method is most suitable to use. The advantage of this research is that one can quickly learn different video summarization techniques, and select the method that is the most suitable according to one’s requirements.},
  Comment                  = {Video summarization. Citation - 78.},
  Url                      = {https://link.springer.com/chapter/10.1007/978-3-642-33564-8_1}
}

@InProceedings{alahi2016social,
  Title                    = {Social lstm: Human trajectory prediction in crowded spaces},
  Author                   = {Alahi, Alexandre and Goel, Kratarth and Ramanathan, Vignesh and Robicquet, Alexandre and Fei-Fei, Li and Savarese, Silvio},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2016},
  Pages                    = {961--971},

  Abstract                 = {Humans navigate complex crowded environments based on social conventions: they respect personal space, yielding right-of-way and avoid collisions. In our work, we propose a data-driven approach to learn these human-human interactions for predicting their future trajectories. This is in contrast to traditional approaches which use hand-crafted functions such as Social forces. We present a new Long Short-Term Memory (LSTM) model which jointly reasons across multiple individuals in a scene. Different from the conventional LSTM, we share the information between multiple LSTMs through a new pooling layer. This layer pools the hidden representation from LSTMs corresponding to neighboring trajectories to capture interactions within this neighborhood. We demonstrate the performance of our method on several public datasets. Our model outperforms previous forecasting methods by more than 42% . We also analyze the trajectories predicted by our model to demonstrate social behaviours such as collision avoidance and group movement, learned by our model.},
  Comment                  = {Visual tracking. Citations - 967},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2016/papers/Alahi_Social_LSTM_Human_CVPR_2016_paper.pdf}
}

@InProceedings{alahi2012freak,
  Title                    = {Freak: Fast retina keypoint},
  Author                   = {Alahi, Alexandre and Ortiz, Raphael and Vandergheynst, Pierre},
  Booktitle                = {2012 IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2012},
  Organization             = {Ieee},
  Pages                    = {510--517},

  Abstract                 = {A large number of vision applications rely on matching keypoints across images. The last decade featured an arms-race towards faster and more robust keypoints and association algorithms: Scale Invariant Feature Transform (SIFT)[17], Speed-up Robust Feature (SURF)[4], and more recently Binary Robust Invariant Scalable Keypoints (BRISK)[I6] to name a few. These days, the deployment of vision algorithms on smart phones and embedded devices with low memory and computation complexity has even upped the ante: the goal is to make descriptors faster to compute, more compact while remaining robust to scale, rotation and noise. To best address the current requirements, we propose a novel keypoint descriptor inspired by the human visual system and more precisely the retina, coined Fast Retina Keypoint (FREAK). A cascade of binary strings is computed by efficiently comparing image intensities over a retinal sampling pattern. Our experiments show that FREAKs are in general faster to compute with lower memory load and also more robust than SIFT, SURF or BRISK. They are thus competitive alternatives to existing keypoints in particular for embedded applications.},
  Comment                  = {Visual tracking. CItations - 2114},
  Url                      = {https://infoscience.epfl.ch/record/175537/files/2069.pdf}
}

@InProceedings{anderson2018bottom,
  Title                    = {Bottom-up and top-down attention for image captioning and visual question answering},
  Author                   = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2018},
  Pages                    = {6077--6086},

  Abstract                 = {Top-down visual attention mechanisms have been used extensively in image captioning and visual question answering (VQA) to enable deeper image understanding through fine-grained analysis and even multiple steps of reasoning. In this work, we propose a combined bottom-up and top-down attention mechanism that enables attention to be calculated at the level of objects and other salient image regions. This is the natural basis for attention to be considered. Within our approach, the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings. Applying this approach to image captioning, our results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9, respectively. Demonstrating the broad applicability of the method, applying the same approach to VQA we obtain first place in the 2017 VQA Challenge.},
  Comment                  = {Mark R-CNN. Citations - 1235.},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf}
}

@InProceedings{Antol2015,
  Title                    = {Vqa: Visual question answering},
  Author                   = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C and Parikh, Devi},
  Booktitle                = {Proceedings of the IEEE international conference on computer vision},
  Year                     = {2015},
  Pages                    = {2425--2433},

  Abstract                 = {We propose the task of free-form and open-ended Visual Question Answering (VQA). Given an image and a natural language question about the image, the task is to provide an accurate natural language answer. Mirroring real-world scenarios, such as helping the visually impaired, both the questions and answers are open-ended. Visual questions selectively target different areas of an image, including background details and underlying context. As a result, a system that succeeds at VQA typically needs a more detailed understanding of the image and complex reasoning than a system producing generic image captions. Moreover, VQA is amenable to automatic evaluation, since many open-ended answers contain only a few words or a closed set of answers that can be provided in a multiple-choice format. We provide a dataset containing ∼0.25M images, ∼0.76M questions, and ∼10M answers (www.visualqa.org), and discuss the information it provides. Numerous baselines for VQA are provided and compared with human performance.},
  Comment                  = {VQA dataset. Citations - 2048},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {http://openaccess.thecvf.com/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf}
}

@InProceedings{antol2015vqa,
  Title                    = {Vqa: Visual question answering},
  Author                   = {Antol, Stanislaw and Agrawal, Aishwarya and Lu, Jiasen and Mitchell, Margaret and Batra, Dhruv and Lawrence Zitnick, C and Parikh, Devi},
  Booktitle                = {Proceedings of the IEEE international conference on computer vision},
  Year                     = {2015},
  Pages                    = {2425--2433},

  Abstract                 = {We propose the task offree-formandopen-endedVisualQuestion Answering (VQA). Given an image and a naturallanguage question about the image, the task is to provide anaccurate natural language answer. Mirroring real-worldscenarios, such as helping the visually impaired, both thequestions and answers are open-ended. Visual questions se-lectively target different areas of an image, including back-ground details and underlying context. As a result, a systemthat succeeds at VQA typically needs a more detailed un-derstanding of the image and complex reasoning than a sys-tem producing generic image captions. Moreover, VQA isamenable to automatic evaluation, since many open-endedanswers contain only a few words or a closed set of answersthat can be provided in a multiple-choice format. We pro-vide a dataset containing∼0.25M images,∼0.76M ques-tions, and∼10M answers (www.visualqa.org), anddiscuss the information it provides. Numerous baselines forVQA are provided and compared with human performance},
  Comment                  = {Question answering. Ciatations - 2070.},
  Url                      = {http://openaccess.thecvf.com/content_iccv_2015/papers/Antol_VQA_Visual_Question_ICCV_2015_paper.pdf}
}

@Article{Badrinarayanan2017,
  Title                    = {Segnet: A deep convolutional encoder-decoder architecture for image segmentation},
  Author                   = {Badrinarayanan, Vijay and Kendall, Alex and Cipolla, Roberto},
  Journal                  = {IEEE transactions on pattern analysis and machine intelligence},
  Year                     = {2017},
  Number                   = {12},
  Pages                    = {2481--2495},
  Volume                   = {39},

  Abstract                 = {We present a novel and practical deep fully convolutional neural network architecture for semantic pixel-wise segmentation termed SegNet. This core trainable segmentation engine consists of an encoder network, a corresponding decoder network followed by a pixel-wise classification layer. The architecture of the encoder network is topologically identical to the 13 convolutional layers in the VGG16 network [1] . The role of the decoder network is to map the low resolution encoder feature maps to full input resolution feature maps for pixel-wise classification. The novelty of SegNet lies is in the manner in which the decoder upsamples its lower resolution input feature map(s). Specifically, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling. This eliminates the need for learning to upsample. The upsampled maps are sparse and are then convolved with trainable filters to produce dense feature maps. We compare our proposed architecture with the widely adopted FCN [2] and also with the well known DeepLab-LargeFOV [3] , DeconvNet [4] architectures. This comparison reveals the memory versus accuracy trade-off involved in achieving good segmentation performance. SegNet was primarily motivated by scene understanding applications. Hence, it is designed to be efficient both in terms of memory and computational time during inference. It is also significantly smaller in the number of trainable parameters than other competing architectures and can be trained end-to-end using stochastic gradient descent. We also performed a controlled benchmark of SegNet and other architectures on both road scenes and SUN RGB-D indoor scene segmentation tasks. These quantitative assessments show that SegNet provides good performance with competitive inference time and most efficient inference memory-wise as compared to other architectures. We also provide a Caffe implementation of SegNet and a web demo at http://mi.eng.cam.ac.uk/projects/segnet/.},
  Comment                  = {SegNet. CItations - 5459},
  Owner                    = {manoj},
  Publisher                = {IEEE},
  Timestamp                = {2020.08.26},
  Url                      = {https://ieeexplore.ieee.org/iel7/34/4359286/07803544.pdf}
}

@Article{bahdanau2014neural,
  Title                    = {Neural machine translation by jointly learning to align and translate},
  Author                   = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  Journal                  = {arXiv preprint arXiv:1409.0473},
  Year                     = {2014},

  Abstract                 = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree
well with our intuition.},
  Comment                  = {Machine translations. Citations - 13991},
  Url                      = {https://arxiv.org/pdf/1409.0473)}
}

@InProceedings{bahdanau2016end,
  Title                    = {End-to-end attention-based large vocabulary speech recognition},
  Author                   = {Bahdanau, Dzmitry and Chorowski, Jan and Serdyuk, Dmitriy and Brakel, Philemon and Bengio, Yoshua},
  Booktitle                = {2016 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  Year                     = {2016},
  Organization             = {IEEE},
  Pages                    = {4945--4949},

  Abstract                 = {Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and
the desired character sequence is learned automatically by an
attention mechanism built into the RNN. For each predicted
character, the attention mechanism scans the input sequence
and chooses relevant frames. We propose two methods to
speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.},
  Comment                  = {Speech recognition. Citations - 776},
  Url                      = {https://arxiv.org/pdf/1508.04395}
}

@InProceedings{bell2016inside,
  Title                    = {Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks},
  Author                   = {Bell, Sean and Lawrence Zitnick, C and Bala, Kavita and Girshick, Ross},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2016},
  Pages                    = {2874--2883},

  Abstract                 = {It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9% to 77.9% mAP. On the new and more challenging MS COCO dataset, we improve state-of-the-art from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection Challenge, our ION model won "Best Student Entry" and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.},
  Comment                  = {Inside-outside net. Citations - 772},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2016/papers/Bell_Inside-Outside_Net_Detecting_CVPR_2016_paper.pdf}
}

@InProceedings{ben2017mutan,
  Title                    = {Mutan: Multimodal tucker fusion for visual question answering},
  Author                   = {Ben-Younes, Hedi and Cadene, R{\'e}mi and Cord, Matthieu and Thome, Nicolas},
  Booktitle                = {Proceedings of the IEEE international conference on computer vision},
  Year                     = {2017},
  Pages                    = {2612--2620},

  Abstract                 = {Bilinear models provide an appealing framework for mixing and merging information in Visual Question Answering (VQA) tasks. They help to learn high level associations between question meaning and visual concepts in the image, but they suffer from huge dimensionality issues. We introduce MUTAN, a multimodal tensor-based Tucker decomposition to efficiently parametrize bilinear interactions between visual and textual representations. Additionally to the Tucker framework, we design a low-rank matrix-based decomposition to explicitly constrain the interaction rank. With MUTAN, we control the complexity of the merging scheme while keeping nice interpretable fusion relations. We show how the Tucker decomposition framework generalizes some of the latest VQA architectures, providing state-of-the-art results.},
  Comment                  = {Visual question answering. Citations - 249},
  Url                      = {http://openaccess.thecvf.com/content_ICCV_2017/papers/Ben-younes_MUTAN_Multimodal_Tucker_ICCV_2017_paper.pdf}
}

@Article{bordes2016learning,
  Title                    = {Learning end-to-end goal-oriented dialog},
  Author                   = {Bordes, Antoine and Boureau, Y-Lan and Weston, Jason},
  Journal                  = {arXiv preprint arXiv:1605.07683},
  Year                     = {2016},

  Abstract                 = {Traditional dialog systems used in goal-oriented applications require a lot of domain-specific handcrafting, which hinders scaling up to new domains. End-to-end dialog systems, in which all components are trained from the dialogs themselves, escape this limitation. But the encouraging success recently obtained in chit-chat dialog may not carry over to goal-oriented settings. This paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. Set in the context of restaurant reservation, our tasks require manipulating sentences and symbols, so as to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). We show similar result patterns on data extracted from an online concierge service.},
  Comment                  = {Conversation model. Citations - 531},
  Url                      = {https://arxiv.org/pdf/1605.07683}
}

@InProceedings{chen2017sca,
  Title                    = {Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning},
  Author                   = {Chen, Long and Zhang, Hanwang and Xiao, Jun and Nie, Liqiang and Shao, Jian and Liu, Wei and Chua, Tat-Seng},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2017},
  Pages                    = {5659--5667},

  Abstract                 = {Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism --- a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (i.e., attentive spatial locations at multiple layers) and what (i.e., attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods.},
  Comment                  = {Image captioning. Citations - 609.},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Chen_SCA-CNN_Spatial_and_CVPR_2017_paper.pdf}
}

@Article{Chen2017,
  Title                    = {Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs},
  Author                   = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  Journal                  = {IEEE transactions on pattern analysis and machine intelligence},
  Year                     = {2017},
  Number                   = {4},
  Pages                    = {834--848},
  Volume                   = {40},

  Abstract                 = {In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or `atrous convolution', as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed “DeepLab” system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online.},
  Comment                  = {Semantic image segementation. Citations - 5527},
  Owner                    = {manoj},
  Publisher                = {IEEE},
  Timestamp                = {2020.08.26},
  Url                      = {https://arxiv.org/pdf/1606.00915}
}

@Article{Chen2014,
  Title                    = {Semantic image segmentation with deep convolutional nets and fully connected crfs},
  Author                   = {Chen, Liang-Chieh and Papandreou, George and Kokkinos, Iasonas and Murphy, Kevin and Yuille, Alan L},
  Journal                  = {arXiv preprint arXiv:1412.7062},
  Year                     = {2014},

  Abstract                 = {Deep Convolutional Neural Networks (DCNNs) have recently shown state of the art performance in high level vision tasks, such as image classification and object detection. This work brings together methods from DCNNs and probabilistic graphical models for addressing the task of pixel-level classification (also called ”semantic image segmentation”). We show that responses at the final layer of DCNNs are not sufficiently localized for accurate object segmentation. This is due to the very invariance properties that make DCNNs good for high level tasks. We overcome this poor localization property of deep networks by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF). Qualitatively, our “DeepLab” system is able to localize segment boundaries at a level of accuracy which is beyond previous methods. Quantitatively, our method sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 71.6% IOU accuracy in the test set. We show how these results can be obtained efficiently: Careful network re-purposing and a novel application of the ’hole’ algorithm from the wavelet community allow dense computation of neural net responses at 8 frames per second on a modern GPU.},
  Comment                  = {Fully connected network for segmentation. Citations - 2621},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://arxiv.org/pdf/1412.7062}
}

@Article{chen2016variational,
  Title                    = {Variational lossy autoencoder},
  Author                   = {Chen, Xi and Kingma, Diederik P and Salimans, Tim and Duan, Yan and Dhariwal, Prafulla and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  Journal                  = {arXiv preprint arXiv:1611.02731},
  Year                     = {2016},

  Abstract                 = {Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only "autoencodes" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution p(z) and decoding distribution p(x|z), we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.},
  Comment                  = {Variational autoencoders. Citations - 373.},
  Url                      = {https://arxiv.org/pdf/1611.02731.pdf,}
}

@Article{chheng2007video,
  Title                    = {Video summarization using clustering},
  Author                   = {Chheng, Tommy},
  Journal                  = {Department of Computer Science University of California, Irvine},
  Year                     = {2007},

  Abstract                 = {In this paper, we approach the problem of video summarization. Wepropose an automated algorithm to identify the unique segments of avideo. The video segments are separated using the k-means clustering.We use Euclidean distance with histograms of the corresponding segmentas the distance metric. YouTube videos are used to test our procedure},
  Comment                  = {Video summarization. Citation - 11.},
  Publisher                = {Citeseer},
  Url                      = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.187.7458&rep=rep1&type=pdf}
}

@Article{Cho2014,
  Title                    = {On the properties of neural machine translation: Encoder-decoder approaches},
  Author                   = {Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  Journal                  = {arXiv preprint arXiv:1409.1259},
  Year                     = {2014},

  Abstract                 = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct
translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder–Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a
sentence automatically.},
  Comment                  = {Machine translation. CItations - 2924},
  Url                      = {https://arxiv.org/pdf/1409.1259}
}

@Article{cho2014properties,
  Title                    = {On the properties of neural machine translation: Encoder-decoder approaches},
  Author                   = {Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
  Journal                  = {arXiv preprint arXiv:1409.1259},
  Year                     = {2014},

  Abstract                 = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a fixed-length representation from a variable-length input sentence, and the decoder generates a correct
translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder–Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we find that the proposed gated recursive convolutional network learns a grammatical structure of a
sentence automatically.},
  Comment                  = {Machine translations. Citations - 2924},
  Url                      = {https://arxiv.org/pdf/1409.1259}
}

@Article{cho2014learning,
  Title                    = {Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  Author                   = {Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  Journal                  = {arXiv preprint arXiv:1406.1078},
  Year                     = {2014},

  Abstract                 = {In this paper, we propose a novel neural network model called RNN Encoder– Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a fixedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder–Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
  Comment                  = {Machine translation. Citations - 10610},
  Url                      = {https://arxiv.org/pdf/1406.1078.pdf?source=post_page---------------------------}
}

@InProceedings{choi2018context,
  Title                    = {Context-aware deep feature compression for high-speed visual tracking},
  Author                   = {Choi, Jongwon and Jin Chang, Hyung and Fischer, Tobias and Yun, Sangdoo and Lee, Kyuewang and Jeong, Jiyeoup and Demiris, Yiannis and Young Choi, Jin},
  Booktitle                = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2018},
  Pages                    = {479--488},

  Abstract                 = {We propose a new context-aware correlation filter basedtracking framework to achieve both high computationalspeed and state-of-the-art performance among real-timetrackers. The major contribution to the high computationalspeed lies in the proposed deep feature compression thatis achieved by a context-aware scheme utilizing multipleexpert auto-encoders; a context in our framework refersto the coarse category of the tracking target according toappearance patterns. In the pre-training phase, one expertauto-encoder is trained per category. In the tracking phase,the best expert auto-encoder is selected for a given target,and only this auto-encoder is used. To achieve high trackingperformance with the compressed feature map, we intro-duce extrinsic denoising processes and a new orthogonalityloss term for pre-training and fine-tuning of the expert auto-encoders. We validate the proposed context-aware frame-work through a number of experiments, where our methodachieves a comparable performance to state-of-the-art track-ers which cannot run in real-time, while running at a signifi-cantly fast speed of over 100 fps},
  Comment                  = {Visual tracking. Citations - 101.},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2018/papers/Choi_Context-Aware_Deep_Feature_CVPR_2018_paper.pdf}
}

@InProceedings{choi2017attentional,
  Title                    = {Attentional correlation filter network for adaptive visual tracking},
  Author                   = {Choi, Jongwon and Jin Chang, Hyung and Yun, Sangdoo and Fischer, Tobias and Demiris, Yiannis and Young Choi, Jin},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2017},
  Pages                    = {4807--4816},

  Abstract                 = {We propose a new tracking framework with an attentionalmechanism that chooses a subset of the associated corre-lation filters for increased robustness and computationalefficiency. The subset of filters is adaptively selected by adeep attentional network according to the dynamic proper-ties of the tracking target. Our contributions are manifold,and are summarised as follows: (i) Introducing the Atten-tional Correlation Filter Network which allows adaptivetracking of dynamic targets. (ii) Utilising an attentional net-work which shifts the attention to the best candidate modules,as well as predicting the estimated accuracy of currently in-active modules. (iii) Enlarging the variety of correlationfilters which cover target drift, blurriness, occlusion, scalechanges, and flexible aspect ratio. (iv) Validating the robust-ness and efficiency of the attentional mechanism for visualtracking through a number of experiments. Our methodachieves similar performance to non real-time trackers, andstate-of-the-art performance amongst real-time trackers.},
  Comment                  = {Visual tracking. Citations - 208},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Choi_Attentional_Correlation_Filter_CVPR_2017_paper.pdf}
}

@InProceedings{chorowski2015attention,
  Title                    = {Attention-based models for speech recognition},
  Author                   = {Chorowski, Jan K and Bahdanau, Dzmitry and Serdyuk, Dmitriy and Cho, Kyunghyun and Bengio, Yoshua},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2015},
  Pages                    = {577--585},

  Abstract                 = {Recurrent sequence generators conditioned on input data through an attention mechanism have recently shown very good performance on a range of tasks including machine translation, handwriting synthesis and image caption generation [3]. We extend the attention-mechanism with features needed for speech recognition. We show that while an adaptation of the model used for machine translation in reaches a competitive 18.7% phoneme error rate (PER) on the TIMIT phoneme recognition task, it can only be applied to utterances which are roughly as long as the ones it was trained on. We offer a qualitative explanation of this failure and propose a novel and generic method of adding location-awareness
to the attention mechanism to alleviate this issue. The new method yields a model that is robust to long inputs and achieves 18% PER in single utterances and 20% in 10-times longer (repeated) utterances. Finally, we propose a change to the attention mechanism that prevents it from concentrating too much on single frames,
which further reduces PER to 17.6% level.},
  Comment                  = {Speech recognition. Citations - 1354},
  Url                      = {http://papers.nips.cc/paper/5847-attention-based-models-for-speech-recognition.pdf}
}

@Article{Chung2014,
  Title                    = {Empirical evaluation of gated recurrent neural networks on sequence modeling},
  Author                   = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  Journal                  = {arXiv preprint arXiv:1412.3555},
  Year                     = {2014},

  Abstract                 = {In this paper we compare different types of recurrent units in recurrent neural net-works (RNNs). Especially, we focus on more sophisticated units that implementa gating mechanism, such as a long short-term memory (LSTM) unit and a re-cently proposed gated recurrent unit (GRU). We evaluate these recurrent units onthe tasks of polyphonic music modeling and speech signal modeling. Our exper-iments revealed that these advanced recurrent units are indeed better than moretraditional recurrent units such astanhunits. Also, we found GRU to be compa-rable to LSTM.},
  Comment                  = {gated RNN. Citations - 5072},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {https://arxiv.org/pdf/1412.3555.pdf?ref=hackernoon.com}
}

@Article{chung2014empirical,
  Title                    = {Empirical evaluation of gated recurrent neural networks on sequence modeling},
  Author                   = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
  Journal                  = {arXiv preprint arXiv:1412.3555},
  Year                     = {2014},

  Abstract                 = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on
the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
  Comment                  = {Machine translation. Citations - 5147},
  Url                      = {https://arxiv.org/pdf/1412.3555.pdf?ref=hackernoon.com}
}

@InProceedings{ciregan2012multi,
  Title                    = {Multi-column deep neural networks for image classification},
  Author                   = {Ciregan, Dan and Meier, Ueli and Schmidhuber, J{\"u}rgen},
  Booktitle                = {2012 IEEE conference on computer vision and pattern recognition},
  Year                     = {2012},
  Organization             = {IEEE},
  Pages                    = {3642--3649},

  Abstract                 = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible, wide and deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
  Comment                  = {DNN for image classification. Citations - 3379.},
  Url                      = {https://ieeexplore.ieee.org/abstract/document/6248110}
}

@InProceedings{Cordts2016,
  Title                    = {The cityscapes dataset for semantic urban scene understanding},
  Author                   = {Cordts, Marius and Omran, Mohamed and Ramos, Sebastian and Rehfeld, Timo and Enzweiler, Markus and Benenson, Rodrigo and Franke, Uwe and Roth, Stefan and Schiele, Bernt},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2016},
  Pages                    = {3213--3223},

  Abstract                 = {Visual understanding of complex urban street scenes is an enabling factor for a wide range of applications. Object detection has benefited enormously from large-scale datasets, especially in the context of deep learning. For semantic urban scene understanding, however, no current dataset adequately captures the complexity of real-world urban scenes. To address this, we introduce Cityscapes, a benchmark suite and large-scale dataset to train and test approaches for pixel-level and instance-level semantic labeling. Cityscapes is comprised of a large, diverse set of stereo video sequences recorded in streets from 50 different cities. 5000 of these images have high quality pixel-level annotations; 20 000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. Crucially, our effort exceeds previous attempts in terms of dataset size, annotation richness, scene variability, and complexity. Our accompanying empirical study provides an in-depth analysis of the dataset characteristics, as well as a performance evaluation of several state-of-the-art approaches based on our benchmark},
  Comment                  = {Cityscapes dataset. CItations - 3405},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2016/papers/Cordts_The_Cityscapes_Dataset_CVPR_2016_paper.pdf}
}

@InProceedings{dai2016r,
  Title                    = {R-fcn: Object detection via region-based fully convolutional networks},
  Author                   = {Dai, Jifeng and Li, Yi and He, Kaiming and Sun, Jian},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2016},
  Pages                    = {379--387},

  Abstract                 = {We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20 times faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn.},
  Comment                  = {R-FCN. Citations - 2907},
  Url                      = {https://papers.nips.cc/paper/6465-r-fcn-object-detection-via-region-based-fully-convolutional-networks.pdf}
}

@InProceedings{Deng2009,
  Title                    = {Imagenet: A large-scale hierarchical image database},
  Author                   = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  Booktitle                = {2009 IEEE conference on computer vision and pattern recognition},
  Year                     = {2009},
  Organization             = {Ieee},
  Pages                    = {248--255},

  Abstract                 = {The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500- 1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.},
  Comment                  = {Imagenet. Citations - 20648},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://www.researchgate.net/profile/Li_Jia_Li/publication/221361415_ImageNet_a_Large-Scale_Hierarchical_Image_Database/links/00b495388120dbc339000000/ImageNet-a-Large-Scale-Hierarchical-Image-Database.pdf}
}

@Article{devlin2015language,
  Title                    = {Language models for image captioning: The quirks and what works},
  Author                   = {Devlin, Jacob and Cheng, Hao and Fang, Hao and Gupta, Saurabh and Deng, Li and He, Xiaodong and Zweig, Geoffrey and Mitchell, Margaret},
  Journal                  = {arXiv preprint arXiv:1505.01809},
  Year                     = {2015},

  Abstract                 = {Two recent approaches have achieved state-of-the-art results in image captioning. The first uses a pipelined process where a set of candidate words is generated by a convolutional neural network (CNN) trained on images, and then a maximum entropy (ME) language model is used to arrange these words into a coherent sentence. The second uses the penultimate activation layer of the CNN as input to a recurrent neural network (RNN) that then generates the caption sequence. In this paper, we compare the merits of these different language modeling approaches for the first time by using the same state-of-the-art CNN as input. We examine issues in the different approaches, including linguistic irregularities, caption repetition, and data set overlap. By combining key aspects of the ME and RNN methods, we achieve a new record performance over previously published results on the benchmark COCO dataset. However, the gains we see in BLEU do not translate to human judgments.},
  Comment                  = {Image captioning. Citations - 216.},
  Url                      = {https://arxiv.org/pdf/1505.01809}
}

@Article{doersch2016tutorial,
  Title                    = {Tutorial on variational autoencoders},
  Author                   = {Doersch, Carl},
  Journal                  = {arXiv preprint arXiv:1606.05908},
  Year                     = {2016},

  Abstract                 = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  Comment                  = {Variational autoencoders. Citations - 752.},
  Url                      = {https://arxiv.org/pdf/1606.05908.pdf http://arxiv.org/abs/1606.05908}
}

@InProceedings{Drachen2014,
  Title                    = {Skill-based differences in spatio-temporal team behaviour in defence of the ancients 2 (dota 2)},
  Author                   = {Drachen, Anders and Yancey, Matthew and Maguire, John and Chu, Derrek and Wang, Iris Yuhui and Mahlmann, Tobias and Schubert, Matthias and Klabajan, Diego},
  Booktitle                = {2014 IEEE Games Media Entertainment},
  Year                     = {2014},
  Organization             = {IEEE},
  Pages                    = {1--8},

  Abstract                 = {Multiplayer Online Battle Arena (MOBA) games are among the most played digital games in the world. In these games, teams of players fight against each other in arena environments, and the gameplay is focused on tactical combat. Mastering MOBAs requires extensive practice, as is exemplified in the popular MOBA Defence of the Ancients 2 (DotA 2). In this paper, we present three data-driven measures of spatio-temporal behavior in DotA 2: 1) Zone changes; 2) Distribution of team members and: 3) Time series clustering via a fuzzy approach. We present a method for obtaining accurate positional data from DotA 2. We investigate how behavior varies across these measures as a function of the skill level of teams, using four tiers from novice to professional players. Results indicate that spatio-temporal behavior of MOBA teams is related to team skill, with professional teams having smaller within-team distances and conducting more zone changes than amateur teams. The temporal distribution of the within-team distances of professional and high-skilled teams also generally follows patterns distinct from lower skill ranks.},
  Comment                  = {Defence of ancients. Citations - 77},
  Owner                    = {manoj},
  Timestamp                = {2020.08.21},
  Url                      = {http://www.academia.edu/download/57772177/1603.07738.pdf}
}

@InProceedings{engel2017neural,
  Title                    = {Neural audio synthesis of musical notes with wavenet autoencoders},
  Author                   = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Norouzi, Mohammad and Eck, Douglas and Simonyan, Karen},
  Booktitle                = {International Conference on Machine Learning},
  Year                     = {2017},
  Organization             = {PMLR},
  Pages                    = {1068--1077},

  Abstract                 = {Generative models in vision have seen rapidprogress due to algorithmic improvements andthe availability of high-quality image datasets. Inthis paper, we offer contributions in both these ar-eas to enable similar progress in audio modeling.First, we detail a powerful new WaveNet-styleautoencoder model that conditions an autoregres-sive decoder on temporal codes learned fromthe raw audio waveform. Second, we introduceNSynth, a large-scale and high-quality dataset ofmusical notes that is an order of magnitude largerthan comparable public datasets. Using NSynth,we demonstrate improved qualitative and quanti-tative performance of the WaveNet autoencoderover a well-tuned spectral autoencoder baseline.Finally, we show that the model learns a mani-fold of embeddings that allows for morphing be-tween instruments, meaningfully interpolating intimbre to create new types of sounds that are re-alistic and expressive.},
  Comment                  = {Wavenet autoencoder. Citations - 240.},
  Url                      = {http://proceedings.mlr.press/v70/engel17a.html}
}

@Book{farlow1984self,
  Title                    = {Self-organizing methods in modeling: GMDH type algorithms},
  Author                   = {Farlow, Stanley J},
  Publisher                = {CrC Press},
  Year                     = {1984},
  Volume                   = {54},

  Abstract                 = {Cybernetics, Self-Organizing Systems with Positive Feedback, Cybernetic Predicting Devices,
Cybernetic Systems with … D. Director, Institute of Cybernetics, Ukrainian Academy of Sciences,
Kiev … SEARCH FOR STRUCTURE James M. Malone II PREDICTED SQUARED ERROR},
  Comment                  = {Cybernaetics. Citations - 927},
  Url                      = {https://books.google.com/books?hl=en&lr=&id=G2_4Eu6hdQcC&oi=fnd&pg=PR7&dq=Cybernetics predicting device&ots=2XHmPQIyel&sig=vGmX-fdtZCFU7DCl2o_juDvkOFY}
}

@Article{firat2016multi,
  Title                    = {Multi-way, multilingual neural machine translation with a shared attention mechanism},
  Author                   = {Firat, Orhan and Cho, Kyunghyun and Bengio, Yoshua},
  Journal                  = {arXiv preprint arXiv:1601.01073},
  Year                     = {2016},

  Abstract                 = {We propose multi-way, multilingual neural machine translation. The proposed approach enables a single neural translation model to
translate between multiple languages, with a number of parameters that grows only linearly with the number of languages. This is made possible by having a single attention mechanism that is shared across all language pairs. We train the proposed multiway, multilingual model on ten language pairs from WMT’15 simultaneously and observe clear performance improvements over models trained on only one language pair. In particular, we observe that the proposed model significantly improves the translation quality of low-resource language pairs.},
  Comment                  = {Machine translation. Citations - 329},
  Url                      = {https://arxiv.org/pdf/1601.01073}
}

@Article{firat2016zero,
  Title                    = {Zero-resource translation with multi-lingual neural machine translation},
  Author                   = {Firat, Orhan and Sankaran, Baskaran and Al-Onaizan, Yaser and Vural, Fatos T Yarman and Cho, Kyunghyun},
  Journal                  = {arXiv preprint arXiv:1606.04164},
  Year                     = {2016},

  Abstract                 = {In this paper, we propose a novel finetuning algorithm for the recently introduced multiway, mulitlingual neural machine translate
that enables zero-resource machine translation. When used together with novel manyto-one translation strategies, we empirically show that this finetuning algorithm allows the multi-way, multilingual model to translate a zero-resource language pair (1) as well as a
single-pair neural translation model trained with up to 1M direct parallel sentences of the same language pair and (2) better than pivotbased translation strategy, while keeping only one additional copy of attention-related parameters.},
  Comment                  = {Machine translation. Citations - 128},
  Url                      = {https://arxiv.org/pdf/1606.04164}
}

@Article{Fu2018,
  Title                    = {Kiwifruit detection in field images using Faster R-CNN with ZFNet},
  Author                   = {Fu, Longsheng and Feng, Yali and Majeed, Yaqoob and Zhang, Xin and Zhang, Jing and Karkee, Manoj and Zhang, Qin},
  Journal                  = {IFAC-PapersOnLine},
  Year                     = {2018},
  Number                   = {17},
  Pages                    = {45--50},
  Volume                   = {51},

  Abstract                 = {A kiwifruit detection system for field images was developed based on the deep convolutional neural network, which has a good robustness against the subjectivity and limitation of the
features selected artificially. Under different lighting conditions, 2,100 sub-images with 784×},
  Comment                  = {ZFNet. Citations - 18},
  Owner                    = {manoj},
  Publisher                = {Elsevier},
  Timestamp                = {2020.08.26},
  Url                      = {https://www.researchgate.net/profile/Longsheng_Fu/publication/327616829_Kiwifruit_detection_in_field_images_using_Faster_R-CNN_with_ZFNet/links/5b9ffaf745851574f7d25805/Kiwifruit-detection-in-field-images-using-Faster-R-CNN-with-ZFNet.pdf}
}

@Article{fukui2016multimodal,
  Title                    = {Multimodal compact bilinear pooling for visual question answering and visual grounding},
  Author                   = {Fukui, Akira and Park, Dong Huk and Yang, Daylen and Rohrbach, Anna and Darrell, Trevor and Rohrbach, Marcus},
  Journal                  = {arXiv preprint arXiv:1606.01847},
  Year                     = {2016},

  Abstract                 = {Modeling textual or visual information with vector representations trained from large language or visual datasets has been successfully explored in recent years. However, tasks such as visual question answering require combining these vector representations with each other. Approaches to multimodal pooling include element-wise product or sum, as well as concatenation of the visual and textual representations. We hypothesize that these methods are not as expressive as an outer product of the visual and textual vectors. As the outer product is typically infeasible due to its high dimensionality, we instead propose utilizing Multimodal Compact Bilinear pooling (MCB) to efficiently and expressively combine multimodal features. We extensively evaluate MCB on the visual question answering and grounding tasks. We consistently show the benefit of MCB over ablations without MCB. For visual question answering, we present an architecture which uses MCB twice, once for predicting attention over spatial features and again to combine the attended representation with the question representation. This model outperforms the state-of-the-art on the Visual7W dataset and the VQA challenge.},
  Comment                  = {Visual question answering. Citations - 785},
  Url                      = {https://arxiv.org/pdf/1606.01847?source=post_page---------------------------}
}

@Article{Fukushima2007,
  Title                    = {Neocognitron},
  Author                   = {Fukushima, Kunihiko},
  Journal                  = {Scholarpedia},
  Year                     = {2007},
  Number                   = {1},
  Pages                    = {1717},
  Volume                   = {2},

  Comment                  = {Citation - 147},
  Owner                    = {manoj},
  Timestamp                = {2020.08.20},
  Url                      = {http://var.scholarpedia.org/article/Neocognitron}
}

@Article{galison1994ontology,
  Title                    = {The ontology of the enemy: Norbert Wiener and the cybernetic vision},
  Author                   = {Galison, Peter},
  Journal                  = {Critical inquiry},
  Year                     = {1994},
  Number                   = {1},
  Pages                    = {228--266},
  Volume                   = {21},

  Abstract                 = {The servomechanical enemy became, in the cybernetic vision of the 1940s, the prototype for
human … Cybernetics no longer appears as a futuristic bandwagon or as a ris- ing worldview that
will … would have to be filtered to gain a smoother curve that the predicting circuit could},
  Comment                  = {Cybernetic vision. Citations - 607.},
  Publisher                = {University of Chicago Press},
  Url                      = {http://jerome-segal.de/Galison94.pdf}
}

@Article{gao2017video,
  Title                    = {Video captioning with attention-based LSTM and semantic consistency},
  Author                   = {Gao, Lianli and Guo, Zhao and Zhang, Hanwang and Xu, Xing and Shen, Heng Tao},
  Journal                  = {IEEE Transactions on Multimedia},
  Year                     = {2017},
  Number                   = {9},
  Pages                    = {2045--2055},
  Volume                   = {19},

  Abstract                 = {Recent progress in using long short-term memory (LSTM) for image captioning has motivated the exploration of their applications for video captioning. By taking a video as a sequence of features, an LSTM model is trained on video-sentence pairs and learns to associate a video to a sentence. However, most existing methods compress an entire video shot or frame into a static representation, without considering attention mechanism which allows for selecting salient features. Furthermore, existing approaches usually model the translating error, but ignore the correlations between sentence semantics and visual content. To tackle these issues, we propose a novel end-to-end framework named aLSTMs, an attention-based LSTM model with semantic consistency, to transfer videos to natural sentences. This framework integrates attention mechanism with LSTM to capture salient structures of video, and explores the correlation between multimodal representations (i.e., words and visual content) for generating sentences with rich semantic content. Specifically, we first propose an attention mechanism that uses the dynamic weighted sum of local two-dimensional convolutional neural network representations. Then, an LSTM decoder takes these visual features at time t and the word-embedding feature at time t-1 to generate important words. Finally, we use multimodal embedding to map the visual and sentence features into a joint space to guarantee the semantic consistence of the sentence description and the video visual content. Experiments on the benchmark datasets demonstrate that our method using single feature can achieve competitive or even better results than the state-of-the-art baselines for video captioning in both BLEU and METEOR.},
  Comment                  = {Video captioning. Citations - 264},
  Publisher                = {IEEE},
  Url                      = {https://ieeexplore.ieee.org/abstract/document/7984828}
}

@Article{ghazvininejad2017knowledge,
  Title                    = {A knowledge-grounded neural conversation model},
  Author                   = {Ghazvininejad, Marjan and Brockett, Chris and Chang, Ming-Wei and Dolan, Bill and Gao, Jianfeng and Yih, Wen-tau and Galley, Michel},
  Journal                  = {arXiv preprint arXiv:1702.01932},
  Year                     = {2017},

  Abstract                 = {Neural network models are capable of generating extremely natural sounding conversational interactions. Nevertheless, these models have yet to demonstrate that they can incorporate content in the form of factual information or entity-grounded opinion that would enable them to serve in more task-oriented conversational applications. This paper presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. We generalize the widely-used Seq2Seq approach by conditioning responses on both conversation history and external "facts", allowing the model to be versatile and applicable in an open-domain setting. Our approach yields significant improvements over a competitive Seq2Seq baseline. Human judges found that our outputs are significantly more informative.},
  Comment                  = {Conversation model. Citations - 223},
  Url                      = {https://arxiv.org/pdf/1702.01932}
}

@InProceedings{giusti2013fast,
  Title                    = {Fast image scanning with deep max-pooling convolutional neural networks},
  Author                   = {Giusti, Alessandro and Cire{\c{s}}an, Dan C and Masci, Jonathan and Gambardella, Luca M and Schmidhuber, J{\"u}rgen},
  Booktitle                = {2013 IEEE International Conference on Image Processing},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {4034--4038},

  Abstract                 = {Deep Neural Networks now excel at image classification, detection and segmentation. When used to scan images by means of a sliding window, however, their high computational complexity can bring even the most powerful hardware to its knees. We show how dynamic programming can speedup the process by orders of magnitude, even when max-pooling layers are present.},
  Comment                  = {Max-pooling CNN. Citations - 249.},
  Url                      = {@inproceedings{giusti2013fast,
 title={Fast image scanning with deep max-pooling convolutional neural networks},
 author={Giusti, Alessandro and Cire{\c{s}}an, Dan C and Masci, Jonathan and Gambardella, Luca M and Schmidhuber, J{\"u}rgen},
 booktitle={2013 IEEE International Conference on Image Processing},
 pages={4034--4038},
 year={2013},
 organization={IEEE}
}}
}

@Article{goodfellow2016nips,
  Title                    = {NIPS 2016 tutorial: Generative adversarial networks},
  Author                   = {Goodfellow, Ian},
  Journal                  = {arXiv preprint arXiv:1701.00160},
  Year                     = {2016},

  Abstract                 = {We propose a new framework for estimating generative models via adversarial nets, in which we simultaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G. The training procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D, a unique solution exists, with G recovering the training data distribution and D equal to 1/2 everywhere. In the case where G and D are defined by multilayer perceptrons, the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference networks during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitatively evaluation of the generated samples.},
  Comment                  = {Generative adversial network. Citations - 22336.},
  Url                      = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf}
}

@InProceedings{Goyal2017,
  Title                    = {The" Something Something" Video Database for Learning and Evaluating Visual Common Sense.},
  Author                   = {Goyal, Raghav and Kahou, Samira Ebrahimi and Michalski, Vincent and Materzynska, Joanna and Westphal, Susanne and Kim, Heuna and Haenel, Valentin and Fruend, Ingo and Yianilos, Peter and Mueller-Freitag, Moritz and others},
  Booktitle                = {ICCV},
  Year                     = {2017},
  Number                   = {4},
  Pages                    = {5},
  Volume                   = {1},

  Abstract                 = {Neural networks trained on datasets such as ImageNet have led to major advances in visual object classification. One obstacle that prevents networks from reasoning more deeply about complex scenes and situations, and from integrating visual knowledge with natural language, like humans do, is their lack of common sense knowledge about the physical world. Videos, unlike still images, contain a wealth of detailed information about the physical world. However, most labelled video datasets represent high-level concepts rather than detailed physical aspects about actions and scenes. In this work, we describe our ongoing collection of the “something-something” database of video prediction tasks whose solutions require a common sense understanding of the depicted situation. The database currently contains more than 100,000 videos across 174 classes, which are defined as caption-templates. We also describe the challenges in crowd-sourcing this data at scale.},
  Comment                  = {VCR dataset. Citations - 191},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {http://openaccess.thecvf.com/content_ICCV_2017/papers/Goyal_The_Something_Something_ICCV_2017_paper.pdf}
}

@InProceedings{graves2013hybrid,
  Title                    = {Hybrid speech recognition with deep bidirectional LSTM},
  Author                   = {Graves, Alex and Jaitly, Navdeep and Mohamed, Abdel-rahman},
  Booktitle                = {2013 IEEE workshop on automatic speech recognition and understanding},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {273--278},

  Abstract                 = {Deep Bidirectional LSTM (DBLSTM) recurrent neural networks have recently been shown to give state-of-the-art performance on the TIMIT speech database. However, the results in that work relied on recurrent-neural-network-specific objective functions, which are difficult to integrate with existing large vocabulary speech recognition systems. This paper investigates the use of DBLSTM as an acoustic model in a standard neural network-HMM hybrid system. We find that a DBLSTM-HMM hybrid gives equally good results on TIMIT as the previous work. It also outperforms both GMM and deep network benchmarks on a subset of the Wall Street Journal corpus. However the improvement in word error rate over the deep network is modest, despite a great increase in framelevel accuracy. We conclude that the hybrid approach with DBLSTM appears to be well suited for tasks where acoustic modelling predominates. Further investigation needs to be conducted to understand how to better leverage the improvements in frame-level accuracy towards better word error rates},
  Comment                  = {Speech recognition. Citations - 1150},
  Url                      = {http://www.cs.toronto.edu/~graves/asru_2013.pdf}
}

@InProceedings{Graves2013,
  Title                    = {Speech recognition with deep recurrent neural networks},
  Author                   = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  Booktitle                = {2013 IEEE international conference on acoustics, speech and signal processing},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {6645--6649},

  Abstract                 = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT
phoneme recognition benchmark, which to our knowledge is
the best recorded score.},
  Comment                  = {Speech recognition. CItation - 6205},
  Url                      = {https://arxiv.org/pdf/1303.5778.pdfï¼‰ï¼š“å¦‚æžœLSTMç”¨äºŽéš è— å±‚ï¼Œæˆ‘ä»¬å°†å¾—åˆ°}
}

@InProceedings{graves2013speech,
  Title                    = {Speech recognition with deep recurrent neural networks},
  Author                   = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
  Booktitle                = {2013 IEEE international conference on acoustics, speech and signal processing},
  Year                     = {2013},
  Organization             = {IEEE},
  Pages                    = {6645--6649},

  Abstract                 = {Recurrent neural networks (RNNs) are a powerful model forsequential data. End-to-end training methods such as Connec-tionist Temporal Classification make it possible to train RNNsfor sequence labelling problems where the input-output align-ment is unknown. The combination of these methods withthe Long Short-term Memory RNN architecture has provedparticularly fruitful, delivering state-of-the-art results in cur-sive handwriting recognition. However RNN performance inspeech recognition has so far been disappointing, with betterresults returned by deep feedforward networks. This paper in-vestigatesdeep recurrent neural networks, which combine themultiple levels of representation that have proved so effectivein deep networks with the flexible use of long range contextthat empowers RNNs. When trained end-to-end with suit-able regularisation, we find that deep Long Short-term Mem-ory RNNs achieve a test set error of 17.7% on the TIMITphoneme recognition benchmark, which to our knowledge isthe best recorded score.},
  Comment                  = {Speech recognition using DRNN. Citations - 6139},
  Owner                    = {manoj},
  Timestamp                = {2020.08.27},
  Url                      = {https://arxiv.org/pdf/1303.5778.pdf%C3%AF%C2%BC%E2%80%B0%C3%AF%C2%BC%C5%A1%E2%80%9C%C3%A5%C2%A6%E2%80%9A%C3%A6%C5%BE%C5%93LSTM%C3%A7%E2%80%9D%C2%A8%C3%A4%C2%BA%C5%BD%C3%A9%C5%A1%20%C3%A8%E2%80%94%20%C3%A5%C2%B1%E2%80%9A%C3%AF%C2%BC%C5%92%C3%A6%CB%86%E2%80%98%C3%A4%C2%BB%C2%AC%C3%A5%C2%B0%E2%80%A0%C3%A5%C2%BE%E2%80%94%C3%A5%CB%86%C2%B0}
}

@InProceedings{graves2009offline,
  Title                    = {Offline handwriting recognition with multidimensional recurrent neural networks},
  Author                   = {Graves, Alex and Schmidhuber, J{\"u}rgen},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2009},
  Pages                    = {545--552},

  Abstract                 = {Offline handwriting recognition---the transcription of images of handwritten text---is an interesting task, in that it combines computer vision with sequence learning. In most systems the two elements are handled separately, with sophisticated preprocessing techniques used to extract the image features and sequential models such as HMMs used to provide the transcriptions. By combining two recent innovations in neural networks---multidimensional recurrent neural networks and connectionist temporal classification---this paper introduces a globally trained offline handwriting recogniser that takes raw pixel data as input. Unlike competing systems, it does not require any alphabet specific preprocessing, and can therefore be used unchanged for any language. Evidence of its generality and power is provided by data from a recent international Arabic recognition competition, where it outperformed all entries (91.4% accuracy compared to 87.2% for the competition winner) despite the fact that neither author understands a word of Arabic.},
  Comment                  = {handwriting recognition with RNN.},
  Url                      = {https://papers.nips.cc/paper/3449-offline-handwriting-recognition-with-multidimensional-recurrent-neural-networks.pdf}
}

@InProceedings{he2017mask,
  Title                    = {Mask r-cnn},
  Author                   = {He, Kaiming and Gkioxari, Georgia and Doll{\'a}r, Piotr and Girshick, Ross},
  Booktitle                = {Proceedings of the IEEE international conference on computer vision},
  Year                     = {2017},
  Pages                    = {2961--2969},

  Abstract                 = {We present a conceptually simple, flexible, and general framework for object instance segmentation. Our approach efficiently detects objects in an image while simultaneously generating a high-quality segmentation mask for each instance. The method, called Mask R-CNN, extends Faster R-CNN by adding a branch for predicting an object mask in parallel with the existing branch for bounding box recognition. Mask R-CNN is simple to train and adds only a small overhead to Faster R-CNN, running at 5 fps. Moreover, Mask R-CNN is easy to generalize to other tasks, e.g., allowing us to estimate human poses in the same framework. We show top results in all three tracks of the COCO suite of challenges, including instance segmentation, bounding-box object detection, and person keypoint detection. Without tricks, Mask R-CNN outperforms all existing, single-model entries on every task, including the COCO 2016 challenge winners. We hope our simple and effective approach will serve as a solid baseline and help ease future research in instance-level recognition. Code will be made available.},
  Comment                  = {Mask R-CNN. Citations - 7710.},
  Url                      = {http://openaccess.thecvf.com/content_ICCV_2017/papers/He_Mask_R-CNN_ICCV_2017_paper.pdf}
}

@InProceedings{hendricks2016deep,
  Title                    = {Deep compositional captioning: Describing novel object categories without paired training data},
  Author                   = {Hendricks, Lisa Anne and Venugopalan, Subhashini and Rohrbach, Marcus and Mooney, Raymond and Saenko, Kate and Darrell, Trevor},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2016},
  Pages                    = {1--10},

  Abstract                 = {While recent deep neural network models have achievedpromising results on the image captioning task, they relylargely on the availability of corpora with paired im-age and sentence captions to describe objects in context.In this work, we propose the Deep Compositional Cap-tioner (DCC) to address the task of generating descriptionsof novel objects which are not present in paired image-sentence datasets. Our method achieves this by leveraginglarge object recognition datasets and external text corporaand by transferring knowledge between semantically sim-ilar concepts. Current deep caption models can only de-scribe objects contained in paired image-sentence corpora,despite the fact that they are pre-trained with large objectrecognition datasets, namely ImageNet. In contrast, ourmodel can compose sentences that describe novel objectsand their interactions with other objects. We demonstrateour model’s ability to describe novel concepts by empir-ically evaluating its performance on MSCOCO and showqualitative results on ImageNet images of objects for whichno paired image-sentence data exist. Further, we extendour approach to generate descriptions of objects in videoclips. Our results show that DCC has distinct advantagesover existing image and video captioning approaches forgenerating descriptions of new objects in context.},
  Comment                  = {VIdeo captioning. Citations - 185},
  Url                      = {https://openaccess.thecvf.com/content_cvpr_2016/papers/Hendricks_Deep_Compositional_Captioning_CVPR_2016_paper.pdf}
}

@Article{Hinton2012,
  Title                    = {Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  Author                   = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  Journal                  = {IEEE Signal processing magazine},
  Year                     = {2012},
  Number                   = {6},
  Pages                    = {82--97},
  Volume                   = {29},

  Abstract                 = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to deter-mine how well each state of each HMM fits a frame or a short window of frames of coefficients that repre-sents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior proba-bilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a vari-ety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition},
  Comment                  = {DNN for speech. Citations - 8210},
  Owner                    = {manoj},
  Publisher                = {IEEE},
  Timestamp                = {2020.08.27},
  Url                      = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/HintonDengYuEtAl-SPM2012.pdf}
}

@Article{hinton2012deep,
  Title                    = {Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
  Author                   = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and others},
  Journal                  = {IEEE Signal processing magazine},
  Year                     = {2012},
  Number                   = {6},
  Pages                    = {82--97},
  Volume                   = {29},

  Abstract                 = {Recurrent neural networks (RNNs) are a powerful model for
sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved
particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in
speech recognition has so far been disappointing, with better
results returned by deep feedforward networks. This paper investigates deep recurrent neural networks, which combine the
multiple levels of representation that have proved so effective
in deep networks with the flexible use of long range context
that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7% on the TIMIT
phoneme recognition benchmark, which to our knowledge is
the best recorded score.},
  Comment                  = {Speech recognition. CItation - 8264},
  Publisher                = {IEEE},
  Url                      = {https://arxiv.org/pdf/1303.5778.pdfï¼‰ï¼š“å¦‚æžœLSTMç”¨äºŽéš è— å±‚ï¼Œæˆ‘ä»¬å°†å¾—åˆ°}
}

@Article{hornik1991approximation,
  Title                    = {Approximation capabilities of multilayer feedforward networks},
  Author                   = {Hornik, Kurt},
  Journal                  = {Neural networks},
  Year                     = {1991},
  Number                   = {2},
  Pages                    = {251--257},
  Volume                   = {4},

  Abstract                 = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to V(p.) per-formance criteria, for arbitrary finite input environment measures Jl., provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives},
  Comment                  = {Multilayer feedforward network. Citations - 4682.},
  Publisher                = {Elsevier},
  Url                      = {http://www.vision.jhu.edu/teaching/learning/deeplearning18/assets/Hornik-91.pdf}
}

@Article{hornik1989multilayer,
  Title                    = {Multilayer feedforward networks are universal approximators.},
  Author                   = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert and others},
  Journal                  = {Neural networks},
  Year                     = {1989},
  Number                   = {5},
  Pages                    = {359--366},
  Volume                   = {2},

  Abstract                 = {universal approximators: standard multilayer feedforward networks are capable of approximating
any measurable function to any desired degree of accuracy - there are no theoretical constraints
for the success of feedforward networks - lack of success is due to inadequate learning, insufficient},
  Comment                  = {Feedforward network. Citations - 19399.},
  Url                      = {https://www.cs.cmu.edu/~bhiksha/courses/deeplearning/Fall.2016/notes/Sonia_Hornik.pdf}
}

@InProceedings{hu2017learning,
  Title                    = {Learning to reason: End-to-end module networks for visual question answering},
  Author                   = {Hu, Ronghang and Andreas, Jacob and Rohrbach, Marcus and Darrell, Trevor and Saenko, Kate},
  Booktitle                = {Proceedings of the IEEE International Conference on Computer Vision},
  Year                     = {2017},
  Pages                    = {804--813},

  Abstract                 = {Natural language questions are inherently compositional,and many are most easily answered by reasoning about theirdecomposition into modular sub-problems. For example, toanswer“is there an equal number of balls and boxes?”wecan look for balls, look for boxes, count them, and com-pare the results. The recently proposed Neural Module Net-work (NMN) architecture [3,2] implements this approach toquestion answering by parsing questions into linguistic sub-structures and assembling question-specific deep networksfrom smaller modules that each solve one subtask. However,existing NMN implementations rely on brittle off-the-shelfparsers, and are restricted to the module configurations pro-posed by these parsers rather than learning them from data.In this paper, we propose End-to-End Module Networks(N2NMNs), which learn to reason by directly predictinginstance-specific network layouts without the aid of a parser.Our model learns to generate networkstructures(by imitat-ing expert demonstrations) while simultaneously learningnetworkparameters(using the downstream task loss). Exper-imental results on the newCLEVRdataset targeted at com-positional question answering show that N2NMNs achievean error reduction of nearly 50% relative to state-of-the-art attentional approaches, while discovering interpretablenetwork architectures specialized for each question.},
  Comment                  = {Question answering. Citations - 307},
  Url                      = {https://openaccess.thecvf.com/content_ICCV_2017/papers/Hu_Learning_to_Reason_ICCV_2017_paper.pdf}
}

@Article{jang2019video,
  Title                    = {Video question answering with spatio-temporal reasoning},
  Author                   = {Jang, Yunseok and Song, Yale and Kim, Chris Dongjoo and Yu, Youngjae and Kim, Youngjin and Kim, Gunhee},
  Journal                  = {International Journal of Computer Vision},
  Year                     = {2019},
  Number                   = {10},
  Pages                    = {1385--1412},
  Volume                   = {127},

  Abstract                 = {Vision and language understanding has emerged as a subject undergoing intense study in Artificial Intelligence. Among many tasks in this line of research, visual question answering (VQA) has been one of the most successful ones, where the goal is to learn a model that understands visual content at region-level details and finds their associations with pairs of questions and answers in the natural language form. Despite the rapid progress in the past few years, most existing work in VQA have focused primarily on images. In this paper, we focus on extending VQA to the video domain and contribute to the literature in three important ways. First, we propose three new tasks designed specifically for video VQA, which require spatio-temporal reasoning from videos to answer questions correctly. Next, we introduce a new large-scale dataset for video VQA named TGIF-QA that extends existing VQA work with our new tasks. Finally, we propose a dual-LSTM based approach with both spatial and temporal attention and show its effectiveness over conventional VQA techniques through empirical evaluations.},
  Comment                  = {Video question answering. Citations - 2},
  Publisher                = {Springer},
  Url                      = {https://link.springer.com/article/10.1007/s11263-019-01189-x}
}

@Article{jean2014using,
  Title                    = {On using very large target vocabulary for neural machine translation},
  Author                   = {Jean, S{\'e}bastien and Cho, Kyunghyun and Memisevic, Roland and Bengio, Yoshua},
  Journal                  = {arXiv preprint arXiv:1412.2007},
  Year                     = {2014},

  Abstract                 = {Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling
a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling
that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be
efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole
target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based
neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured
by BLEU) on both the English→German and English→French translation tasks of WMT’14.},
  Comment                  = {Machine translations. Citaions - 792},
  Url                      = {https://arxiv.org/pdf/1412.2007}
}

@Article{ji2019video,
  Title                    = {Video summarization with attention-based encoder-decoder networks},
  Author                   = {Ji, Zhong and Xiong, Kailin and Pang, Yanwei and Li, Xuelong},
  Journal                  = {IEEE Transactions on Circuits and Systems for Video Technology},
  Year                     = {2019},

  Abstract                 = {Given the explosive growth of online videos, it is becoming increasingly important to relieve the tedious work of browsing and managing the video content of interest. Video summarization aims at providing such a technique by transforming one or multiple videos into a compact one. However, conventional multi-video summarization methods often fail to produce satisfying results as they ignore the users’ search intents. To this end, this paper proposes a novel query-aware approach by formulating the multi-video summarization in a sparse coding framework, where the web images searched by a query are taken as the important preference information to reveal the query intent. To provide a user-friendly summarization, this paper also develops an event-keyframe presentation structure to present keyframes in groups of specific events related to the query by using an unsupervised multi-graph fusion method. Moreover, we release a new public dataset named MVS1K, which contains about 1000 videos from 10 queries and their video tags, manual annotations, and associated web images. Extensive experiments on the MVS1K and TVSum datasets demonstrate that our approaches produce competitively objective and subjective results.},
  Comment                  = {VIdeo summarization. Citation - 16},
  Publisher                = {IEEE},
  Url                      = {https://www.sciencedirect.com/science/article/pii/S002002551830762X}
}

@InProceedings{johnson2016densecap,
  Title                    = {Densecap: Fully convolutional localization networks for dense captioning},
  Author                   = {Johnson, Justin and Karpathy, Andrej and Fei-Fei, Li},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2016},
  Pages                    = {4565--4574},

  Abstract                 = {We introduce the dense captioning task, which requires a computer vision system to both localize and describe salient regions in images in natural language. The dense captioning task generalizes object detection when the descriptions consist of a single word, and Image Captioning when one predicted region covers the full image. To address the localization and description task jointly we propose a Fully Convolutional Localization Network (FCLN) architecture that processes an image with a single, efficient forward pass, requires no external regions proposals, and can be trained end-to-end with a single round of optimization. The architecture is composed of a Convolutional Network, a novel dense localization layer, and Recurrent Neural Network language model that generates the label sequences. We evaluate our network on the Visual Genome dataset, which comprises 94,000 images and 4,100,000 region-grounded captions. We observe both speed and accuracy improvements over baselines based on current state of the art approaches in both generation and retrieval settings.},
  Comment                  = {Image captioning. Citations - 761},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2016/papers/Johnson_DenseCap_Fully_Convolutional_CVPR_2016_paper.pdf}
}

@InProceedings{kalchbrenner2013recurrent,
  Title                    = {Recurrent continuous translation models},
  Author                   = {Kalchbrenner, Nal and Blunsom, Phil},
  Booktitle                = {Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  Year                     = {2013},
  Pages                    = {1700--1709},

  Abstract                 = {We introduce a class of probabilistic continuous translation models called Recurrent Continuous Translation Models that are purely based on continuous representations for words, phrases and sentences and do not rely on alignments or phrasal translation units.
The models have a generation and a conditioning aspect. The generation of the translation is modelled with a target Recurrent Language Model, whereas the conditioning on the source sentence is modelled with a Convolutional Sentence Model. Through various experiments, we show first that our models obtain a perplexity with respect to gold translations that is > 43% lower than that of stateof-the-art alignment-based translation models. Secondly, we show that they are remarkably sensitive to the word order, syntax, and meaning of the source sentence despite lacking alignments. Finally we show that they match a state-of-the-art system when rescoring n-best lists of translations.},
  Comment                  = {Machine translation. Citations - 1097},
  Url                      = {https://www.aclweb.org/anthology/D13-1176.pdf}
}

@InProceedings{karpathy2015deep,
  Title                    = {Deep visual-semantic alignments for generating image descriptions},
  Author                   = {Karpathy, Andrej and Fei-Fei, Li},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2015},
  Pages                    = {3128--3137},

  Abstract                 = {We present a model that generates natural language descriptions of images and their regions. Our approach leverages datasets of images and their sentence descriptions to learn about the inter-modal correspondences between language and visual data. Our alignment model is based on a novel combination of Convolutional Neural Networks over image regions, bidirectional Recurrent Neural Networks over sentences, and a structured objective that aligns the two modalities through a multimodal embedding. We then describe a Multimodal Recurrent Neural Network architecture that uses the inferred alignments to learn to generate novel descriptions of image regions. We demonstrate that our alignment model produces state of the art results in retrieval experiments on Flickr8K, Flickr30K and MSCOCO datasets. We then show that the generated descriptions significantly outperform retrieval baselines on both full images and on a new dataset of region-level annotations.},
  Comment                  = {Image captioning. Citaitons - 3693},
  Url                      = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Karpathy_Deep_Visual-Semantic_Alignments_2015_CVPR_paper.pdf}
}

@InProceedings{karpathy2014deep,
  Title                    = {Deep fragment embeddings for bidirectional image sentence mapping},
  Author                   = {Karpathy, Andrej and Joulin, Armand and Fei-Fei, Li F},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2014},
  Pages                    = {1889--1897},

  Abstract                 = {We introduce a model for bidirectional retrieval of images and sentences througha deep, multi-modal embedding of visual and natural language data. Unlike pre-vious models that directly map images or sentences into a common embeddingspace, our model works on a finer level and embeds fragments of images (ob-jects) and fragments of sentences (typed dependency tree relations) into a com-mon space. We then introduce a structured max-margin objective that allows ourmodel to explicitly associate these fragments across modalities. Extensive exper-imental evaluation shows that reasoning on both the global level of images andsentences and the finer level of their respective fragments improves performanceon image-sentence retrieval tasks. Additionally, our model provides interpretablepredictions for the image-sentence retrieval task since the inferred inter-modalalignment of fragments is explicit.},
  Comment                  = {Image captioning. Citaions - 641.},
  Url                      = {https://papers.nips.cc/paper/5281-deep-fragment-embeddings-for-bidirectional-image-sentence-mapping.pdf}
}

@Article{karras2017progressive,
  Title                    = {Progressive growing of gans for improved quality, stability, and variation},
  Author                   = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
  Journal                  = {arXiv preprint arXiv:1710.10196},
  Year                     = {2017},

  Abstract                 = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly fine details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CelebA images at 1024^2. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CelebA dataset.},
  Comment                  = {Progressive growing of GANs. Citations - 2076.},
  Url                      = {https://arxiv.org/pdf/1710.10196.pdf?__hstc=200028081.1bb630f9cde2cb5f07430159d50a3c91.1524009600081.1524009600082.1524009600083.1&__hssc=200028081.1.1524009600084&__hsfp=1773666937}
}

@Article{kawaguchi2017generalization,
  Title                    = {Generalization in deep learning},
  Author                   = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  Journal                  = {arXiv preprint arXiv:1710.05468},
  Year                     = {2017},

  Abstract                 = {This paper provides theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also discuss approaches to provide non-vacuous generalization guarantees for deep learning. Based on theoretical observations, we propose new open problems and discuss the limitations of our results.},
  Comment                  = {Genneralization. Citations - 171.},
  Url                      = {https://arxiv.org/pdf/1710.05468}
}

@Article{kazemi2017show,
  Title                    = {Show, ask, attend, and answer: A strong baseline for visual question answering},
  Author                   = {Kazemi, Vahid and Elqursh, Ali},
  Journal                  = {arXiv preprint arXiv:1704.03162},
  Year                     = {2017},

  Abstract                 = {This paper presents a new baseline for visual question answering task. Given an image and a question in natural language, our model produces accurate answers according to the content of the image. Our model, while being architecturally simple and relatively small in terms of trainable parameters, sets a new state of the art on both unbalanced and balanced VQA benchmark. On VQA 1.0 open ended challenge, our model achieves 64.6% accuracy on the test-standard set without using additional data, an improvement of 0.4% over state of the art, and on newly released VQA 2.0, our model scores 59.7% on validation set outperforming best previously reported results by 0.5%. The results presented in this paper are especially interesting because very similar models have been tried before but significantly lower performance were reported. In light of the new results we hope to see more meaningful research on visual question answering in the future.},
  Comment                  = {Visual question answering. Citations - 88},
  Url                      = {https://arxiv.org/pdf/1704.03162}
}

@Article{kim2017learning,
  Title                    = {Learning to discover cross-domain relations with generative adversarial networks},
  Author                   = {Kim, Taeksoo and Cha, Moonsu and Kim, Hyunsoo and Lee, Jung Kwon and Kim, Jiwon},
  Journal                  = {arXiv preprint arXiv:1703.05192},
  Year                     = {2017},

  Abstract                 = {While humans easily recognize relations between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the relations. To avoid costly pairing, we address the task of discovering cross-domain relations given unpaired data. We propose a method based on generative adversarial networks that learns to discover relations between different domains (DiscoGAN). Using the discovered relations, our proposed network successfully transfers style from one domain to another while preserving key attributes such as orientation and face identity. Source code for official implementation is publicly available this https URL},
  Comment                  = {Adversial network. Citations - 986.},
  Url                      = {https://arxiv.org/pdf/1703.05192.pdf?ref=hackernoon.com}
}

@InProceedings{kingma2016improved,
  Title                    = {Improved variational inference with inverse autoregressive flow},
  Author                   = {Kingma, Durk P and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2016},
  Pages                    = {4743--4751},

  Abstract                 = {The framework of normalizing flows provides a general strategy for flexible vari-ational inference of posteriors over latent variables. We propose a new type ofnormalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlierpublished flows, scales well to high-dimensional latent spaces. The proposed flowconsists of a chain of invertible transformations, where each transformation isbased on an autoregressive neural network. In experiments, we show that IAFsignificantly improves upon diagonal Gaussian approximate posteriors. In addition,we demonstrate that a novel type of variational autoencoder, coupled with IAF, iscompetitive with neural autoregressive models in terms of attained log-likelihoodon natural images, while allowing significantly faster synthesis.},
  Comment                  = {Variational autoencoders. Citations - 821.},
  Url                      = {https://papers.nips.cc/paper/6581-improved-variational-inference-with-inverse-autoregressive-flow.pdf}
}

@Article{kingma2019introduction,
  Title                    = {An introduction to variational autoencoders},
  Author                   = {Kingma, Diederik P and Welling, Max},
  Journal                  = {arXiv preprint arXiv:1906.02691},
  Year                     = {2019},

  Abstract                 = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  Comment                  = {Variational autoencoders. Citations - 89.},
  Url                      = {https://arxiv.org/pdf/1906.02691}
}

@InProceedings{Kirillov2019,
  Title                    = {Panoptic segmentation},
  Author                   = {Kirillov, Alexander and He, Kaiming and Girshick, Ross and Rother, Carsten and Doll{\'a}r, Piotr},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2019},
  Pages                    = {9404--9413},

  Abstract                 = {We propose and study a task we name panoptic segmentation (PS). Panoptic segmentation unifies the typically distinct tasks of semantic segmentation (assign a class label to each pixel) and instance segmentation (detect and segment each object instance). The proposed task requires generating a coherent scene segmentation that is rich and complete, an important step toward real-world vision systems. While early work in computer vision addressed related image/scene parsing tasks, these are not currently
popular, possibly due to lack of appropriate metrics or associated recognition challenges. To address this, we propose a novel panoptic quality (PQ) metric that captures performance for all classes (stuff and things) in an interpretable and unified manner. Using the proposed metric, we perform a rigorous study of both human and machine performance for PS on three existing datasets, revealing interesting insights about the task. The aim of our work is to revive the interest of the community in a more unified
view of image segmentation. For more analysis and up-todate results, please check the arXiv version of the paper: https://arxiv.org/abs/1801.00868.},
  Comment                  = {Panoptic segmentation. CItations - 199},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {http://openaccess.thecvf.com/content_CVPR_2019/papers/Kirillov_Panoptic_Segmentation_CVPR_2019_paper.pdf}
}

@Article{Klein2017,
  Title                    = {Opennmt: Open-source toolkit for neural machine translation},
  Author                   = {Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Senellart, Jean and Rush, Alexander M},
  Journal                  = {arXiv preprint arXiv:1701.02810},
  Year                     = {2017},

  Abstract                 = {We describe an open-source toolkit for neural machine translation (NMT). The toolkit prioritizes efficiency, modularity, and extensibility with the goal of supporting NMT research into model architectures, feature representations, and source modalities, while maintaining competitive performance and reasonable training requirements. The toolkit consists of modeling and translation support, as well as detailed pedagogical documentation about the underlying techniques.},
  Comment                  = {Machine translation. Citations - 991},
  Url                      = {https://arxiv.org/pdf/1701.02810}
}

@Article{klein2017opennmt,
  Title                    = {Opennmt: Open-source toolkit for neural machine translation},
  Author                   = {Klein, Guillaume and Kim, Yoon and Deng, Yuntian and Senellart, Jean and Rush, Alexander M},
  Journal                  = {arXiv preprint arXiv:1701.02810},
  Year                     = {2017}
}

@Article{Krishna2017,
  Title                    = {Visual genome: Connecting language and vision using crowdsourced dense image annotations},
  Author                   = {Krishna, Ranjay and Zhu, Yuke and Groth, Oliver and Johnson, Justin and Hata, Kenji and Kravitz, Joshua and Chen, Stephanie and Kalantidis, Yannis and Li, Li-Jia and Shamma, David A and others},
  Journal                  = {International journal of computer vision},
  Year                     = {2017},
  Number                   = {1},
  Pages                    = {32--73},
  Volume                   = {123},

  Abstract                 = {Despite progress in perceptual tasks such as image classification, computers still perform poorly on cognitive tasks such as image description and question answering. Cognition is core to tasks that involve not just recognizing, but reasoning about our visual world. However, models used to tackle the rich content in images for cognitive tasks are still being trained using the same datasets designed for perceptual tasks. To achieve success at cognitive tasks, models need to understand the interactions and relationships between objects in an image. When asked "What vehicle is the person riding?", computers will need to identify the objects in an image as well as the relationships riding(man, carriage) and pulling(horse, carriage) in order to answer correctly that "the person is riding a horse-drawn carriage".
In this paper, we present the Visual Genome dataset to enable the modeling of such relationships. We collect dense annotations of objects, attributes, and relationships within each image to learn these models. Specifically, our dataset contains over 100K images where each image has an average of 21 objects, 18 attributes, and 18 pairwise relationships between objects. We canonicalize the objects, attributes, relationships, and noun phrases in region descriptions and questions answer pairs to WordNet synsets. Together, these annotations represent the densest and largest dataset of image descriptions, objects, attributes, relationships, and question answers.},
  Comment                  = {Visual genome dataset. Citations - 1355},
  Owner                    = {manoj},
  Publisher                = {Springer},
  Timestamp                = {2020.08.26},
  Url                      = {https://arxiv.org/pdf/1602.07332}
}

@InProceedings{kristan2018sixth,
  Title                    = {The sixth visual object tracking vot2018 challenge results},
  Author                   = {Kristan, Matej and Leonardis, Ales and Matas, Jiri and Felsberg, Michael and Pflugfelder, Roman and ˇCehovin Zajc, Luka and Vojir, Tomas and Bhat, Goutam and Lukezic, Alan and Eldesokey, Abdelrahman and others},
  Booktitle                = {Proceedings of the European Conference on Computer Vision (ECCV)},
  Year                     = {2018},
  Pages                    = {0--0},

  Abstract                 = {The Visual Object Tracking challenge VOT2018 is the sixth annual tracker benchmarking activity organized by the VOT initiative. Results of over eighty trackers are presented; many are state-of-the-art trackers published at major computer vision conferences or in journals in the recent years. The evaluation included the standard VOT and other popular methodologies for short-term tracking analysis and a "real-time" experiment simulating a situation where a tracker processes images as if provided by a continuously running sensor. A long-term tracking subchallenge has been introduced to the set of standard VOT sub-challenges. The new subchallenge focuses on long-term tracking properties, namely coping with target disappearance and reappearance. A new dataset has been compiled and a performance evaluation methodology that focuses on long-term tracking capabilities has been adopted. The VOT toolkit has been updated to support both standard short-term and the new longterm tracking subchallenges. Performance of the tested trackers typically by far exceeds standard baselines. The source code for most of the trackers is publicly available from the VOT page. The dataset, the evaluation kit and the results are publicly available at the challenge website.},
  Comment                  = {Visual tracking. Citations - 199},
  Url                      = {http://openaccess.thecvf.com/content_ECCVW_2018/papers/11129/Kristan_The_sixth_Visual_Object_Tracking_VOT2018_challenge_results_ECCVW_2018_paper.pdf}
}

@InProceedings{krizhevsky2012imagenet,
  Title                    = {Imagenet classification with deep convolutional neural networks},
  Author                   = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2012},
  Pages                    = {1097--1105},

  Abstract                 = {We trained a large, deep convolutional neural network to classify the 1.2 millionhigh-resolution images in the ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. On the test data, we achieved top-1 and top-5 error rates of 37.5%and 17.0% which is considerably better than the previous state-of-the-art. Theneural network, which has 60 million parameters and 650,000 neurons, consistsof five convolutional layers, some of which are followed by max-pooling layers,and three fully-connected layers with a final 1000-way softmax. To make train-ing faster, we used non-saturating neurons and a very efficient GPU implemen-tation of the convolution operation. To reduce overfitting in the fully-connectedlayers we employed a recently-developed regularization method called “dropout”that proved to be very effective. We also entered a variant of this model in theILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%,compared to 26.2% achieved by the second-best entry},
  Comment                  = {ImageNet. Citations - 69143.},
  Url                      = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf}
}

@Article{lebret2015phrase,
  Title                    = {Phrase-based image captioning},
  Author                   = {Lebret, R{\'e}mi and Pinheiro, Pedro O and Collobert, Ronan},
  Journal                  = {arXiv preprint arXiv:1502.03671},
  Year                     = {2015},

  Abstract                 = {Generating a novel textual description of an image is an interesting problem that connects computer vision and natural language processing. In this paper, we present a simple model that is able to generate descriptive sentences given a sample image. This model has a strong focus on the syntax of the descriptions. We train a purely bilinear model that learns a metric between an image representation (generated from a previously trained Convolutional Neural Network) and phrases that are used to described them. The system is then able to infer phrases from a given image sample. Based on caption syntax statistics, we propose a simple language model that can produce relevant descriptions for a given test image using the phrases inferred. Our approach, which is considerably simpler than state-of-the-art models, achieves comparable results in two popular datasets for the task: Flickr30k and the recently proposed Microsoft COCO.},
  Comment                  = {Image captioning. Citations - 103.},
  Url                      = {https://arxiv.org/pdf/1502.03671}
}

@Article{LeCun1989,
  Title                    = {Backpropagation applied to handwritten zip code recognition},
  Author                   = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
  Journal                  = {Neural computation},
  Year                     = {1989},
  Number                   = {4},
  Pages                    = {541--551},
  Volume                   = {1},

  Owner                    = {manoj},
  Publisher                = {MIT Press},
  Timestamp                = {2020.08.20}
}

@Article{lei2018tvqa,
  Title                    = {Tvqa: Localized, compositional video question answering},
  Author                   = {Lei, Jie and Yu, Licheng and Bansal, Mohit and Berg, Tamara L},
  Journal                  = {arXiv preprint arXiv:1809.01696},
  Year                     = {2018},

  Abstract                 = {Recent years have witnessed an increasing interest in image-based question-answering (QA) tasks. However, due to data limitations, there has been much less work on video-based QA. In this paper, we present TVQA, a large-scale video QA dataset based on 6 popular TV shows. TVQA consists of 152,545 QA pairs from 21,793 clips, spanning over 460 hours of video. Questions are designed to be compositional in nature, requiring systems to jointly localize relevant moments within a clip, comprehend subtitle-based dialogue, and recognize relevant visual concepts. We provide analyses of this new dataset as well as several baselines and a multi-stream end-to-end trainable neural network framework for the TVQA task},
  Comment                  = {Video question answering. Citations - 87.},
  Url                      = {https://arxiv.org/pdf/1809.01696}
}

@Article{li2015diversity,
  Title                    = {A diversity-promoting objective function for neural conversation models},
  Author                   = {Li, Jiwei and Galley, Michel and Brockett, Chris and Gao, Jianfeng and Dolan, Bill},
  Journal                  = {arXiv preprint arXiv:1510.03055},
  Year                     = {2015},

  Abstract                 = {Sequence-to-sequence neural network models for generation of conversational responses tend to generate safe, commonplace responses (e.g., "I don't know") regardless of the input. We suggest that the traditional objective function, i.e., the likelihood of output (response) given input (message) is unsuited to response generation tasks. Instead we propose using Maximum Mutual Information (MMI) as the objective function in neural models. Experimental results demonstrate that the proposed MMI models produce more diverse, interesting, and appropriate responses, yielding substantive gains in BLEU scores on two conversational datasets and in human evaluations.},
  Comment                  = {Conversation models. CItations - 857},
  Url                      = {https://arxiv.org/pdf/1510.03055.pdf)}
}

@InProceedings{liang2015recurrent,
  Title                    = {Recurrent convolutional neural network for object recognition},
  Author                   = {Liang, Ming and Hu, Xiaolin},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2015},
  Pages                    = {3367--3375},

  Abstract                 = {In recent years, the convolutional neural network (CNN) has achieved great success in many computer vision tasks. Partially inspired by neuroscience, CNN shares many properties with the visual system of the brain. A prominent difference is that CNN is typically a feed-forward architecture while in the visual system recurrent connections are abundant. Inspired by this fact, we propose a recurrent CNN (RCNN) for object recognition by incorporating recurrent connections into each convolutional layer. Though the input is static, the activities of RCNN units evolve over time so that the activity of each unit is modulated by the activities of its neighboring units. This property enhances the ability of the model to integrate the context information, which is important for object recognition. Like other recurrent neural networks, unfolding the RCNN through time can result in an arbitrarily deep network with a fixed number of parameters. Furthermore, the unfolded network has multiple paths, which can facilitate the learning process. The model is tested on four benchmark object recognition datasets: CIFAR-10, CIFAR-100, MNIST and SVHN. With fewer trainable parameters, RCNN outperforms the state-of-the-art models on all of these datasets. Increasing the number of parameters leads to even better performance. These results demonstrate the advantage of the recurrent structure over purely feed-forward structure for object recognition.},
  Comment                  = {Recurrent CNN. Citations - 678.},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2015/papers/Liang_Recurrent_Convolutional_Neural_2015_CVPR_paper.pdf}
}

@InProceedings{lin2017refinenet,
  Title                    = {Refinenet: Multi-path refinement networks for high-resolution semantic segmentation},
  Author                   = {Lin, Guosheng and Milan, Anton and Shen, Chunhua and Reid, Ian},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2017},
  Pages                    = {1925--1934},

  Abstract                 = {Recently, very deep convolutional neural networks(CNNs) have shown outstanding performance in objectrecognition and have also been the first choice for denseclassification problems such as semantic segmentation.However, repeated subsampling operations like pooling orconvolution striding in deep CNNs lead to a significant de-crease in the initial image resolution. Here, we presentRefineNet, a generic multi-path refinement network thatexplicitly exploits all the information available along thedown-sampling process to enable high-resolution predic-tion using long-range residual connections. In this way,the deeper layers that capture high-level semantic featurescan be directly refined using fine-grained features from ear-lier convolutions. The individual components of RefineNetemploy residual connections following the identity map-ping mindset, which allows for effective end-to-end train-ing. Further, we introduce chained residual pooling, whichcaptures rich background context in an efficient manner. Wecarry out comprehensive experiments and set new state-of-the-art results on seven public datasets. In particular,we achieve an intersection-over-union score of83.4on thechallenging PASCAL VOC 2012 dataset, which is the bestreported result to date.},
  Comment                  = {Semantic segmentation. Citations - 1241.},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Lin_RefineNet_Multi-Path_Refinement_CVPR_2017_paper.pdf}
}

@Article{Lin2016,
  Title                    = {Masked face detection via a modified LeNet},
  Author                   = {Lin, Shaohui and Cai, Ling and Lin, Xianming and Ji, Rongrong},
  Journal                  = {Neurocomputing},
  Year                     = {2016},
  Pages                    = {197--202},
  Volume                   = {218},

  Abstract                 = {Detecting masked faces in the wild has been emerging recently, which has rich applications ranging from violence video retrieval to video surveillance. Its accurate detection retains as an open problem, mainly due to the difficulties of low-resolution and arbitrary viewing angles, as well as the limitation of collecting sufficient amount of training samples. Such difficulties have been significantly challenged the design of effective handcraft features as well as robust detectors. In this paper, we tackle these problems by proposing a learn-based feature design and classifier training paradigm. More particularly, a modified LeNet, termed MLeNet, is presented, which modifies the number of units in output layer of LeNet to suit for a specific classification. Meanwhile, MLeNet further increases the number of feature maps with smaller filter size. To further reduce overfitting and improve the performance with a small quantity of training samples, we firstly increase the training dataset by horizontal reflection and then learn MLeNet via combining both pre-training and fine-tuning. We evaluate the proposed model on a real-world masked face detection dataset. Quantitative evaluations over several state-of-the-arts and alternative solution have demonstrated the accuracy and robustness of the proposed model.},
  Comment                  = {Citation - 12},
  Owner                    = {manoj},
  Publisher                = {Elsevier},
  Timestamp                = {2020.08.20},
  Url                      = {https://www.sciencedirect.com/science/article/abs/pii/S0925231216309523}
}

@InProceedings{Lin2014,
  Title                    = {Microsoft coco: Common objects in context},
  Author                   = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  Booktitle                = {European conference on computer vision},
  Year                     = {2014},
  Organization             = {Springer},
  Pages                    = {740--755},

  Abstract                 = {We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed
statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model.},
  Comment                  = {COCO dataset. CItations - 10987},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://link.springer.com/content/pdf/10.1007/978-3-319-10602-1_48.pdf}
}

@Article{liu2016attention,
  Title                    = {Attention correctness in neural image captioning},
  Author                   = {Liu, Chenxi and Mao, Junhua and Sha, Fei and Yuille, Alan},
  Journal                  = {arXiv preprint arXiv:1605.09553},
  Year                     = {2016},

  Abstract                 = {Attention mechanisms have recently been introduced in deep learning for various tasks in natural language processing and computer vision. But despite their popularity, the "correctness" of the implicitly-learned attention maps has only been assessed qualitatively by visualization of several examples. In this paper we focus on evaluating and improving the correctness of attention in neural image captioning models. Specifically, we propose a quantitative evaluation metric for the consistency between the generated attention maps and human annotations, using recently released datasets with alignment between regions in images and entities in captions. We then propose novel models with different levels of explicit supervision for learning attention maps during training. The supervision can be strong when alignment between regions and caption entities are available, or weak when only object segments and categories are provided. We show on the popular Flickr30k and COCO datasets that introducing supervision of attention maps during training solidly improves both attention correctness and caption quality, showing the promise of making machine perception more human-like.},
  Comment                  = {Image captioning. Citations - 138},
  Url                      = {https://arxiv.org/pdf/1605.09553}
}

@InProceedings{Liu2016,
  Title                    = {Ssd: Single shot multibox detector},
  Author                   = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C},
  Booktitle                = {European conference on computer vision},
  Year                     = {2016},
  Organization             = {Springer},
  Pages                    = {21--37},

  Abstract                 = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300×300 input, SSD achieves 74.3 % mAP on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512×512 input, SSD achieves 76.9 % mAP, outperforming a comparable state of the art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at https://github.com/weiliu89/caffe/tree/ssd.},
  Comment                  = {SSD. Citations - 9937},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://arxiv.org/pdf/1512.02325.pdf%EF%BC%89}
}

@InProceedings{long2015fully,
  Title                    = {Fully convolutional networks for semantic segmentation},
  Author                   = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2015},
  Pages                    = {3431--3440},

  Abstract                 = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixels-to-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efficient inference and learning. We define and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classification networks (AlexNet, the VGG net, and GoogLeNet) into fully convolutional networks and transfer their learned representations by fine-tuning to the segmentation task. We then define a skip architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, fine layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20% relative improvement to 62.2% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one fifth of a second for a typical image.},
  Comment                  = {Semantic segmentations. Citations - 17908},
  Url                      = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf}
}

@Article{lowe2015ubuntu,
  Title                    = {The ubuntu dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems},
  Author                   = {Lowe, Ryan and Pow, Nissan and Serban, Iulian and Pineau, Joelle},
  Journal                  = {arXiv preprint arXiv:1506.08909},
  Year                     = {2015},

  Abstract                 = {This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost 1 million multi-turn dialogues, with a total of over 7 million utterances and 100 million words. This provides a unique resource for research into building dialogue managers based on neural language models that can make use of large amounts of unlabeled data. The dataset has both the multi-turn property of conversations in the Dialog State Tracking Challenge datasets, and the unstructured nature of interactions from microblog services such as Twitter. We also describe two neural learning architectures suitable for analyzing this dataset, and provide benchmark performance on the task of selecting the best next response.},
  Comment                  = {Conversation model. Citations - 537},
  Url                      = {https://arxiv.org/pdf/1506.08909}
}

@InProceedings{lu2017knowing,
  Title                    = {Knowing when to look: Adaptive attention via a visual sentinel for image captioning},
  Author                   = {Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2017},
  Pages                    = {375--383},

  Abstract                 = {Attention-based neural encoder-decoder frameworks have been widely adopted for image captioning. Most methods force visual attention to be active for every generated word. However, the decoder likely requires little to no visual information from the image to predict non-visual words such as "the" and "of". Other words that may seem visual can often be predicted reliably just from the language model e.g., "sign" after "behind a red stop" or "phone" following "talking on a cell". In this paper, we propose a novel adaptive attention model with a visual sentinel. At each time step, our model decides whether to attend to the image (and if so, to which regions) or to the visual sentinel. The model decides whether to attend to the image and where, in order to extract meaningful information for sequential word generation. We test our method on the COCO image captioning 2015 challenge dataset and Flickr30K. Our approach sets the new state-of-the-art by a significant margin.},
  Comment                  = {Image captioning. Citations - 642},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Lu_Knowing_When_to_CVPR_2017_paper.pdf}
}

@Article{luong2015effective,
  Title                    = {Effective approaches to attention-based neural machine translation},
  Author                   = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D},
  Journal                  = {arXiv preprint arXiv:1508.04025},
  Year                     = {2015},

  Url                      = {https://arxiv.org/pdf/1508.04025)}
}

@Article{makhzani2015adversarial,
  Title                    = {Adversarial autoencoders},
  Author                   = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  Journal                  = {arXiv preprint arXiv:1511.05644},
  Year                     = {2015},

  Abstract                 = {In this paper, we propose the "adversarial autoencoder" (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
  Comment                  = {Adversial autoencoders. Citations - 1368.},
  Url                      = {https://arxiv.org/pdf/1511.05644.pdf]}
}

@InProceedings{malinowski2015ask,
  Title                    = {Ask your neurons: A neural-based approach to answering questions about images},
  Author                   = {Malinowski, Mateusz and Rohrbach, Marcus and Fritz, Mario},
  Booktitle                = {Proceedings of the IEEE international conference on computer vision},
  Year                     = {2015},
  Pages                    = {1--9},

  Abstract                 = {We address a question answering task on real-world images that is set up as a Visual Turing Test. By combining latest advances in image representation and natural language processing, we propose Neural-Image-QA, an end-to-end formulation to this problem for which all parts are trained jointly. In contrast to previous efforts, we are facing a multi-modal problem where the language output (answer) is conditioned on visual and natural language input (image and question). Our approach Neural-Image-QA doubles the performance of the previous best approach on this problem. We provide additional insights into the problem by analyzing how much information is contained only in the language part for which we provide a new human baseline. To study human consensus, which is related to the ambiguities inherent in this challenging task, we propose two novel metrics and collect additional answers which extends the original DAQUAR dataset to DAQUAR-Consensus.},
  Comment                  = {Visual question answering. Citations - 483},
  Url                      = {http://openaccess.thecvf.com/content_iccv_2015/papers/Malinowski_Ask_Your_Neurons_ICCV_2015_paper.pdf}
}

@Article{maninis2018video,
  Title                    = {Video object segmentation without temporal information},
  Author                   = {Maninis, K-K and Caelles, Sergi and Chen, Yuhua and Pont-Tuset, Jordi and Leal-Taix{\'e}, Laura and Cremers, Daniel and Van Gool, Luc},
  Journal                  = {IEEE transactions on pattern analysis and machine intelligence},
  Year                     = {2018},
  Number                   = {6},
  Pages                    = {1515--1530},
  Volume                   = {41},

  Abstract                 = {Video Object Segmentation, and video processing in general, has been historically dominated by methods that rely on the temporal consistency and redundancy in consecutive video frames. When the temporal smoothness is suddenly broken, such as when an object is occluded, or some frames are missing in a sequence, the result of these methods can deteriorate significantly. This paper explores the orthogonal approach of processing each frame independently, i.e., disregarding the temporal information. In particular, it tackles the task of semi-supervised video object segmentation: the separation of an object from the background in a video, given its mask in the first frame. We present Semantic One-Shot Video Object Segmentation (OSVOSS), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic information, learned on ImageNet, to the task of foreground segmentation, and finally to learning the appearance of a single annotated object of the test sequence (hence one shot). We show that instance-level semantic information, when combined effectively, can dramatically improve the results of our previous method, OSVOS. We perform experiments on two recent single-object video segmentation databases, which show that OSVOSS is both the fastest and most accurate method in the state of the art. Experiments on multi-object video segmentation show that OSVOSS obtains competitive results.},
  Comment                  = {Segmentation. Citation - 114.},
  Publisher                = {IEEE},
  Url                      = {https://arxiv.org/pdf/1709.06031}
}

@Article{mcculloch1943logical,
  Title                    = {A logical calculus of the ideas immanent in nervous activity},
  Author                   = {McCulloch, Warren S and Pitts, Walter},
  Journal                  = {The bulletin of mathematical biophysics},
  Year                     = {1943},
  Number                   = {4},
  Pages                    = {115--133},
  Volume                   = {5},

  Abstract                 = {Because of the “all-or-none” character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
  Comment                  = {Citations - 19989},
  Publisher                = {Springer},
  Url                      = {http://aiplaybook.a16z.com/reference-material/mcculloch-pitts-1943-neural-networks.pdf}
}

@InProceedings{Mikolov2011,
  Title                    = {Extensions of recurrent neural network language model},
  Author                   = {Mikolov, Tom{\'a}{\v{s}} and Kombrink, Stefan and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and Khudanpur, Sanjeev},
  Booktitle                = {2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  Year                     = {2011},
  Organization             = {IEEE},
  Pages                    = {5528--5531},

  Abstract                 = {We present several modifications of the original recurrent neural net work language model (RNN LM). While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.},
  Comment                  = {Language model. Citations - 4914},
  Url                      = {http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf}
}

@InProceedings{mikolov2011extensions,
  Title                    = {Extensions of recurrent neural network language model},
  Author                   = {Mikolov, Tom{\'a}{\v{s}} and Kombrink, Stefan and Burget, Luk{\'a}{\v{s}} and {\v{C}}ernock{\`y}, Jan and Khudanpur, Sanjeev},
  Booktitle                = {2011 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  Year                     = {2011},
  Organization             = {IEEE},
  Pages                    = {5528--5531},

  Abstract                 = {We present several modifications of the original recurrent neural net work language model (RNN LM). While this model has been shown to significantly outperform many competitive language modeling techniques in terms of accuracy, the remaining problem is the computational complexity. In this work, we show approaches that lead to more than 15 times speedup for both training and testing phases. Next, we show importance of using a backpropagation through time algorithm. An empirical comparison with feedforward networks is also provided. In the end, we discuss possibilities how to reduce the amount of parameters in the model. The resulting RNN model can thus be smaller, faster both during training and testing, and more accurate than the basic one.},
  Comment                  = {Language model. Citations - 4914},
  Url                      = {http://www.fit.vutbr.cz/research/groups/speech/servite/2010/rnnlm_mikolov.pdf}
}

@Book{minsky2017perceptrons,
  Title                    = {Perceptrons: An introduction to computational geometry},
  Author                   = {Minsky, Marvin and Papert, Seymour A},
  Publisher                = {MIT press},
  Year                     = {2017},

  Abstract                 = {The first systematic study of parallelism in computation by two pioneers in the field. Reissue
of the 1988 Expanded Edition with a new foreword by Léon Bottou In 1969, ten years after
the discovery of the perceptron—which showed that a machine could be taught to perform},
  Comment                  = {Perceptron. Citations - 9870.},
  Url                      = {https://books.google.com/books?hl=en&lr=&id=PLQ5DwAAQBAJ&oi=fnd&pg=PR5&dq=minsky perceptrons&ots=zyLGAHno_0&sig=lUsInQ-xgA6OqWEhpzfZo6h32k8}
}

@Article{Moravcik2017,
  Title                    = {Deepstack: Expert-level artificial intelligence in heads-up no-limit poker},
  Author                   = {Morav{\v{c}}{\'\i}k, Matej and Schmid, Martin and Burch, Neil and Lis{\`y}, Viliam and Morrill, Dustin and Bard, Nolan and Davis, Trevor and Waugh, Kevin and Johanson, Michael and Bowling, Michael},
  Journal                  = {Science},
  Year                     = {2017},
  Number                   = {6337},
  Pages                    = {508--513},
  Volume                   = {356},

  Abstract                 = {Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker is the quintessential game of imperfect information, and a longstanding challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated with statistical significance professional poker players in heads-up no-limit Texas hold’em. The approach is
theoretically sound and is shown to produce more difficult to exploit strategies than prior approaches.},
  Comment                  = {Deepstack poker game. Citation - 424},
  Owner                    = {manoj},
  Publisher                = {American Association for the Advancement of Science},
  Timestamp                = {2020.08.21},
  Url                      = {https://arxiv.org/pdf/1701.01724}
}

@InProceedings{nguyen2017plug,
  Title                    = {Plug \& play generative networks: Conditional iterative generation of images in latent space},
  Author                   = {Nguyen, Anh and Clune, Jeff and Bengio, Yoshua and Dosovitskiy, Alexey and Yosinski, Jason},
  Booktitle                = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2017},
  Pages                    = {4467--4477},

  Abstract                 = {Generating high-resolution, photo-realistic images has been a long-standing goal in machine learning. Recently, Nguyen et al. 2016 showed one interesting way to synthesize novel images by performing gradient descent in the latent space of a generator network to maximize the activations of one or multiple neurons in a separate classifier network. In this paper we extend this method by introducing an additional prior on the latent code, improving both sample quality and sample diversity, leading to a state-of-the-art generative model that produces high quality images at higher resolutions (227x227) than previous generative models, and does so for all 1000 ImageNet categories. In addition, we provide a unified probabilistic interpretation of related activation maximization methods and call the general class of models "Plug and Play Generative Networks". PPGNs are composed of (1) a generator network G that is capable of drawing a wide range of image types and (2) a replaceable "condition" network C that tells the generator what to draw. We demonstrate generation of images conditioned on a class - when C is an ImageNet classification network - and also conditioned on a caption - when C is an image captioning network. Our method also improves the state of the art of Deep Multifaceted Feature Visualization, which involves synthetically generating the set of inputs that activate a neuron in order to better understand how deep neural networks operate. Finally, we show that our model performs reasonably well at the task of image inpainting. While we operate on images in this paper, the approach is modality agnostic and can be applied to many types of data.},
  Comment                  = {Generative networks. Citations - 404.},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Nguyen_Plug__Play_CVPR_2017_paper.pdf}
}

@InProceedings{noh2016image,
  Title                    = {Image question answering using convolutional neural network with dynamic parameter prediction},
  Author                   = {Noh, Hyeonwoo and Hongsuck Seo, Paul and Han, Bohyung},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2016},
  Pages                    = {30--38},

  Abstract                 = {We tackle image question answering (ImageQA) problem by learning a convolutional neural network (CNN) with a dynamic parameter layer whose weights are determined adaptively based on questions. For the adaptive parameter prediction, we employ a separate parameter prediction network, which consists of gated recurrent unit (GRU) taking a question as its input and a fully-connected layer generating a set of candidate weights as its output. However, it is challenging to construct a parameter prediction network for a large number of parameters in the fully-connected dynamic parameter layer of the CNN. We reduce the complexity of this problem by incorporating a hashing technique, where the candidate weights given by the parameter prediction network are selected using a predefined hash function to determine individual weights in the dynamic parameter layer. The proposed network---joint network with the CNN for ImageQA and the parameter prediction network---is trained end-to-end through back-propagation, where its weights are initialized using a pre-trained CNN and GRU. The proposed algorithm illustrates the state-of-the-art performance on all available public ImageQA benchmarks.},
  Comment                  = {Question answering. Citations - 252},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2016/papers/Noh_Image_Question_Answering_CVPR_2016_paper.pdf}
}

@InProceedings{Oh2015,
  Title                    = {Action-conditional video prediction using deep networks in atari games},
  Author                   = {Oh, Junhyuk and Guo, Xiaoxiao and Lee, Honglak and Lewis, Richard L and Singh, Satinder},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2015},
  Pages                    = {2863--2871},

  Abstract                 = {Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future image-frames depend on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, actionconditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.},
  Comment                  = {Atari games by deep neural network. Citation -},
  Owner                    = {manoj},
  Timestamp                = {2020.08.21},
  Url                      = {http://papers.nips.cc/paper/5859-action-conditional-video-prediction-using-deep-networks-in-atari-games.pdf}
}

@InProceedings{oord2018parallel,
  Title                    = {Parallel wavenet: Fast high-fidelity speech synthesis},
  Author                   = {Oord, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and Driessche, George and Lockhart, Edward and Cobo, Luis and Stimberg, Florian and others},
  Booktitle                = {International conference on machine learning},
  Year                     = {2018},
  Organization             = {PMLR},
  Pages                    = {3918--3926},

  Abstract                 = {The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today’s massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, a 1000x speed up relative to the original WaveNet, and capable of serving multiple English and Japanese voices in a production setting.},
  Comment                  = {Parallel Wavenets. Citations - 328.},
  Url                      = {http://proceedings.mlr.press/v80/oord18a/oord18a.pdf}
}

@Article{oord2016wavenet,
  Title                    = {Wavenet: A generative model for raw audio},
  Author                   = {Oord, Aaron van den and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  Journal                  = {arXiv preprint arXiv:1609.03499},
  Year                     = {2016},

  Abstract                 = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  Comment                  = {WaveNet. Citations - 2090},
  Url                      = {https://arxiv.org/pdf/1609.03499.pdf?utm_source=Sailthru&utm_medium=email&utm_campaign=Uncubed Entry #61 - April 3, 2019&utm_term=entry}
}

@InProceedings{Pan2016,
  Title                    = {Hierarchical recurrent neural encoder for video representation with application to captioning},
  Author                   = {Pan, Pingbo and Xu, Zhongwen and Yang, Yi and Wu, Fei and Zhuang, Yueting},
  Booktitle                = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2016},
  Pages                    = {1029--1038},

  Abstract                 = {Recently, deep learning approach, especially deep Con-volutional Neural Networks (ConvNets), have achievedoverwhelming accuracy with fast processing speed for im-age classification. Incorporating temporal structure withdeep ConvNets for video representation becomes a funda-mental problem for video content analysis. In this paper,we propose a new approach, namely Hierarchical RecurrentNeural Encoder (HRNE), to exploit temporal information ofvideos. Compared to recent video representation inferenceapproaches, this paper makes the following three contribu-tions. First, our HRNE is able to efficiently exploit videotemporal structure in a longer range by reducing the lengthof input information flow, and compositing multiple consec-utive inputs at a higher level. Second, computation oper-ations are significantly lessened while attaining more non-linearity. Third, HRNE is able to uncover temporal tran-sitions between frame chunks with different granularities,i.e. it can model the temporal transitions between framesas well as the transitions between segments. We apply thenew method to video captioning where temporal informa-tion plays a crucial role. Experiments demonstrate that ourmethod outperforms the state-of-the-art on video captioningbenchmarks.},
  Comment                  = {Video captioning. CItations - 293},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper.pdf}
}

@InProceedings{pan2016hierarchical,
  Title                    = {Hierarchical recurrent neural encoder for video representation with application to captioning},
  Author                   = {Pan, Pingbo and Xu, Zhongwen and Yang, Yi and Wu, Fei and Zhuang, Yueting},
  Booktitle                = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2016},
  Pages                    = {1029--1038},

  Abstract                 = {Recently, deep learning approach, especially deep Convolutional Neural Networks (ConvNets), have achieved overwhelming accuracy with fast processing speed for image classification. Incorporating temporal structure with deep ConvNets for video representation becomes a fundamental problem for video content analysis. In this paper, we propose a new approach, namely Hierarchical Recurrent Neural Encoder (HRNE), to exploit temporal information of videos. Compared to recent video representation inference approaches, this paper makes the following three contributions. First, our HRNE is able to efficiently exploit video temporal structure in a longer range by reducing the length of input information flow, and compositing multiple consecutive inputs at a higher level. Second, computation operations are significantly lessened while attaining more non-linearity. Third, HRNE is able to uncover temporal transitions between frame chunks with different granularities, i.e. it can model the temporal transitions between frames as well as the transitions between segments. We apply the new method to video captioning where temporal information plays a crucial role. Experiments demonstrate that our method outperforms the state-of-the-art on video captioning benchmarks.},
  Comment                  = {Video captioning. Citations - 293.},
  Url                      = {@inproceedings{pan2016hierarchical,
 title={Hierarchical recurrent neural encoder for video representation with application to captioning},
 author={Pan, Pingbo and Xu, Zhongwen and Yang, Yi and Wu, Fei and Zhuang, Yueting},
 booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
 pages={1029--1038},
 year={2016}
}}
}

@InProceedings{pan2017video,
  Title                    = {Video captioning with transferred semantic attributes},
  Author                   = {Pan, Yingwei and Yao, Ting and Li, Houqiang and Mei, Tao},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2017},
  Pages                    = {6504--6512},

  Abstract                 = {Automatically generating natural language descriptions of videos plays a fundamental challenge for computer vision community. Most recent progress in this problem has been achieved through employing 2-D and/or 3-D Convolutional Neural Networks (CNNs) to encode video content and Recurrent Neural Networks (RNNs) to decode a sentence. In this paper, we present Long Short-Term Memory with Transferred Semantic Attributes (LSTM-TSA)---a novel deep architecture that incorporates the transferred semantic attributes learnt from images and videos into the CNN plus RNN framework, by training them in an end-to-end manner. The design of LSTM-TSA is highly inspired by the facts that 1) semantic attributes play a significant contribution to captioning, and 2) images and videos carry complementary semantics and thus can reinforce each other for captioning. To boost video captioning, we propose a novel transfer unit to model the mutually correlated attributes learnt from images and videos. Extensive experiments are conducted on three public datasets, i.e., MSVD, M-VAD and MPII-MD. Our proposed LSTM-TSA achieves to-date the best published performance in sentence generation on MSVD: 52.8% and 74.0% in terms of BLEU@4 and CIDEr-D. Superior results are also reported on M-VAD and MPII-MD when compared to state-of-the-art methods.},
  Comment                  = {Video captioning. CItations - 183},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Pan_Video_Captioning_With_CVPR_2017_paper.pdf}
}

@Article{panda2017multi,
  Title                    = {Multi-view surveillance video summarization via joint embedding and sparse optimization},
  Author                   = {Panda, Rameswar and Roy-Chowdhury, Amit K},
  Journal                  = {IEEE Transactions on Multimedia},
  Year                     = {2017},
  Number                   = {9},
  Pages                    = {2010--2021},
  Volume                   = {19},

  Abstract                 = {Most traditional video summarization methods are designed to generate effective summaries for single-view videos, and thus, they cannot fully exploit the complicated intra- and inter-view correlations in summarizing multi-view videos in a camera network. In this paper, with the aim of summarizing multi-view videos, we introduce a novel unsupervised framework via joint embedding and sparse representative selection. The objective function is twofold. The first is to capture the multiview correlations via an embedding, which helps in extracting a diverse set of representatives. The second is to use a ℓ 2,1 -norm to model the sparsity while selecting representative shots for the summary. We propose to jointly optimize both of the objectives, such that embedding cannot only characterize the correlations, but also indicate the requirements of sparse representative selection. We present an efficient alternating algorithm based on half-quadratic minimization to solve the proposed non-smooth and non-convex objective with convergence analysis. A key advantage of the proposed approach with respect to the state-of-the-art is that it can summarize multi-view videos without assuming any prior correspondences/alignment between them, e.g., uncalibrated camera networks. Rigorous experiments on several multi-view datasets demonstrate that our approach clearly outperforms the state-of-the-art methods.},
  Comment                  = {Video summarization. Citation - 33.},
  Publisher                = {IEEE},
  Url                      = {https://arxiv.org/pdf/1706.03121}
}

@Article{paszke2016enet,
  Title                    = {Enet: A deep neural network architecture for real-time semantic segmentation},
  Author                   = {Paszke, Adam and Chaurasia, Abhishek and Kim, Sangpil and Culurciello, Eugenio},
  Journal                  = {arXiv preprint arXiv:1606.02147},
  Year                     = {2016},

  Abstract                 = {The ability to perform pixel-wise semantic segmentation in real-time is of paramount importance in mobile applications. Recent deep neural networks aimed at this task have the disadvantage of requiring a large number of floating point operations and have long run-times that hinder their usability. In this paper, we propose a novel deep neural network architecture named ENet (efficient neural network), created specifically for tasks requiring low latency operation. ENet is up to 18× faster, requires 75× less FLOPs, has 79× less parameters, and provides similar or better accuracy to existing models. We have tested it on CamVid, Cityscapes and SUN datasets and report on comparisons with existing state-of-the-art methods, and the trade-offs between accuracy and processing time of a network. We present performance measurements of the proposed architecture on embedded systems and suggest possible software improvements that could make ENet even faster.},
  Comment                  = {Semantic segmentations. Citations - 761},
  Url                      = {https://arxiv.org/pdf/1606.02147.pdf)ì €}
}

@Article{perez2017effectiveness,
  Title                    = {The effectiveness of data augmentation in image classification using deep learning},
  Author                   = {Perez, Luis and Wang, Jason},
  Journal                  = {arXiv preprint arXiv:1712.04621},
  Year                     = {2017},

  Abstract                 = {n this paper, we explore and compare multiple solutionsto the problem of data augmentation in image classification.Previous work has demonstrated the effectiveness of dataaugmentation through simple techniques, such as cropping,rotating, and flipping input images. We artificially con-strain our access to data to a small subset of the ImageNetdataset, and compare each data augmentation technique inturn. One of the more successful data augmentations strate-gies is the traditional transformations mentioned above. Wealso experiment with GANs to generate images of differentstyles. Finally, we propose a method to allow a neural net tolearn augmentations that best improve the classifier, whichwe call neural augmentation. We discuss the successes andshortcomings of this method on various datasets},
  Comment                  = {Image classification using DNN. Citations - 725.},
  Url                      = {https://arxiv.org/pdf/1712.04621.pdf?source=post_page---------------------------}
}

@InProceedings{Redmon2016,
  Title                    = {You only look once: Unified, real-time object detection},
  Author                   = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2016},
  Pages                    = {779--788},

  Abstract                 = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is less likely to predict false positives on background. Finally, YOLO learns very general representations of objects. It outperforms other detection methods, including DPM and R-CNN, when generalizing from natural images to other domains like artwork.},
  Comment                  = {YOLO. CItations - 10673},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Redmon_You_Only_Look_CVPR_2016_paper.pdf}
}

@InProceedings{redmon2017yolo9000,
  Title                    = {YOLO9000: better, faster, stronger},
  Author                   = {Redmon, Joseph and Farhadi, Ali},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2017},
  Pages                    = {7263--7271},

  Abstract                 = {We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that don't have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.},
  Comment                  = {YOLO9000. Citations - 5497},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Redmon_YOLO9000_Better_Faster_CVPR_2017_paper.pdf}
}

@InProceedings{ren2015faster,
  Title                    = {Faster r-cnn: Towards real-time object detection with region proposal networks},
  Author                   = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2015},
  Pages                    = {91--99},

  Abstract                 = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2% mAP) and 2012 (70.4% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster_rcnn.},
  Comment                  = {Faster RCNN. Citations - 21399.},
  Url                      = {http://papers.nips.cc/paper/5638-faster-r-cnn-towards-real-time-object-detection-with-region-proposal-networks.pdf}
}

@InProceedings{rennie2017self,
  Title                    = {Self-critical sequence training for image captioning},
  Author                   = {Rennie, Steven J and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  Booktitle                = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2017},
  Pages                    = {7008--7024},

  Abstract                 = {Recently it has been shown that policy-gradient methods for reinforcement learning can be utilized to train deep end-to-end systems directly on non-differentiable metrics for the task at hand. In this paper we consider the problem of optimizing image captioning systems using reinforcement learning, and show that by carefully optimizing our systems using the test metrics of the MSCOCO task, significant gains in performance can be realized. Our systems are built using a new optimization approach that we call self-critical sequence training (SCST). SCST is a form of the popular REINFORCE algorithm that, rather than estimating a "baseline" to normalize the rewards and reduce variance, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences. Using this approach, estimating the reward signal (as actor-critic methods must do) and estimating normalization (as REINFORCE algorithms typically do) is avoided, while at the same time harmonizing the model with respect to its test-time inference procedure. Empirically we find that directly optimizing the CIDEr metric with SCST and greedy decoding at test-time is highly effective. Our results on the MSCOCO evaluation sever establish a new state-of-the-art on the task, improving the best result in terms of CIDEr from 104.9 to 114.7.},
  Comment                  = {Image captioning. Citations - 671},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2017/papers/Rennie_Self-Critical_Sequence_Training_CVPR_2017_paper.pdf}
}

@InProceedings{Ronneberger2015,
  Title                    = {U-net: Convolutional networks for biomedical image segmentation},
  Author                   = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  Booktitle                = {International Conference on Medical image computing and computer-assisted intervention},
  Year                     = {2015},
  Organization             = {Springer},
  Pages                    = {234--241},

  Abstract                 = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  Comment                  = {U-net. Citations - 16950},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://arxiv.org/pdf/1505.04597.pdf)%E5%92%8C[Tiramisu](https://arxiv.org/abs/1611.09326}
}

@TechReport{rumelhart1985learning,
  Title                    = {Learning internal representations by error propagation},
  Author                   = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
  Institution              = {California Univ San Diego La Jolla Inst for Cognitive Science},
  Year                     = {1985},

  Comment                  = {Internal representation by error propagation. Citations - 26607.},
  Url                      = {https://apps.dtic.mil/dtic/tr/fulltext/u2/a164453.pdf}
}

@Article{sak2014long,
  Title                    = {Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition},
  Author                   = {Sak, Ha{\c{s}}im and Senior, Andrew and Beaufays, Fran{\c{c}}oise},
  Journal                  = {arXiv preprint arXiv:1402.1128},
  Year                     = {2014},

  Abstract                 = {Long Short-Term Memory (LSTM) is a recurrent neural network
(RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike
feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models. 1},
  Comment                  = {Speech reocgnition. Citations - 558},
  Url                      = {https://arxiv.org/pdf/1402.1128}
}

@InProceedings{salakhutdinov2009deep,
  Title                    = {Deep boltzmann machines},
  Author                   = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
  Booktitle                = {Artificial intelligence and statistics},
  Year                     = {2009},
  Pages                    = {448--455},

  Abstract                 = {We present a new learning algorithm for Boltz-mann machines that contain many layers of hid-den variables. Data-dependent expectations areestimated using a variational approximation thattends to focus on a single mode, and data-independent expectations are approximated us-ing persistent Markov chains. The use of twoquite different techniques for estimating the twotypes of expectation that enter into the gradientof the log-likelihood makes it practical to learnBoltzmann machines with multiple hidden lay-ers and millions of parameters. The learning canbe made more efficient by using a layer-by-layer“pre-training” phase that allows variational in-ference to be initialized with a single bottom-up pass. We present results on the MNIST andNORB datasets showing that deep Boltzmannmachines learn good generative models and per-form well on handwritten digit and visual objectrecognition tasks.},
  Comment                  = {Deep boltzman machine. Citations - 2147.},
  Url                      = {http://www.jmlr.org/proceedings/papers/v5/salakhutdinov09a/salakhutdinov09a.pdf}
}

@InProceedings{santoro2017simple,
  Title                    = {A simple neural network module for relational reasoning},
  Author                   = {Santoro, Adam and Raposo, David and Barrett, David G and Malinowski, Mateusz and Pascanu, Razvan and Battaglia, Peter and Lillicrap, Timothy},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2017},
  Pages                    = {4967--4976},

  Abstract                 = {Relational reasoning is a central component of generally intelligent behavior, buthas proven difficult for neural networks to learn. In this paper we describe how touse Relation Networks (RNs) as a simple plug-and-play module to solve problemsthat fundamentally hinge on relational reasoning. We tested RN-augmented net-works on three tasks: visual question answering using a challenging dataset calledCLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoningabout dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a generalcapacity to solve relational questions, but can gain this capacity when augmentedwith RNs. Thus, by simply augmenting convolutions, LSTMs, and MLPs withRNs, we can remove computational burden from network components that arenot well-suited to handle relational reasoning, reduce overall network complexity,and gain a general ability to reason about the relations between entities and theirproperties.},
  Comment                  = {Visual question answering. Citations - 774},
  Url                      = {https://papers.nips.cc/paper/7082-a-simple-neural-network-module-for-relational-reasoning.pdf}
}

@InProceedings{seide2011conversational,
  Title                    = {Conversational speech transcription using context-dependent deep neural networks},
  Author                   = {Seide, Frank and Li, Gang and Yu, Dong},
  Booktitle                = {Twelfth annual conference of the international speech communication association},
  Year                     = {2011},

  Abstract                 = {We apply the recently proposed Context-Dependent Deep-Neural-Network HMMs, or CD-DNN-HMMs, to speech-to-texttranscription. For single-pass speaker-independent recognitionon the RT03S Fisher portion of phone-call transcription bench-mark (Switchboard), the word-error rate is reduced from 27.4%,obtained by discriminatively trained Gaussian-mixture HMMs,to 18.5%—a 33% relative improvement.CD-DNN-HMMs combine classic artificial-neural-networkHMMs with traditional tied-state triphones and deep-belief-network pre-training. They had previously been shown to re-duce errors by 16% relatively when trained on tens of hours ofdata using hundreds of tied states. This paper takes CD-DNN-HMMs further and applies them to transcription using over 300hours of training data, over 9000 tied states, and up to 9 hiddenlayers, and demonstrates how sparseness can be exploited.On four less well-matched transcription tasks, we observerelative error reductions of 22–28%.},
  Comment                  = {DNN. Citations - 919.},
  Url                      = {https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/CD-DNN-HMM-SWB-Interspeech2011-Pub.pdf}
}

@Article{sennrich2017nematus,
  Title                    = {Nematus: a toolkit for neural machine translation},
  Author                   = {Sennrich, Rico and Firat, Orhan and Cho, Kyunghyun and Birch, Alexandra and Haddow, Barry and Hitschler, Julian and Junczys-Dowmunt, Marcin and L{\"a}ubli, Samuel and Barone, Antonio Valerio Miceli and Mokry, Jozef and others},
  Journal                  = {arXiv preprint arXiv:1703.04357},
  Year                     = {2017},

  Abstract                 = {We present Nematus, a toolkit for Neural Machine Translation. The toolkit prioritizes high translation accuracy, usability, and extensibility. Nematus has been used to build top-performing submissions to shared translation tasks at WMT and IWSLT, and has been used to train systems for production environments.},
  Comment                  = {Machine translation. Citations - 316},
  Url                      = {https://arxiv.org/pdf/1703.04357}
}

@Article{sennrich2015neural,
  Title                    = {Neural machine translation of rare words with subword units},
  Author                   = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  Journal                  = {arXiv preprint arXiv:1508.07909},
  Year                     = {2015},

  Abstract                 = {Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words,
for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and
loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation
techniques, including simple character n -gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks
English →German and English →Russian by up to 1.1 and 1.3 BLEU, respectively},
  Comment                  = {Machine translations. Citations - 2667},
  Url                      = {https://arxiv.org/pdf/1508.07909.pdf)}
}

@Article{shang2015neural,
  Title                    = {Neural responding machine for short-text conversation},
  Author                   = {Shang, Lifeng and Lu, Zhengdong and Li, Hang},
  Journal                  = {arXiv preprint arXiv:1503.02364},
  Year                     = {2015},

  Abstract                 = {We propose Neural Responding Machine (NRM), a neural network-based response generator for Short-Text Conversation. NRM takes the general encoder-decoder framework: it formalizes the
generation of response as a decoding process based on the latent representation of the input text, while both encoding and decoding are realized with recurrent neural networks (RNN). The NRM is trained with a large amount of one-round conversation data collected from a microblogging service. Empirical study shows that NRM can generate grammatically correct and content-wise appropriate responses to over 75% of the input text, outperforming state-of-the-arts in the same setting, including retrieval-based and SMT-based models.},
  Comment                  = {Conversation modelling. Citations - 788},
  Url                      = {https://arxiv.org/pdf/1503.02364}
}

@InProceedings{shen2018natural,
  Title                    = {Natural tts synthesis by conditioning wavenet on mel spectrogram predictions},
  Author                   = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and others},
  Booktitle                = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  Year                     = {2018},
  Organization             = {IEEE},
  Pages                    = {4779--4783},

  Abstract                 = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize time-domain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of 4.53 comparable to a MOS of 4.58 for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the conditioning input to WaveNet instead of linguistic, duration, and F0 features. We further show that using this compact acoustic intermediate representation allows for a significant reduction in the size of the WaveNet architecture.},
  Comment                  = {Wavenet. Citations - 614.},
  Url                      = {https://arxiv.org/pdf/1712.05884.pdf）-A}
}

@Article{shen2015minimum,
  Title                    = {Minimum risk training for neural machine translation},
  Author                   = {Shen, Shiqi and Cheng, Yong and He, Zhongjun and He, Wei and Wu, Hua and Sun, Maosong and Liu, Yang},
  Journal                  = {arXiv preprint arXiv:1512.02433},
  Year                     = {2015},

  Abstract                 = {We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks.},
  Comment                  = {Machine translations. Citations - 303},
  Url                      = {https://arxiv.org/pdf/1512.02433}
}

@Article{Silver2017,
  Title                    = {Mastering the game of go without human knowledge},
  Author                   = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and others},
  Journal                  = {nature},
  Year                     = {2017},
  Number                   = {7676},
  Pages                    = {354--359},
  Volume                   = {550},

  Abstract                 = {A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from selfplay. Here, we introduce an algorithm based solely on reinforcement learning, without human data, guidance, or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo’s own move selections and also the winner of AlphaGo’s games. This neural network improves the strength of tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo.},
  Comment                  = {Go game. CItation - 3950},
  Owner                    = {manoj},
  Publisher                = {Nature Publishing Group},
  Timestamp                = {2020.08.21},
  Url                      = {https://discovery.ucl.ac.uk/id/eprint/10045895/1/agz_unformatted_nature.pdf}
}

@InProceedings{sutskever2014sequence,
  Title                    = {Sequence to sequence learning with neural networks},
  Author                   = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  Booktitle                = {Advances in neural information processing systems},
  Year                     = {2014},
  Pages                    = {3104--3112},

  Abstract                 = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM’s BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its
BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but
not target sentences) improved the LSTM’s performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
  Comment                  = {Machine translation. Citations - 12084},
  Url                      = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf}
}

@InProceedings{Szegedy2015,
  Title                    = {Going deeper with convolutions},
  Author                   = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2015},
  Pages                    = {1--9},

  Abstract                 = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little
work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of
5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble
model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.
1},
  Comment                  = {Machine translations. Citations - 4344},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://arxiv.org/pdf/1508.04025)}
}

@Article{Szegedy2013,
  Title                    = {Intriguing properties of neural networks},
  Author                   = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  Journal                  = {arXiv preprint arXiv:1312.6199},
  Year                     = {2013},

  Abstract                 = {Deep neural networks are highly expressive models that have recently achieved state of the art performance on speech and visual recognition tasks. While their expressiveness is the reason they succeed, it also causes them to learn uninterpretable solutions that could have counter-intuitive properties. In this paper we report two such properties. First, we find that there is no distinction between individual high level units and random linear combinations of high level units, according to various methods of unit analysis. It suggests that it is the space, rather than the individual units, that contains the semantic information in the high layers of neural networks. Second, we find that deep neural networks learn input-output mappings that are fairly discontinuous to a significant extent. We can cause the network to misclassify an image by applying a certain hardly perceptible perturbation, which is found by maximizing the network’s prediction error. In addition, the specific nature of these perturbations is not a random artifact of learning: the same perturbation can cause a different network, that was trained on a different subset of the dataset, to misclassify the same input.},
  Comment                  = {Properties of neural net. Citations - 5096},
  Owner                    = {manoj},
  Timestamp                = {2020.08.26},
  Url                      = {https://arxiv.org/pdf/1312.6199.pdf?source=post_page---------------------------}
}

@InProceedings{tapaswi2016movieqa,
  Title                    = {Movieqa: Understanding stories in movies through question-answering},
  Author                   = {Tapaswi, Makarand and Zhu, Yukun and Stiefelhagen, Rainer and Torralba, Antonio and Urtasun, Raquel and Fidler, Sanja},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2016},
  Pages                    = {4631--4640},

  Abstract                 = {We introduce the MovieQA dataset which aims to eval-uate automatic story comprehension from both video andtext. The dataset consists of 14,944 questions about 408movies with high semantic diversity. The questions rangefrom simpler “Who” did “What” to “Whom”, to “Why”and “How” certain events occurred. Each question comeswith a set of five possible answers; a correct one and fourdeceiving answers provided by human annotators. Ourdataset is unique in that it contains multiple sources ofinformation – video clips, plots, subtitles, scripts, andDVS [32]. We analyze our data through various statisticsand methods. We further extend existing QA techniquesto show that question-answering with such open-ended se-mantics is hard. We make this data set public along with anevaluation benchmark to encourage inspiring work in thischallenging domain.},
  Comment                  = {Video question answering. Citations - 319},
  Url                      = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Tapaswi_MovieQA_Understanding_Stories_CVPR_2016_paper.pdf}
}

@InProceedings{teney2018tips,
  Title                    = {Tips and tricks for visual question answering: Learnings from the 2017 challenge},
  Author                   = {Teney, Damien and Anderson, Peter and He, Xiaodong and Van Den Hengel, Anton},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2018},
  Pages                    = {4223--4232},

  Abstract                 = {Deep Learning has had a transformative impact on Com-puter Vision, but for all of the success there is also a signif-icant cost. This is that the models and procedures used areso complex and intertwined that it is often impossible to dis-tinguish the impact of the individual design and engineer-ing choices each model embodies. This ambiguity divertsprogress in the field, and leads to a situation where devel-oping a state-of-the-art model is as much an art as a sci-ence. As a step towards addressing this problem we presenta massive exploration of the effects of the myriad architec-tural and hyperparameter choices that must be made in gen-erating a state-of-the-art model. The model is of particularinterest because it won the 2017 Visual Question AnsweringChallenge. We provide a detailed analysis of the impact ofeach choice on model performance, in the hope that it willinform others in developing models, but also that it mightset a precedent that will accelerate scientific progress in thefield.},
  Comment                  = {Visual question answering. Citations - 186.},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2018/papers/Teney_Tips_and_Tricks_CVPR_2018_paper.pdf}
}

@Article{van2016heiga,
  Title                    = {Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves, Nal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. Wavenet: A generative model for raw audio},
  Author                   = {Van Den Oord, Aaron and Dieleman, Sander},
  Journal                  = {arXiv preprint arXiv:1609.03499},
  Year                     = {2016},
  Number                   = {3},
  Volume                   = {2}
}

@Article{vendrov2015order,
  Title                    = {Order-embeddings of images and language},
  Author                   = {Vendrov, Ivan and Kiros, Ryan and Fidler, Sanja and Urtasun, Raquel},
  Journal                  = {arXiv preprint arXiv:1511.06361},
  Year                     = {2015},

  Abstract                 = {Hypernymy, textual entailment, and image captioning can be seen as special cases of a single visual-semantic hierarchy over words, sentences, and images. In this paper we advocate for explicitly modeling the partial order structure of this hierarchy. Towards this goal, we introduce a general method for learning ordered representations, and show how it can be applied to a variety of tasks involving images and language. We show that the resulting representations improve performance over current approaches for hypernym prediction and image-caption retrieval.},
  Comment                  = {Image captioning. Citation - 350.},
  Url                      = {https://arxiv.org/pdf/1511.06361}
}

@InProceedings{Venugopalan2015,
  Title                    = {Sequence to sequence-video to text},
  Author                   = {Venugopalan, Subhashini and Rohrbach, Marcus and Donahue, Jeffrey and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
  Booktitle                = {Proceedings of the IEEE international conference on computer vision},
  Year                     = {2015},
  Pages                    = {4534--4542},

  Abstract                 = {Real-world videos often have complex dynamics; andmethods for generating open-domain video descriptionsshould be sensitive to temporal structure and allow both in-put (sequence of frames) and output (sequence of words) ofvariable length. To approach this problem, we propose anovel end-to-end sequence-to-sequence model to generatecaptions for videos. For this we exploit recurrent neural net-works, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. OurLSTM model is trained on video-sentence pairs and learnsto associate a sequence of video frames to a sequence ofwords in order to generate a description of the event in thevideo clip. Our model naturally is able to learn the tem-poral structure of the sequence of frames as well as the se-quence model of the generated sentences, i.e. a languagemodel. We evaluate several variants of our model that ex-ploit different visual features on a standard set of YouTubevideos and two movie description datasets (M-VAD andMPII-MD).},
  Comment                  = {Video captioning. Citations - 919},
  Url                      = {http://openaccess.thecvf.com/content_iccv_2015/papers/Venugopalan_Sequence_to_Sequence_ICCV_2015_paper.pdf}
}

@InProceedings{venugopalan2015sequence,
  Title                    = {Sequence to sequence-video to text},
  Author                   = {Venugopalan, Subhashini and Rohrbach, Marcus and Donahue, Jeffrey and Mooney, Raymond and Darrell, Trevor and Saenko, Kate},
  Booktitle                = {Proceedings of the IEEE international conference on computer vision},
  Year                     = {2015},
  Pages                    = {4534--4542},

  Abstract                 = {Real-world videos often have complex dynamics; methods for generating open-domain video descriptions should be senstive to temporal structure and allow both input (sequence of frames) and output (sequence of words) of variable length. To approach this problem we propose a novel end-to-end sequence-to-sequence model to generate captions for videos. For this we exploit recurrent neural networks, specifically LSTMs, which have demonstrated state-of-the-art performance in image caption generation. Our LSTM model is trained on video-sentence pairs and learns to associate a sequence of video frames to a sequence of words in order to generate a description of the event in the video clip. Our model naturally is able to learn the temporal structure of the sequence of frames as well as the sequence model of the generated sentences, i.e. a language model. We evaluate several variants of our model that exploit different visual features on a standard set of YouTube videos and two movie description datasets (M-VAD and MPII-MD).},
  Comment                  = {Video captioning. Citations - 919},
  Url                      = {http://openaccess.thecvf.com/content_iccv_2015/papers/Venugopalan_Sequence_to_Sequence_ICCV_2015_paper.pdf}
}

@Article{venugopalan2014translating,
  Title                    = {Translating videos to natural language using deep recurrent neural networks},
  Author                   = {Venugopalan, Subhashini and Xu, Huijuan and Donahue, Jeff and Rohrbach, Marcus and Mooney, Raymond and Saenko, Kate},
  Journal                  = {arXiv preprint arXiv:1412.4729},
  Year                     = {2014},

  Abstract                 = {Solving the visual symbol grounding problem has long been a goal of artificial intelligence. The field appears to be advancing closer to this goal with recent breakthroughs in deep learning for natural language grounding in static images. In this paper, we propose to translate videos directly to sentences using a unified deep neural network with both convolutional and recurrent structure. Described video datasets are scarce, and most existing methods have been applied to toy domains with a small vocabulary of possible words. By transferring knowledge from 1.2M+ images with category labels and 100,000+ images with captions, our method is able to create sentence descriptions of open-domain videos with large vocabularies. We compare our approach with recent work using language generation metrics, subject, verb, and object prediction accuracy, and a human evaluation.},
  Comment                  = {Video captioning. CItations - 722},
  Url                      = {https://arxiv.org/pdf/1412.4729}
}

@Article{vinyals2015neural,
  Title                    = {A neural conversational model},
  Author                   = {Vinyals, Oriol and Le, Quoc},
  Journal                  = {arXiv preprint arXiv:1506.05869},
  Year                     = {2015},

  Abstract                 = {Conversational modeling is an important task in natural language understanding and machine intelligence. Although previous approaches exist, they are often restricted to specific domains (e.g., booking an airline ticket) and require hand-crafted rules. In this paper, we present a simple approach for this task which uses the recently proposed sequence to sequence framework. Our model converses by predicting the next sentence given the previous sentence or sentences in a conversation. The strength of our model is that it can be trained end-to-end and thus requires much fewer hand-crafted rules. We find that this straightforward model can generate simple conversations given a large conversational training dataset. Our preliminary results suggest that, despite optimizing the wrong objective function, the model is able to converse well. It is able extract knowledge from both a domain specific dataset, and from a large, noisy, and general domain dataset of movie subtitles. On a domain-specific IT helpdesk dataset, the model can find a solution to a technical problem via conversations. On a noisy open-domain movie transcript dataset, the model can perform simple forms of common sense reasoning. As expected, we also find that the lack of consistency is a common failure mode of our model.},
  Comment                  = {Conversation modelling. Citations - 1385},
  Url                      = {https://arxiv.org/pdf/1506.05869.pdf)}
}

@Article{vinyals2016show,
  Title                    = {Show and tell: Lessons learned from the 2015 mscoco image captioning challenge},
  Author                   = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
  Journal                  = {IEEE transactions on pattern analysis and machine intelligence},
  Year                     = {2016},
  Number                   = {4},
  Pages                    = {652--663},
  Volume                   = {39},

  Abstract                 = {Automatically describing the content of an image is a fundamental problem in artificial intelligence that connects computer vision and natural language processing. In this paper, we present a generative model based on a deep recurrent architecture that combines recent advances in computer vision and machine translation and that can be used to generate natural sentences describing an image. The model is trained to maximize the likelihood of the target description sentence given the training image. Experiments on several datasets show the accuracy of the model and the fluency of the language it learns solely from image descriptions. Our model is often quite accurate, which we verify both qualitatively and quantitatively. Finally, given the recent surge of interest in this task, a competition was organized in 2015 using the newly released COCO dataset. We describe and analyze the various improvements we applied to our own baseline and show the resulting performance in the competition, which we won ex-aequo with a team from Microsoft Research.},
  Comment                  = {Image captioning. CItations - 496},
  Publisher                = {IEEE},
  Url                      = {https://ieeexplore.ieee.org/iel7/34/4359286/07505636.pdf}
}

@Article{Wang2016,
  Title                    = {Where does AlphaGo go: From church-turing thesis to AlphaGo thesis and beyond},
  Author                   = {Wang, Fei-Yue and Zhang, Jun Jason and Zheng, Xinhu and Wang, Xiao and Yuan, Yong and Dai, Xiaoxiao and Zhang, Jie and Yang, Liuqing},
  Journal                  = {IEEE/CAA Journal of Automatica Sinica},
  Year                     = {2016},
  Number                   = {2},
  Pages                    = {113--120},
  Volume                   = {3},

  Owner                    = {manoj},
  Publisher                = {IEEE},
  Timestamp                = {2020.08.21},
  Url                      = {http://159.226.21.68/bitstream/173211/11604/1/Where%20does%20AlphaGo%20go%EF%BC%9A%20from%20Church-Turing%20Thesis%20to%20AlphaGo%20Thesis%20and%20beyond.pdf}
}

@Article{wang2017r,
  Title                    = {R $\^{} 3$: Reinforced reader-ranker for open-domain question answering},
  Author                   = {Wang, Shuohang and Yu, Mo and Guo, Xiaoxiao and Wang, Zhiguo and Klinger, Tim and Zhang, Wei and Chang, Shiyu and Tesauro, Gerald and Zhou, Bowen and Jiang, Jing},
  Journal                  = {arXiv preprint arXiv:1709.00023},
  Year                     = {2017},

  Abstract                 = {In recent years researchers have achieved considerable success applying neural network methods to question answering (QA). These approaches have achieved state of the art results in simplified closed-domain settings such as the SQuAD (Rajpurkar et al., 2016) dataset, which provides a pre-selected passage, from which the answer to a given question may be extracted. More recently, researchers have begun to tackle open-domain QA, in which the model is given a question and access to a large corpus (e.g., wikipedia) instead of a pre-selected passage (Chen et al., 2017a). This setting is more complex as it requires large-scale search for relevant passages by an information retrieval component, combined with a reading comprehension model that "reads" the passages to generate an answer to the question. Performance in this setting lags considerably behind closed-domain performance. In this paper, we present a novel open-domain QA system called Reinforced Ranker-Reader (R3), based on two algorithmic innovations. First, we propose a new pipeline for open-domain QA with a Ranker component, which learns to rank retrieved passages in terms of likelihood of generating the ground-truth answer to a given question. Second, we propose a novel method that jointly trains the Ranker along with an answer-generation Reader model, based on reinforcement learning. We report extensive experimental results showing that our method significantly improves on the state of the art for multiple open-domain QA datasets.},
  Comment                  = {Question answering. Citations - 150},
  Url                      = {https://arxiv.org/pdf/1709.00023}
}

@InProceedings{wang2017gated,
  Title                    = {Gated self-matching networks for reading comprehension and question answering},
  Author                   = {Wang, Wenhui and Yang, Nan and Wei, Furu and Chang, Baobao and Zhou, Ming},
  Booktitle                = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  Year                     = {2017},
  Pages                    = {189--198},

  Abstract                 = {In this paper, we present the gated self-matching networks for reading compre-hension style question answering, whichaims to answer questions from a given pas-sage. We first match the question and pas-sage with gated attention-based recurrentnetworks to obtain the question-aware pas-sage representation. Then we propose aself-matching attention mechanism to re-fine the representation by matching thepassage against itself, which effectivelyencodes information from the whole pas-sage. We finally employ the pointer net-works to locate the positions of answersfrom the passages. We conduct extensiveexperiments on the SQuAD dataset. Thesingle model achieves 71.3% on the evalu-ation metrics of exact match on the hiddentest set, while the ensemble model furtherboosts the results to 75.9%. At the time ofsubmission of the paper, our model holdsthe first place on the SQuAD leaderboardfor both single and ensemble model},
  Comment                  = {Question answering. Citations - 429},
  Url                      = {https://www.aclweb.org/anthology/P17-1018.pdf}
}

@InProceedings{wang2018video,
  Title                    = {Video captioning via hierarchical reinforcement learning},
  Author                   = {Wang, Xin and Chen, Wenhu and Wu, Jiawei and Wang, Yuan-Fang and Yang Wang, William},
  Booktitle                = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  Year                     = {2018},
  Pages                    = {4213--4222},

  Abstract                 = {Video captioning is the task of automatically generating a textual description of the actions in a video. Although previous work (e.g. sequence-to-sequence model) has shown promising results in abstracting a coarse description of a short video, it is still very challenging to caption a video containing multiple fine-grained actions with a detailed description. This paper aims to address the challenge by proposing a novel hierarchical reinforcement learning framework for video captioning, where a high-level Manager module learns to design sub-goals and a low-level Worker module recognizes the primitive actions to fulfill the sub-goal. With this compositional framework to reinforce video captioning at different levels, our approach significantly outperforms all the baseline methods on a newly introduced large-scale dataset for fine-grained video captioning. Furthermore, our non-ensemble model has already achieved the state-of-the-art results on the widely-used MSR-VTT dataset.},
  Comment                  = {Video captioning. Citations - 103.},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_Video_Captioning_via_CVPR_2018_paper.pdf}
}

@InCollection{werbos1982applications,
  Title                    = {Applications of advances in nonlinear sensitivity analysis},
  Author                   = {Werbos, Paul J},
  Booktitle                = {System modeling and optimization},
  Publisher                = {Springer},
  Year                     = {1982},
  Pages                    = {762--770},

  Abstract                 = {The following paper summarizes the major properties and applications of a collection of algorithms involving differentiation and optimization at minimum cost. The areas of application include the sensitivity analysis of models, new work in statistical or econometric estimation, optimization, artificial intelligence and neuron modelling. The details, references and derivations can be obtained by requesting „Sensitivity Analysis Methods for Nonlinear Systems“ from Forecast Analysis and Evaluation Team, Quality Assurance, OSS/EIA, Room 7413, Department of Energy, Washington, DC 20461.},
  Comment                  = {Nonlinear sensitivity. Citations - 284.},
  Url                      = {https://link.springer.com/chapter/10.1007/BFb0006203}
}

@Article{Williams2007,
  Title                    = {Partially observable Markov decision processes for spoken dialog systems},
  Author                   = {Williams, Jason D and Young, Steve},
  Journal                  = {Computer Speech \& Language},
  Year                     = {2007},
  Number                   = {2},
  Pages                    = {393--422},
  Volume                   = {21},

  Abstract                 = {In a spoken dialog system, determining which action a machine should take in a given situation is a difficult problem because automatic speech recognition is unreliable and hence the state of the conversation can never be known with certainty. Much of the research in spoken dialog systems centres on mitigating this uncertainty and recent work has focussed on three largely disparate techniques: parallel dialog state hypotheses, local use of confidence scores, and automated planning. While in isolation each of these approaches can improve action selection, taken together they currently lack a unified statistical framework that admits global optimization. In this paper we cast a spoken dialog system as a partially observable Markov decision process (POMDP). We show how this formulation unifies and extends existing techniques to form a single principled framework. A number of illustrations are used to show qualitatively the potential benefits of POMDPs compared to existing techniques, and empirical results from dialog simulations are presented which demonstrate significant quantitative gains. Finally, some of the key challenges to advancing this method – in particular scalability – are briefly outlined.},
  Comment                  = {Markov decision process. Citations - 891},
  Owner                    = {manoj},
  Publisher                = {Elsevier},
  Timestamp                = {2020.08.27},
  Url                      = {http://svr-www.eng.cam.ac.uk/~sjy/papers/wiyo07-j.pdf}
}

@Article{williams2007partially,
  Title                    = {Partially observable Markov decision processes for spoken dialog systems},
  Author                   = {Williams, Jason D and Young, Steve},
  Journal                  = {Computer Speech \& Language},
  Year                     = {2007},
  Number                   = {2},
  Pages                    = {393--422},
  Volume                   = {21},

  Abstract                 = {In a spoken dialog system, determining which action a machine should take in a given situation is a difficult problem because automatic speech recognition is unreliable and hence the state of the conversation can never be known with certainty. Much of the research in spoken dialog systems centres on mitigating this uncertainty and recent work has focussed on three largely disparate techniques: parallel dialog state hypotheses, local use of confidence scores, and automated planning. While in isolation each of these approaches can improve action selection, taken together they currently lack a unified statistical framework that admits global optimization. In this paper we cast a spoken dialog system as a partially observable Markov decision process (POMDP). We show how this formulation unifies and extends existing techniques to form a single principled framework. A number of illustrations are used to show qualitatively the potential benefits of POMDPs compared to existing techniques, and empirical results from dialog simulations are presented which demonstrate significant quantitative gains. Finally, some of the key challenges to advancing this method – in particular scalability – are briefly outlined.},
  Comment                  = {Markov decision process. Citations - 891},
  Owner                    = {manoj},
  Publisher                = {Elsevier},
  Timestamp                = {2020.08.27},
  Url                      = {http://svr-www.eng.cam.ac.uk/~sjy/papers/wiyo07-j.pdf}
}

@Article{wu2016google,
  Title                    = {Google's neural machine translation system: Bridging the gap between human and machine translation},
  Author                   = {Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  Journal                  = {arXiv preprint arXiv:1609.08144},
  Year                     = {2016},

  Abstract                 = {Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference – sometimes prohibitively so in the case of very large data sets and large models. Several authors have also charged that NMT systems lack robustness, particularly when input sentences contain rare words. These issues have hindered NMT’s use in practical deployments and services, where both accuracy and
speed are essential. In this work, we present GNMT, Google’s Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using residual connections as well as attention connections from the decoder network to the encoder. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (“wordpieces”) for both input and output. This method provides a good balance between the flexibility of “character”-delimited models and the efficiency of “word”-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. To directly optimize the translation BLEU scores, we consider refining the models by using reinforcement learning, but we found that the improvement in the BLEU scores did not reflect in the human evaluation. On the WMT’14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google’s phrase-based production system.},
  Comment                  = {Machine translation. Citation - 3090},
  Url                      = {https://arxiv.org/pdf/1609.08144.pdf (7}
}

@InProceedings{xiang2015learning,
  Title                    = {Learning to track: Online multi-object tracking by decision making},
  Author                   = {Xiang, Yu and Alahi, Alexandre and Savarese, Silvio},
  Booktitle                = {Proceedings of the IEEE international conference on computer vision},
  Year                     = {2015},
  Pages                    = {4705--4713},

  Abstract                 = {Online Multi-Object Tracking (MOT) has wide appli-cations in time-critical video analysis scenarios, such asrobot navigation and autonomous driving. In tracking-by-detection, a major challenge of online MOT is how torobustly associate noisy object detections on a new videoframe with previously tracked objects. In this work, weformulate the online MOT problem as decision making inMarkov Decision Processes (MDPs), where the lifetime ofan object is modeled with a MDP. Learning a similarityfunction for data association is equivalent to learning a pol-icy for the MDP, and the policy learning is approached ina reinforcement learning fashion which benefits from bothadvantages of offline-learning and online-learning for dataassociation. Moreover, our framework can naturally handlethe birth/death and appearance/disappearance of targets bytreating them as state transitions in the MDP while leverag-ing existing online single object tracking methods. We con-duct experiments on the MOT Benchmark [24] to verify theeffectiveness of our method.},
  Comment                  = {Object tracking. Citations - 440.},
  Url                      = {https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Xiang_Learning_to_Track_ICCV_2015_paper.pdf}
}

@InProceedings{xiong2016dynamic,
  Title                    = {Dynamic memory networks for visual and textual question answering},
  Author                   = {Xiong, Caiming and Merity, Stephen and Socher, Richard},
  Booktitle                = {International conference on machine learning},
  Year                     = {2016},
  Pages                    = {2397--2406},

  Abstract                 = {Neural network architectures with memory and attention mechanisms exhibit certain reasoning capabilities required for question answering. One such architecture, the dynamic memory network (DMN), obtained high accuracy on a variety of language tasks. However, it was not shown
whether the architecture achieves strong results
for question answering when supporting facts are
not marked during training or whether it could
be applied to other modalities such as images.
Based on an analysis of the DMN, we propose
several improvements to its memory and input
modules. Together with these changes we introduce a novel input module for images in order
to be able to answer visual questions. Our new
DMN+ model improves the state of the art on
both the Visual Question Answering dataset and
the bAbI-10k text question-answering dataset
without supporting fact supervision.},
  Comment                  = {Question answering. Citation - 541},
  Url                      = {http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf?luicode=10000011&lfid=231522type=1&t=10&q=#ICML2016#&featurecode=newtitle
麻麻！这个小姐姐撩我！我要娶她٩( 'ω' )و&u=http://www.jmlr.org/proceedings/papers/v48/xiong16.pdf}
}

@Article{Xiong2016,
  Title                    = {Dynamic coattention networks for question answering},
  Author                   = {Xiong, Caiming and Zhong, Victor and Socher, Richard},
  Journal                  = {arXiv preprint arXiv:1611.01604},
  Year                     = {2016},

  Abstract                 = {Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1.},
  Comment                  = {Question answering. Citations - 448},
  Url                      = {https://arxiv.org/pdf/1611.01604}
}

@InProceedings{xu2017video,
  Title                    = {Video question answering via gradually refined attention over appearance and motion},
  Author                   = {Xu, Dejing and Zhao, Zhou and Xiao, Jun and Wu, Fei and Zhang, Hanwang and He, Xiangnan and Zhuang, Yueting},
  Booktitle                = {Proceedings of the 25th ACM international conference on Multimedia},
  Year                     = {2017},
  Pages                    = {1645--1653},

  Abstract                 = {Recently image question answering (ImageQA) has gained lotsof attention in the research community. However, as its naturalextension, video question answering (VideoQA) is less explored.Although both tasks look similar, VideoQA is more challengingmainly because of the complexity and diversity of videos. As such,simply extending the ImageQA methods to videos is insufficient andsuboptimal. Particularly, working with the video needs to model itsinherent temporal structure and analyze the diverse informationit contains. In this paper, we consider exploiting the appearanceand motion information resided in the video with a novel attentionmechanism. More specifically, we propose an end-to-end modelwhich gradually refines its attention over the appearance and mo-tion features of the video using the question as guidance. Thequestion is processed word by word until the model generates thefinal optimized attention. The weighted representation of the video,as well as other contextual information, are used to generate theanswer. Extensive experiments show the advantages of our modelcompared to other baseline models. We also demonstrate the effec-tiveness of our model by analyzing the refined attention weightsduring the question answering procedure.},
  Comment                  = {Video question answering. Citations - 59.},
  Url                      = {https://www.comp.nus.edu.sg/~xiangnan/papers/mm17-videoQA.pdf}
}

@Article{xue2017unifying,
  Title                    = {Unifying the video and question attentions for open-ended video question answering},
  Author                   = {Xue, Hongyang and Zhao, Zhou and Cai, Deng},
  Journal                  = {IEEE Transactions on Image Processing},
  Year                     = {2017},
  Number                   = {12},
  Pages                    = {5656--5666},
  Volume                   = {26},

  Abstract                 = {Video question answering is an important task toward scene understanding and visual data retrieval. However, current visual question answering works mainly focus on a single static image, which is distinct from the dynamic and sequential visual data in the real world. Their approaches cannot utilize the temporal information in videos. In this paper, we introduce the task of free-form open-ended video question answering. The open-ended answers enable wider applications compared with the common multiple-choice tasks in Visual-QA. We first propose a data set for open-ended Video-QA with the automatic question generation approaches. Then, we propose our sequential video attention and temporal question attention models. These two models apply the attention mechanism on videos and questions, while preserving the sequential and temporal structures of the guides. The two models are integrated into the model of unified attention. After the video and the question are encoded, the answers are generated wordwisely from our models by a decoder. In the end, we evaluate our models on the proposed data set. The experimental results demonstrate the effectiveness of our proposed model.},
  Comment                  = {Video question answering. Citations - 25.},
  Publisher                = {IEEE},
  Url                      = {https://ieeexplore.ieee.org/abstract/document/8017608/}
}

@Article{yang2017improved,
  Title                    = {Improved variational autoencoders for text modeling using dilated convolutions},
  Author                   = {Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and Berg-Kirkpatrick, Taylor},
  Journal                  = {arXiv preprint arXiv:1702.08139},
  Year                     = {2017},

  Abstract                 = {Recent work on generative modeling of text has found that variational auto-encoders (VAE) incorporating LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the effective context from previously generated words. In experiments, we find that there is a trade off between the contextual capacity of the decoder and the amount of encoding information used. We show that with the right decoder, VAE can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive experimental result on the use VAE for generative modeling of text. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.},
  Comment                  = {Variational autoencoders. Citations - 198.},
  Url                      = {https://arxiv.org/pdf/1702.08139}
}

@InProceedings{yao2018exploring,
  Title                    = {Exploring visual relationship for image captioning},
  Author                   = {Yao, Ting and Pan, Yingwei and Li, Yehao and Mei, Tao},
  Booktitle                = {Proceedings of the European conference on computer vision (ECCV)},
  Year                     = {2018},
  Pages                    = {684--699},

  Abstract                 = {It is always well believed that modeling relationships be-tween objects would be helpful for representing and eventually describingan image. Nevertheless, there has not been evidence in support of the ideaon image description generation. In this paper, we introduce a new designto explore the connections between objects for image captioning underthe umbrella of attention-based encoder-decoder framework. Specifically,we present Graph Convolutional Networks plus Long Short-Term Mem-ory (dubbed as GCN-LSTM) architecture that novelly integrates bothsemantic and spatial object relationships into image encoder.Technical-ly, we build graphs over the detected objects in an image based on theirspatial and semantic connections. The representations of each region pro-posed on objects are then refined by leveraging graph structure throughGCN. With the learnt region-level features, our GCN-LSTM capitalizeson LSTM-based captioning framework with attention mechanismfor sen-tence generation. Extensive experiments are conducted on COCOimagecaptioning dataset, and superior results are reported when comparingto state-of-the-art approaches. More remarkably, GCN-LSTM increasesCIDEr-D performance from 120.1% to 128.7% on COCO testing set.},
  Comment                  = {Image captioning. CItations - 103},
  Url                      = {http://openaccess.thecvf.com/content_ECCV_2018/papers/Ting_Yao_Exploring_Visual_Relationship_ECCV_2018_paper.pdf}
}

@InProceedings{Yao2017,
  Title                    = {Boosting image captioning with attributes},
  Author                   = {Yao, Ting and Pan, Yingwei and Li, Yehao and Qiu, Zhaofan and Mei, Tao},
  Booktitle                = {Proceedings of the IEEE International Conference on Computer Vision},
  Year                     = {2017},
  Pages                    = {4894--4902},

  Abstract                 = {Automatically describing an image with a natural lan-guage has been an emerging challenge in both fields ofcomputer vision and natural language processing. In thispaper, we present Long Short-Term Memory with Attributes(LSTM-A) - a novel architecture that integrates attributesinto the successful Convolutional Neural Networks (CNNs)plus Recurrent Neural Networks (RNNs) image captioningframework, by training them in an end-to-end manner. Par-ticularly, the learning of attributes is strengthened by in-tegrating inter-attribute correlations into Multiple InstanceLearning (MIL). To incorporate attributes into captioning,we construct variants of architectures by feeding imagerepresentations and attributes into RNNs in different waysto explore the mutual but also fuzzy relationship betweenthem. Extensive experiments are conducted on COCO im-age captioning dataset and our framework shows clear im-provements when compared to state-of-the-art deep mod-els. More remarkably, we obtain METEOR/CIDEr-D of25.5%/100.2% on testing data of widely used and publiclyavailable splits in [10] when extracting image representa-tions by GoogleNet and achieve superior performance onCOCO captioning Leaderboard.},
  Comment                  = {Image captioning. Citations - 347},
  Url                      = {http://openaccess.thecvf.com/content_ICCV_2017/papers/Yao_Boosting_Image_Captioning_ICCV_2017_paper.pdf}
}

@InProceedings{yao2017boosting,
  Title                    = {Boosting image captioning with attributes},
  Author                   = {Yao, Ting and Pan, Yingwei and Li, Yehao and Qiu, Zhaofan and Mei, Tao},
  Booktitle                = {Proceedings of the IEEE International Conference on Computer Vision},
  Year                     = {2017},
  Pages                    = {4894--4902},

  Abstract                 = {Automatically describing an image with a natural language has been an emerging challenge in both fields of computer vision and natural language processing. In this paper, we present Long Short-Term Memory with Attributes (LSTM-A) - a novel architecture that integrates attributes into the successful Convolutional Neural Networks (CNNs) plus Recurrent Neural Networks (RNNs) image captioning framework, by training them in an end-to-end manner. Particularly, the learning of attributes is strengthened by integrating inter-attribute correlations into Multiple Instance Learning (MIL). To incorporate attributes into captioning, we construct variants of architectures by feeding image representations and attributes into RNNs in different ways to explore the mutual but also fuzzy relationship between them. Extensive experiments are conducted on COCO image captioning dataset and our framework shows clear improvements when compared to state-of-the-art deep models. More remarkably, we obtain METEOR/CIDEr-D of 25.5%/100.2% on testing data of widely used and publicly available splits in [10] when extracting image representations by GoogleNet and achieve superior performance on COCO captioning Leaderboard.},
  Comment                  = {Image captioning. Citations - 347.},
  Url                      = {http://openaccess.thecvf.com/content_ICCV_2017/papers/Yao_Boosting_Image_Captioning_ICCV_2017_paper.pdf}
}

@InProceedings{you2016image,
  Title                    = {Image captioning with semantic attention},
  Author                   = {You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2016},
  Pages                    = {4651--4659},

  Abstract                 = {Automatically generating a natural language description of an image has attracted interests recently both because of its importance in practical applications and because it connects two major artificial intelligence fields: computer vision and natural language processing. Existing approaches are either top-down, which start from a gist of an image and convert it into words, or bottom-up, which come up with words describing various aspects of an image and then combine them. In this paper, we propose a new algorithm that combines both approaches through a model of semantic attention. Our algorithm learns to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of recurrent neural networks. The selection and fusion form a feedback connecting the top-down and bottom-up computation. We evaluate our algorithm on two public benchmarks: Microsoft COCO and Flickr30K. Experimental results show that our algorithm significantly outperforms the state-of-the-art approaches consistently across different evaluation metrics.},
  Comment                  = {Image captioning. Citations - 934},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2016/papers/You_Image_Captioning_With_CVPR_2016_paper.pdf}
}

@InProceedings{yu2016video,
  Title                    = {Video paragraph captioning using hierarchical recurrent neural networks},
  Author                   = {Yu, Haonan and Wang, Jiang and Huang, Zhiheng and Yang, Yi and Xu, Wei},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2016},
  Pages                    = {4584--4593},

  Abstract                 = {We present an approach that exploits hierarchical Recur-rent Neural Networks (RNNs) to tackle the video captioningproblem, i.e., generating one or multiple sentences to de-scribe a realistic video. Our hierarchical framework con-tains a sentence generator and a paragraph generator. Thesentence generator produces one simple short sentence thatdescribes a specific short video interval. It exploits bothtemporal- and spatial-attention mechanisms to selectivelyfocus on visual elements during generation. The paragraphgenerator captures the inter-sentence dependency by takingas input the sentential embedding produced by the sentencegenerator, combining it with the paragraph history, andoutputting the new initial state for the sentence generator.We evaluate our approach on two large-scale benchmarkdatasets: YouTubeClips and TACoS-MultiLevel. The exper-iments demonstrate that our approach significantly outper-forms the current state-of-the-art methods with BLEU@4scores 0.499 and 0.305 respectively.},
  Comment                  = {Video captioning. Citations - 413},
  Url                      = {http://openaccess.thecvf.com/content_cvpr_2016/papers/Yu_Video_Paragraph_Captioning_CVPR_2016_paper.pdf}
}

@Article{yu2014deep,
  Title                    = {Deep learning for answer sentence selection},
  Author                   = {Yu, Lei and Hermann, Karl Moritz and Blunsom, Phil and Pulman, Stephen},
  Journal                  = {arXiv preprint arXiv:1412.1632},
  Year                     = {2014},

  Abstract                 = {Answer sentence selection is the task of identifying sentences that contain the answer to a given question. This is an important problem in its own right as well as in the larger context of open domain question answering. We propose a novel approach to solving this task via means of distributed representations, and learn to match questions with answers by considering their semantic encoding. This contrasts prior work on this task, which typically relies on classifiers with large numbers of hand-crafted syntactic and semantic features and various external resources. Our approach does not require any feature engineering nor does it involve specialist linguistic data, making this model easily applicable to a wide range of domains and languages. Experimental results on a standard benchmark dataset from TREC demonstrate that---despite its simplicity---our model matches state of the art performance on the answer sentence selection task.},
  Comment                  = {Question answering. Citations - 369},
  Url                      = {https://arxiv.org/pdf/1412.1632}
}

@InProceedings{yuan2016grammatical,
  Title                    = {Grammatical error correction using neural machine translation},
  Author                   = {Yuan, Zheng and Briscoe, Ted},
  Booktitle                = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  Year                     = {2016},
  Pages                    = {380--386},

  Abstract                 = {This paper presents the first study using neural machine translation (NMT) for grammatical error correction (GEC). We propose a twostep approach to handle the rare word problem in NMT, which has been proved to be useful and effective for the GEC task. Our best NMTbased system trained on the CLC outperforms our SMT-based system when testing on the publicly available FCE test set. The same system achieves an F0.5 score of 39.90% on the CoNLL-2014 shared task test set, outperforming the state-of-the-art and demonstrating that the NMT-based GEC system generalises effectively.},
  Comment                  = {Machine translations. Citations - 118},
  Url                      = {https://www.aclweb.org/anthology/N16-1042.pdf}
}

@Article{zeng2016leveraging,
  Title                    = {Leveraging video descriptions to learn video question answering},
  Author                   = {Zeng, Kuo-Hao and Chen, Tseng-Hung and Chuang, Ching-Yao and Liao, Yuan-Hong and Niebles, Juan Carlos and Sun, Min},
  Journal                  = {arXiv preprint arXiv:1611.04021},
  Year                     = {2016},

  Abstract                 = {We propose a scalable approach to learn video-based question answering (QA): answer a "free-form natural language question" about a video content. Our approach automatically harvests a large number of videos and descriptions freely available online. Then, a large number of candidate QA pairs are automatically generated from descriptions rather than manually annotated. Next, we use these candidate QA pairs to train a number of video-based QA methods extended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et al. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect candidate QA pairs, we propose a self-paced learning procedure to iteratively identify them and mitigate their effects in training. Finally, we evaluate performance on manually generated video-based QA pairs. The results show that our self-paced learning procedure is effective, and the extended SS model outperforms various baselines.},
  Comment                  = {Video question answering. Citations - 74},
  Url                      = {https://arxiv.org/pdf/1611.04021}
}

@InProceedings{zhang2016summary,
  Title                    = {Summary transfer: Exemplar-based subset selection for video summarization},
  Author                   = {Zhang, Ke and Chao, Wei-Lun and Sha, Fei and Grauman, Kristen},
  Booktitle                = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  Year                     = {2016},
  Pages                    = {1059--1067},

  Abstract                 = {Video summarization has unprecedented importance to help us digest, browse, and search today's ever-growing video collections. We propose a novel subset selection technique that leverages supervision in the form of human-created summaries to perform automatic keyframe-based video summarization. The main idea is to nonparametrically transfer summary structures from annotated videos to unseen test videos. We show how to extend our method to exploit semantic side information about the video's category/genre to guide the transfer process by those training videos semantically consistent with the test input. We also show how to generalize our method to subshot-based summarization, which not only reduces computational costs but also provides more flexible ways of defining visual similarity across subshots spanning several frames. We conduct extensive evaluation on several benchmarks and demonstrate promising results, outperforming existing methods in several settings.},
  Comment                  = {Video summarization. Citation - 146.},
  Url                      = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Zhang_Summary_Transfer_Exemplar-Based_CVPR_2016_paper.pdf}
}

@InProceedings{zhang2016video,
  Title                    = {Video summarization with long short-term memory},
  Author                   = {Zhang, Ke and Chao, Wei-Lun and Sha, Fei and Grauman, Kristen},
  Booktitle                = {European conference on computer vision},
  Year                     = {2016},
  Organization             = {Springer},
  Pages                    = {766--782},

  Abstract                 = {We propose a novel supervised learning technique for summarizing videos by automatically selecting keyframes or key subshots. Casting the task as a structured prediction problem, our main idea is to use Long Short-Term Memory (LSTM) to model the variable-range temporal dependency among video frames, so as to derive both representative and compact video summaries. The proposed model successfully accounts for the sequential structure crucial to generating meaningful video summaries, leading to state-of-the-art results on two benchmark datasets. In addition to advances in modeling techniques, we introduce a strategy to address the need for a large amount of annotated data for training complex learning approaches to summarization. There, our main idea is to exploit auxiliary annotated video summarization datasets, in spite of their heterogeneity in visual styles and contents. Specifically, we show that domain adaptation techniques can improve learning by reducing the discrepancies in the original datasets’ statistical properties.},
  Comment                  = {Video summarization. Citation - 333.},
  Url                      = {https://arxiv.org/pdf/1605.08110}
}

@InProceedings{zhao2017video,
  Title                    = {Video Question Answering via Hierarchical Spatio-Temporal Attention Networks.},
  Author                   = {Zhao, Zhou and Yang, Qifan and Cai, Deng and He, Xiaofei and Zhuang, Yueting and Zhao, Zhou and Yang, Qifan and Cai, Deng and He, Xiaofei and Zhuang, Yueting},
  Booktitle                = {IJCAI},
  Year                     = {2017},
  Pages                    = {3518--3524},

  Abstract                 = {Open-ended video question answering is a chal-lenging problem in visual information retrieval,which automatically generates the natural languageanswer from the referenced video content accord-ing to the question.However, the existing vi-sual question answering works only focus on thestatic image, which may be ineffectively appliedto video question answering due to the lack ofmodeling the temporal dynamics of video con-tents.In this paper, we consider the problemof open-ended video question answering from theviewpoint of spatio-temporal attentional encoder-decoder learning framework. We propose the hi-erarchical spatio-temporal attention network forlearning the joint representation of the dynamicvideo contents according to the given question.We then develop the spatio-temporal attentionalencoder-decoder learning method with multi-stepreasoning process for open-ended video questionanswering. We construct a large-scale video ques-tion answering dataset. The extensive experimentsshow the effectiveness of our method},
  Comment                  = {Video question answering. Citations - 51.},
  Url                      = {https://pdfs.semanticscholar.org/834d/688a4a8fb893e081fcf20ff52f989b21cc17.pdf}
}

@Article{zhu2017uncovering,
  Title                    = {Uncovering the temporal context for video question answering},
  Author                   = {Zhu, Linchao and Xu, Zhongwen and Yang, Yi and Hauptmann, Alexander G},
  Journal                  = {International Journal of Computer Vision},
  Year                     = {2017},
  Number                   = {3},
  Pages                    = {409--421},
  Volume                   = {124},

  Abstract                 = {n this work, we introduce Video Question Answeringin temporal domain to infer the past, describe the presentand predict the future. We present an encoder-decoder ap-proach using Recurrent Neural Networks to learn tempo-ral structures of videos and introduce a dual-channel rank-ing loss to answer multiple-choice questions. We exploreapproaches for finer understanding of video content usingquestion form of “fill-in-the-blank”, and managed to col-lect 109,895 video clips with duration over 1,000 hoursfrom TACoS, MPII-MD, MEDTest 14 datasets, while thecorresponding 390,744 questions are generated from an-notations. Extensive experiments demonstrate that our ap-proach significantly outperforms the compared baselines.},
  Comment                  = {Video captioning. Citations - 135.},
  Publisher                = {Springer},
  Url                      = {https://arxiv.org/pdf/1511.04670}
}

@Article{,
  Abstract                 = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  Comment                  = {Inceptionnet. Citations - 23666},
  Owner                    = {manoj},
  Timestamp                = {2020.08.20},
  Url                      = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf}
}

